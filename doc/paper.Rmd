---
title: Adaption of the Chumbley Score to matching of bullet striation marks

# to produce blinded version set to 1
blinded: 0

authors: 
- name: Ganesh Krishnan
  thanks: The authors gratefully acknowledge ...
  affiliation: Department of YYY, University of XXX
  
- name: Heike Hofmann
  affiliation: Department of ZZZ, University of WWW

keywords:
- 3 to 6 keywords
- that do not appear in the title

abstract: |
  
bibliography: bibliography
biblio-style: apsr

output: 
  pdf_document:
    citation_package: natbib
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    template: template.tex

---

\newcommand{\hh}[1]{{\textcolor{orange}{#1}}}
\newcommand{\cited}[1]{{\textcolor{red}{#1}}}


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
library(knitr)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  comment = "",
  fig.height = 6,
  fig.width = 6,
  fig.align = "center",
  out.width= '\\textwidth',
  cache = FALSE,
  fig.path='figures/',
  echo=FALSE
)
options(knitr.table.format = "latex")
library(tidyverse)
```

```{r functions}
errorrate <- function(data, alpha) {
  summ <- data %>% filter(!is.na(p_value)) %>%
    mutate(signif = p_value < alpha) %>%
    group_by(wv, wo, match, signif) %>% tally()
  summ$error <- with(summ, match != signif)
  summ$alpha <- alpha
  
  rates <- summ %>% group_by(wv, wo, match, alpha) %>% summarize(
    rate = n[error==TRUE]/sum(n)
  )
  rates %>% spread(match,rate) %>% rename(
    actual = `FALSE`,
    beta = `TRUE`
  )
}
```

```{r data}
if (!file.exists("../data/all-sigs.rds")) {
  files <- dir("../data/signatures", pattern="csv")
  all <- data.frame()
  for (file in files) {
    tmp <- read.csv(file=file.path(path="../data/signatures", file)) 
    if (length(grep("x",names(tmp)) > 0)) tmp <- tmp %>% select(-x)
    all <- rbind(all, tmp)
  }
  all <- all %>% filter(land1_id < land2_id)
  saveRDS(all, file="../data/all-sigs.rds")
} else {
  all <- readRDS("../data/all-sigs.rds")
}

errors <- rbind(errorrate(all, 0.001), 
                errorrate(all, 0.005), 
                errorrate(all, 0.01), 
                errorrate(all, 0.05))
```

# Introduction

\hh{Ganesh, could you start to flesh out the intro?}

\begin{enumerate}
\item Problem statement: same source matching based on striations
\item Existing solution for toolmarks: Chumbley Score 
\item How is the chumbley score computed? 
\item Potential limitations in the application to striation marks on bullet lands: smaller in width and curved - need to adjust parameter settings; something similar is done in \citet{afte-chumbley} for toolmark comparisons of slip-joint pliers.
\item Using cross-validation setup to identify appropriate parameter settings for (a) signatures and (b) profiles directly
\end{enumerate}
 
# Toolmarks

\hh{Ganesh, it's never a good idea to copy directly from another paper, even if you want to cite it. Always make sure to make it absolutely clear that it is not your work, otherwise you might be accused of plagiarism.}

Citation from \citet{chumbley}:

\cited{Used an optimization window size of n = 500 and a validation window size of m = 50. It is visually clear that when the two tool marks match, the U-statistic tends to be relatively large, and when they do not match, the value tends to be relatively small.}

\cited{The p-Values associated with each U-statistic, are calculated under the assumption that the nonmatch distribution is standard normal. As should be the case, the p-values for nonmatching pairs are approximately uniformly distributed between zero and one, while the p-values for matching pairs are concentrated on small values (suggesting that they would be declared to be “matching”).}

\cited{The classification rates corresponding to the p-values where the rows correspond to the known status of the tool marks and columns represent the model conclusion. Of 50 pairs of known matching marks, we misclassified three pairs as nonmatches for false-negative error rate of 6\% and 0 pairs of known nonmatching tool marks were identified as being matches.}

\cited{Null Hypothesis: The toolmarks were not made by the same tool.
Alternate: Toolmarks made by the same tool.}

```{r cars, echo=FALSE}
a<- matrix(NA, ncol = 3, nrow = 2)
toolmark.falsepositive<- data.frame(a)
colnames(toolmark.falsepositive)<- c("Classification", "Match", "Non-Match")
toolmark.falsepositive$Classification<- c("Match", "Non-Match")
toolmark.falsepositive$Match<- c(47,0)
toolmark.falsepositive$`Non-Match`<- c(3,50)
toolmark.falsepositive

```

# Bullets

Following on similar lines, the first step here is to first identify what difference does different window sizes of optimization and the validation step have, when adapting the toolmark method to bullets.

The marking made on bullets are smaller than toolmarks and is also less wider. The idea is to find out possible areas of error while adapting the score based method proposed for toolmarks.

Bullet signatures being compared at this time are from the Hamby 44 and Hamby 252 data, which have the best set of known-matched and known non-matches.

Bullet signatures are chosen by
1. Filtering out Land_id for Profiles from the Hamby 44 and Hamby 252 data and removing 
all NA values
2. run_id = 3 is chosen, Seems like the signatures generated from this run_id give the closest
match. Different run_id's have some different settings for generating the signatures
(The level of smoothing does not seem to be one of them)

The bullet signatures when generated already included the LOWESS smoothing.
Therefore, the coarseness factor is set to 1 while running the chumbley_non_random()
which generates the same_shift, different_shift, U-Stat and P_value parameters.


WO 320 wv 50
```{r echo=FALSE}
# a<- matrix(NA, ncol = 3, nrow = 2)
# bullet.falsepositive<- data.frame(a)
# colnames(bullet.falsepositive)<- c("Classification", "FALSe", "TRUe")
# bullet.falsepositive$Classification<- c("FALSe", "TRUe")
# bullet.falsepositive$FALSe<- c(77564,412)
# bullet.falsepositive$TRUe<- c(4022,674)
# bullet.falsepositive

all %>% filter(wo==320, wv==50) %>% xtabs(data=., ~signif+match)
```
Wo 200 WV 50
```{r echo=FALSE}
# bullet.falsepositive$FALSe<- c(78817,383)
# bullet.falsepositive$TRUe<- c(4004,798)
# bullet.falsepositive

all %>% filter(wo==200, wv==50) %>% xtabs(data=., ~signif+match)
```
Wo 80 wv 50
```{r echo=FALSE}
# bullet.falsepositive$FALSe<- c(78817,450)
# bullet.falsepositive$TRUe<- c(4680,775)
# bullet.falsepositive

all %>% filter(wo==80, wv==50) %>% xtabs(data=., ~signif+match)

```

# Results

## Signatures

Signatures of lands for all Hamby-44 and Hamby-252 scans made available through the NIST ballistics database \citep{nist} were considered. Both of these sets of scans are part of the larger Hamby study \citep{hamby} and each consist of twenty known bullets (two each from ten consecutively rifled Ruger P85 barrels) and fifteen questioned bullets (each matching one of the ten barrels). Ground truth for both of these Hamby sets is known and was used to assess correctness of the tests results. 

We used the adjusted Chumbley method as proposed in \citet{hadler} and implemented in the R package `toolmaRk` \citep{toolmark} on all pairwise land-to-land comparisons of the Hamby scans (a total of 85,491 comparisons) in the following manner:

\hh{ XXX maybe we should include a table with an overview of the different parameter settings}

### Signatures

Figure \ref{fig:type2} gives an overview of type II error rates observed when varying the window size in the optimization step.


```{r type2, fig.width=8, fig.height=5, out.width='\\textwidth', fig.cap="Type II error rates observed across a range of window sizes for optimization $wo$. For a window size of $wo = 120$ we see a drop in type II error rate across all type I rates considered. Smaller validation sizes $wv$ are typically associated with a smaller type II error."}
greens <- RColorBrewer::brewer.pal(name="Greens", n = 6)
errors %>% filter(wv %in% c(30,50)) %>%
  ggplot(aes(x = wo, y = beta, colour=factor(alpha))) + 
  geom_point(aes(shape=factor(wv))) +
#  facet_grid(.~wo, labeller="label_both") + 
  geom_smooth(aes(group=alpha), se=FALSE, method="loess") +
  theme_bw() +
  scale_colour_manual("Nominal type I error", values=greens[-(1:2)]) +
  scale_shape_discrete("Size of \nvalidation window") +
  xlab("Window size for optimization") +
  ylab("Type II error rate")
```

Figure \ref{fig:type1} compares nominal (fixed) type I error and actually observed type I errors for the parameter settings in table XXX. With an increasing size of the window used in the optimization step the observed type I error rate decreases (slighty). This might be related to the increasing number of tests that fail for larger window sizes, in particular for non-matching striae (see fig \ref{fig:missings}).

```{r type1, fig.width=8, fig.height=5, out.width='\\textwidth', fig.cap="Comparison of observed and nominal type I error rates  across a range of window sizes for optimization $wo$. The horizontal line in each facet indicates the nominal type I error rate.  As the optimization window increase the observed type I error rate gets smaller. A smaller validation window tends to be associated with a higher type I error rate."}
errors %>% filter(wv %in% c(30,50)) %>%
  ggplot(aes(x = wo, y = actual, colour=factor(alpha))) + 
  geom_hline(aes(yintercept=alpha), colour="grey30") +
  geom_point(aes(shape=factor(wv))) +
  #  facet_grid(.~wo, labeller="label_both") + 
  geom_smooth(aes(group=alpha), se=FALSE, method="lm") +
  theme_bw() +
#  scale_y_log10(breaks=c(0.001,.005, 0.01, 0.05)) +
  scale_shape_discrete("Size of \nvalidation window") +
  ylab("Observed type I error rate") +
  xlab("Window size for optimization") +
  scale_colour_brewer("Nominal type I error alpha", palette="Set2") +
  facet_wrap(~alpha, labeller="label_both", scales="free") +
  theme(legend.position="bottom")
  
```

\hh{Some of the test results are missing - we should look into the numbers and parameter settings for those.}

```{r missings, fig.width=8, fig.height=5, fig.cap="Number of failed tests."}
ns <- all %>% group_by(wv, wo, match) %>% summarize(
  n_ss = mean(same_shift_n),
  n_ds = mean(diff_shift_n),
  zero_ss = sum(same_shift_n==0),
  zero_ds = sum(diff_shift_n==0),
  miss = sum(is.na(p_value))
)
ns %>% filter(wv %in% c(30, 50)) %>% 
  ggplot(aes(x = wo, y = miss/85491*100, shape=factor(wv), colour = factor(match))) + 
  geom_smooth(method="lm", size=.5, se=FALSE) +
  geom_point() + 
  theme_bw() +
  ylab("Percent of failed tests.") +
  xlab("Size of optimization window") +
  scale_colour_brewer("Match", palette="Set1") +
  scale_shape_discrete("Size of \nvalidation\nwindow")

```

```{r wo120, fig.width = 8, fig.height=4.5}
wo120 <- errors %>% filter(wo==120)
wo120 %>% gather(type, error, actual:beta) %>%
  ggplot(aes(x = wv, y = error, colour = factor(alpha)))  + 
  geom_line() + geom_point() + facet_wrap(~type, scales="free") +
  theme_bw()+ 
  theme(legend.position="bottom")
```