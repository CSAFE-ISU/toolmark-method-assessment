---
title: "Adaption of the Chumbley Score to matching of bullet striation marks"
authors:
- affiliation: Department of Statistics, Iowa State University 
  name: Ganesh Krishnan
  thanks: The authors gratefully acknowledge ...
- affiliation: Department of Statistics and CSAFE, Iowa State University 
  name: Heike Hofmann
biblio-style: apsr
output:
  pdf_document:
    citation_package: natbib
    fig_caption: yes
    keep_tex: yes
    latex_engine: pdflatex
    template: template.tex
  html_document: default
blinded: 0
keywords:
- 3 to 6 keywords
- that do not appear in the title
bibliography: bibliography
abstract: null
---


\newcommand{\hh}[1]{{\textcolor{orange}{#1}}}
\newcommand{\gk}[1]{{\textcolor{green}{#1}}}
\newcommand{\cited}[1]{{\textcolor{red}{#1}}}

\tableofcontents
\newpage
```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
library(knitr)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  comment = "",
  fig.height = 6,
  fig.width = 6,
  fig.align = "center",
  out.width= '\\textwidth',
  cache = FALSE,
  fig.path='figures/',
  echo=FALSE,
  cache=TRUE
)
options(knitr.table.format = "latex")
library(tidyverse)
library(kableExtra)
library(xtable)
library(gridExtra)
```

```{r functions}
errorrate <- function(data, alpha) {
  summ <- data %>% filter(!is.na(p_value)) %>%
    mutate(signif = p_value < alpha) %>%
    group_by(wv, wo, match, coarse, signif) %>% tally()
  summ$error <- with(summ, match != signif)
  summ$alpha <- alpha
  
  rates <- summ %>% dplyr::group_by(wv, wo, coarse, match, alpha) %>% dplyr::summarize(
    rate = n[error==TRUE]/sum(n)
  )
  totals <- summ %>% dplyr::group_by(wv, wo, coarse, alpha) %>% dplyr::summarize(
    total= sum(n[error==TRUE])/sum(n)
  )
  rates <- rates %>% spread(match,rate) %>% dplyr::rename(
    actual = `FALSE`,
    beta = `TRUE`
  )
  left_join(rates, totals)
}


# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
#  ref: http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
```

```{r data}
if (!file.exists("../data/all-sigs.rds")) {
  files <- dir("../data/signatures", pattern="csv")
  all <- data.frame()
  for (file in files) {
    tmp <- read.csv(file=file.path(path="../data/signatures", file)) 
    if (length(grep("x",names(tmp)) > 0)) tmp <- tmp %>% select(-x)
    all <- rbind(all, tmp)
  }
  all <- all %>% filter(land1_id < land2_id)
  saveRDS(all, file="../data/all-sigs.rds")
} else {
  all <- readRDS("../data/all-sigs.rds")
}

all$coarse <- 1
errors <- rbind(errorrate(all, 0.001), 
                errorrate(all, 0.005), 
                errorrate(all, 0.01), 
                errorrate(all, 0.05))
errors$type <- "signatures"

ns <- all %>% group_by(wv, wo, match) %>% summarize(
  n_ss = mean(same_shift_n),
  n_ds = mean(diff_shift_n),
  zero_ss = sum(same_shift_n==0),
  zero_ds = sum(diff_shift_n==0),
  miss = sum(is.na(p_value)),
  missperc = miss/n()*100
)
```

```{r data-profiles}
if (!file.exists("../data/all-profiles.rds")) {
  files <- dir("../data/profiles/", pattern="csv")
  allp <- data.frame()
  for (file in files) {
    tmp <- read.csv(file=file.path(path="../data/profiles", file)) 
    #print(names(tmp))
    if (length(grep("coarse",names(tmp))) == 0)
      tmp$coarse <- gsub(".*coar-(.*).csv", "\\1", file)
    if (length(grep("x",names(tmp)) > 0)) tmp <- tmp %>% select(-x)
    allp <- rbind(allp, tmp)
  }
  allp <- allp %>% filter(land1_id < land2_id)
  allp <- allp %>% mutate(coarse = as.numeric(gsub("pt", ".", coarse)))
  saveRDS(allp, file="../data/all-profiles.rds")
} else {
  allp <- readRDS("../data/all-profiles.rds")
}

errors2 <- rbind(errorrate(allp, 0.001), 
                errorrate(allp, 0.005), 
                errorrate(allp, 0.01), 
                errorrate(allp, 0.05))
errors2$type <- "profiles"

```

```{r data-sig-coars}
if (!file.exists("../data/all-sig-c.rds")) {
  files <- dir("../data/sig_diff_coarseness/", pattern="csv")
  all_sig_c <- data.frame()
  for (file in files) {
    tmp <- read.csv(file=file.path(path="../data/sig_diff_coarseness/", file)) 
    #print(names(tmp))
    if (length(grep("coarse",names(tmp))) == 0)
      tmp$coarse <- gsub(".*coarse-(.*).csv", "\\1", file)
    if (length(grep("x",names(tmp)) > 0)) tmp <- tmp %>% select(-x)
    all_sig_c <- rbind(all_sig_c, tmp)
  }
  all_sig_c <- all_sig_c %>% filter(land1_id < land2_id)
  all_sig_c <- all_sig_c %>% mutate(coarse = as.numeric(gsub("pt", ".", coarse)))
  saveRDS(all_sig_c, file="../data/all-sig-c.rds")
} else {
  all_sig_c <- readRDS("../data/all-sig-c.rds")
}

errors_sig_c <- rbind(errorrate(all_sig_c, 0.001), 
                errorrate(all_sig_c, 0.005), 
                errorrate(all_sig_c, 0.01), 
                errorrate(all_sig_c, 0.05))


```


# Introduction and Background

## Motivation
Same source analyses are a major part of an Forensic Toolmark Examiner's job. In current practice examiners  make these comparisons by visual inspection under a comparison microscope and come to one of the following four conclusions: identification, inconclusive, elimination or unsuitable for examination~\citep{afte-toolmarks1998}. These conclusions are made on the basis of "unique surface contours" of the two toolmarks being in "sufficient agreement" \citep{afte-toolmarks1998}. AFTE describes the term "sufficient agreement" as the possibility of another tool producing the markings under comparison, as practically impossible \citep{afte-toolmarks1998}. This subjectivity in the assessment as well as the lack of error rates are the main points of criticisms  first raised by the National Research Council in 2009 \citep{NAS:2009} and later emphasized further by the President's Council of Advisors on Science and Technology \citep{pcast2016}.

Technological advances, such as profilometers and confocal microscopy allow to measure 3D surfaces in a high-resolution digitized form. This technology has become more accessible over the last decade, and has made its way into topological images of ballistics evidence, such as bullet lands and breech faces \citep{DeKinder1, DeKinder2, Bachrach1, vorburger2016}.
Digitized images of 3D surfaces of form the data basis of statistical analysis of toolmarks. A statistical approach based on data  removes both subjectivity from the assessment and allows a quantification of error rates for both false positive and false negative identifications.

\hh{In the next page and a half it is easy to lose the red line. It might help to include a table with an overview.
The table should include the reference to the paper, the data used, the statistical method and the associated error rates.}
Various toolmarks have been studied in the literature:  \citet{manytoolmarks1} and \citet{chumbley} have been analyzing screwdriver marks  digitized using a profilometer;  \citet{manytoolmarks2} have investigated 3D marks from screwdriver, tongue and groove pliers captured using a confocal microscope; \citet{afte-chumbley} have been investigated digitized marks from slip-joint pliers generated by a surface profilometer. 

\hh{We need an additional sentence here to get from the data to the statistical methods ... }
\citet{manytoolmarks2} define a relative distance metric and use it as similarity measure between two toolmarks. \citet{manytoolmarks1} extract many small segments in the markings of two toolmarks and compare similiarity using a maximum pearson correlation coefficient. 
The Chumbley scoring method, first introduced by \citet{chumbley}, uses a similar but more extensive framework based on a Mann-Whitney U test of the resulting correlation coefficients. This approach is non-deterministic, because segments are chosen randomly.  \citep{hadler} make the score deterministic for each pair of toolmarks by choosing segments for comparison systematically. This approach also ensures independence between segements of striae.  In this paper, we are investigating the applicability of the Chumbley scoring method by \citet{hadler} to assess striation marks on bullet lands for same-source identification.

Striation marks on bullets are made by impurities in the barrel. As the bullet travels through the barrel, these imperfections leave "scratches" on the bullet surface. Typically, only striation marks in the land engraved areas (LEAs) are considered \citet{afte-article1992}. Bullet lands are depressed areas between the  grooves made by the rifling action of the barrel. Compared to toolmarks made by screwdrivers striation marks on bullets are typically much smaller, both in length and in width. Bullets also have a curved cross-sectional topography. Figure \ref{fig:rgl} shows us how the signature from a bullet land (bottom) lines up with the image of the land (top) from which it was extracted. We can also see in the figure how the depth and relative position of the striation markings seen in the image are interpreted as the signature.

Bullet matching methods are usually based on these associated signatures. \citet{chu2013} use an automatic method for counting consecutive matching striae (CMS). The authors report an error rate of 52% of the known same source lands comparisons as misidentified (false negative) and zero false positives for known different source lands. \citet{ma2004} and \citet{vorburger2011} discuss CCF (cross-correlation function) and its discriminating power and applicability for same-source analyses of bullets, but do not provide any error rates in their discussion. \citet{aoas} use multiple features like CCF, CMS, D (distance measure) etc in a random forest based method and compare every land against every other land of  digitised versions of Hamby 252 and Hamby 44 \citep{hamby} published on the NIST Ballistics Database \citep{nist}. The authors report an out-of-bag overall error rate of 0.46%, comprised of an error rate of 30.05% of  same-source pairs that were not identified and an error rate of 0.026% of different-source pairs that were incorrectly identified as same-source.

The Chumbley score  provides us with another approach in the same-source assessment of bullet striation marks. \citet{chumbley} compare two toolmarks for same-source. The data for this study was obtained from 50 sequentially manufactured screwdriver tips. \citet{chumbley} report error rates for markings made by the tips at different angles. For markings made at 30 degree the authors  report an average false negative error rate of 0.089  and an average false positive error rate of 0.023. For other angles of 60 and 85 degrees the false negatives error rate is 0.09 while the rate of false positives decreases to 0.01. The paper by \citet{hadler} is based on the same data but the authors focus on  markings made under the same angle. The error rates associated with the deterministic version of the score are  0.06 for false  negatives and a false positive error rate of 0.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/B6-B2-L6-rescaled.png}
```{r, eval=FALSE, echo=FALSE, fig.width=8, fig.height=3, out.width='\\textwidth'}
land <- read.csv("../data/b6b2l6.csv")
land %>% ggplot(aes(x = y, y=resid)) +geom_line() +theme_bw() +
  ylab(expression(paste("Depth (in ",mu,m,")", sep=""))) + 
  xlab(expression(paste("Relative Location (in ",mu,m,")", sep="")))
```

\caption{\label{fig:rgl} Image of a bullet land from a confocal light microscope at 20 fold magnification (top) and a chart of the corresponding signature of the same land (bottom). The dotted lines connect some peaks visible in both visualizations.}

\end{figure}

## Scans for land engraved areas

Comparisons of striae from bullets are usually based on comparisons of striae in land engraved areas, which are extracted in form of cross sections, called *profiles* \citep{aoas,ma2004}. From profiles bullet *signatures* \citep{chu2013,aoas} are extracted as residuals of a loess fit or Gaussian filter. This effectively removes topographic structure from the data in the attempt to increase the signal to noise ratio. The span of the loess fit was found using cross-validation, as described by \citet{aoas}.

\hh{don't split the discussion on the size. between the next paragraph and }
<!--The majority of Bullet profiles and signatures of the Hamby study \citep{hamby} used in this paper are extracted by procedures mentioned by  \citet{aoas}.--> 
There are two sources of scans for sets from the Hamby study available to us: scans of Hamby 44 and Hamby 252 are available from the NIST database \citep{nist}. Hamby 44 has also been made available to us and has been scanned locally for CSAFE at the Roy J.\ Carver High Resolution Microscopy Facility using a Sensofar confocal light microscope. 
Scans in the NIST database are made with a NanoFocus at 20x magnification. The resolutions of the two instruments are different: the NIST scans are taken at a resolution of 1.5625 $\mu m$ per pixel, while the CSAFE scans are available at a resolution of  0.645 $\mu m$ per pixel.
The length of an average bullet land from Hamby (9 mm Ruger P85) is about 2 millimeter, resulting in signatures of about 1200 pixels for NIST scans, and about 3000 pixels for CSAFE scans.

In comparison, scans from the profilometer used by \citet{chumbley, hadler} were taken at a resolution of about 0.73 $\mu m$ per pixel. The screw driver toolmarks are  about 7 mm in length \citep{manytoolmarks1}, for a total of over 9000 pixels for the width of these scans.

This severe limitation in the amount of available data poses the main challenge in adapting the Chumbley score to matching bullet lands, because of the resulting loss in power.



## The Chumbley Score Test


The Chumbley score algorithm takes input in form of two digitized toolmarks. The toolmark is in form of $z(t)$ which is a spatial process for location indexed by $t$. $t$ here denotes equally spaced pixel locations for the striation marks under consideration, $t = 1, ..., T$. Let further $z^s(t)$ denote a vector of markings of length $s$ starting in location  $t$. 

Let $x(t_1)$, $t_1 = 1,2,...T_1$ and $y(t_2)$, $t_2 = 1,2...T_2$ be two digitized toolmarks (where $T_1$ and $T_2$ are not necessarily equal). 
<!--The vectorized processes representing two sets of striaes are therefore shown as $x(t_1)$, $t_1 = 1,2,...T_1$ and $y(t_2)$, $t_2 = 1,2...T_2$. Here $x$ and $y$ denote two instances of the process $z(*)$ which means two striation marks, while the indexing $t$ or pixel range is shown as $t_1$ and $t_2$ for $x$ and $y$ respectively. This representation of $t$ as $t_1$ and $t_2$ lets us identify them as either of same or different lengths.--> 
The toolmarks under consideration are potentially from two different sources or the same source. $T_1$ and $T_2$, as represented above, are the final pixel indexes of each marking and therefore give the respective lengths of the markings. 

In a pre-processing step  the two markings are smoothed using a lowess \citep{lowess} with coarseness parameter $c$. 
 Originally, this smoothing is intended to remove drift and (sub)class characteristics from individual markings, however, in the setting of matching bullet striae, we can also make use of this mechanism to separate bullet curvature in profiles from signatures before matching signatures. 
 
 
After removing sub-class structure, 
the Chumbley scores is calculated in two steps: an optimization step and a validation step.
In the optimization step, the two markings are aligned horizontally such that within a pre-defined window of length $w_o$ the correlation between $x(t_1)$ and $y(t_2)$ is maximized:
\[
\left(t_1^o, t_2^o\right) = \mathop{\arg \max}\limits_{1 \le t_1 \le T_1, 1 \le t_2 \le T_2} \text{cor} \left(x^{w_o} (t_1), y^{w_o}(t_2) \right)
\]
This results in an optimal vertical (in-phase) shift of $t_1^o - t_2^o$ for aligning the two markings. \hh{We will denote the relative optimal locations as $t_1^*$ and $t_2^*$, where $t_i^* = t_i^o/(T_i-w_o)$ for $i=1,2$, such that $t_1^*, t_2^* \in [0,1]$. Once (sub-)class characteristics are removed, the relative optimal locations should be distributed according to a uniform distribution in $[0,1]$. }

In the validation step,  two sets of windows of size $w_v$ are chosen from both markings (see Figure \ref{fig:win-comparison}). In the first set, pairs of windows are extracted from the two markings using the optimal vertical shift as determined in the first step, whereas for the second set the windows are extracted using a different (out-of-phase) shift. 

More precisely, \citet{hadler} define same-shift pairs of length $w_v$ to start in $(q^{(1)}_i, q^{(2)}_i)$, where $q^{(k)}_i$ has the form
\begin{eqnarray*}
q^{(k)}_i &=& t_k^* + i w_v \text{ for } i = -1, -2, ...\\
q^{(k)}_i &=& t_k^* + w_ o + i w_v \text{ for } i = 0, 1, 2, ...
\end{eqnarray*}
for all integer values of $i$ for which both $0 <  q^{(1)}_i \le T_1 - w_v$ and $0 <  q^{(2)}_i \le T_2 - w_v$.

Similarly, different-shift pairs are based on signature segments of length $w_v$ starting in $(q^{(1)}_i, q^{(2)}_{-i})$, for all integer values of $i$ for which both $0 <  q^{(1)}_i \le T_1 - w_v$ and $0 <  q^{(2)}_{-i} \le T_2 - w_v$.



```{r win-comparison, warning=FALSE, fig.width=8, fig.height=3.5, fig.cap="Two markings made by the same source. For convenience, the markings are moved into phase on the left and out-of phase on the right. In-phase (left) and out-of-phase (right) samples are shown by the light grey background. The Chumbley-score is based on a Mann-Whitney U test of the correlations derived from these two sets of samples."}
sigs_graphics <- readRDS("../data/sigs_generate_window.rds")
sigs_graphics$sigs_land_id_1_match <- sigs_graphics$sigs_land_id_1_match %>% group_by(land_id) %>% mutate(y = y-min(y, na.rm=T))

sigs_graphics.match<- as.data.frame(sigs_graphics[1])
sigs_graphics.nonmatch<- as.data.frame(sigs_graphics[2])
colnames(sigs_graphics.match)<- gsub("^.*\\.","", colnames(sigs_graphics.match))
colnames(sigs_graphics.nonmatch)<- gsub("^.*\\.","", colnames(sigs_graphics.nonmatch))

# Matching signature
d1<- sigs_graphics.match[which(sigs_graphics.match$land_id==1,arr.ind = TRUE),]
d2<- sigs_graphics.match[which(sigs_graphics.match$land_id==276,arr.ind = TRUE),]
data1<-d1 %>%select(l30)
#data1<- data1$y
data2<- d2 %>%select(l30)
#data2<- data2$y
window_opt = 120 
window_val = 50
coarse = 1
data1<- matrix(unlist(data1))
data2<- matrix(unlist(data2))

  unity <- function(x) {x / sqrt(sum(x^2))} ## normalize columns of a matrix to make correlation computation faster
  
  ####################################################
  ##Clean the marks and compute the smooth residuals##
  ####################################################
  
  data1 <- matrix(data1[round((0.01*nrow(data1))):round(0.99*nrow(data1)),], ncol = 1)
  data2 <- matrix(data2[round((0.01*nrow(data2))):round(0.99*nrow(data2)),], ncol = 1)
  
  ##Normalize the tool marks
  y1 <- data1 - lowess(y = data1,  x = 1:nrow(data1), f= coarse)$y
  y2 <- data2 - lowess(y = data2,  x = 1:nrow(data2), f= coarse)$y
  
  ############################################
  ##Compute the observed maximum correlation##
  ############################################
  
  #####################
  ##Optimization step##
  #####################
  ##Each column in these matrices corresponds to a window in the respective tool mark
  y1_mat_opt <- matrix(NA, ncol = length(1:(length(y1) - (window_opt - 1))), nrow = window_opt)
  for(l in 1:(length(y1) - (window_opt - 1))){
    y1_mat_opt[,l] <- y1[l:(l+(window_opt - 1))]
  }
  y2_mat_opt <- matrix(NA, ncol = length(1:(length(y2) - (window_opt - 1))), nrow = window_opt)
  for(l in 1:(length(y2) - (window_opt - 1))){
    y2_mat_opt[,l] <- y2[l:(l+(window_opt - 1))]
  }
  
  ##Compute the correlation between all pairs of windows for the two marks
  ##Rows in the following matrix are mark 2, columns are mark 1
  y2_mat_opt <- apply(scale(y2_mat_opt), 2, unity)
  y1_mat_opt <- apply(scale(y1_mat_opt), 2, unity)
  corr_mat_opt <- t(y2_mat_opt) %*% y1_mat_opt ##correlation matrix
  max_corr_opt_loc <- which(corr_mat_opt == max(corr_mat_opt), arr.ind = TRUE) ##pair of windows maximizing the correlation

sigs1.min = sigs_graphics[[1]] %>% filter(land_id==1) %>% select(y) %>% min()
sigs2.min = sigs_graphics[[1]] %>% filter(land_id==276) %>% select(y) %>% min()
window_val <- 3*window_val*0.645

same.shift <- -(365-338)*1/.645 + sigs2.min - sigs1.min 
xs <- seq(100, 1800, length=10)
rects <- data.frame(ymin=-Inf, ymax= Inf, xmin = xs, xmax = xs+window_val)


p1 <- sigs_graphics[[1]] %>% filter(land_id %in% c(1, 276)) %>% 
  ggplot() + 
  geom_segment(aes(x = xmin, y=ymin, xend = xmin, yend=ymax), 
               colour="grey50", linetype=3, data = rects) +
  geom_segment(aes(x = xmax, y=ymin, xend = xmax, yend=ymax), 
               colour="grey50", linetype=3, data = rects) +
  geom_rect(aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), 
            data=rects, fill= "grey90") +
  geom_line(aes(x = y-same.shift*I(land_id==276), 
                y=l30-5*I(land_id==276), group=land_id)) + 
  theme_bw() +
  theme(axis.title.y = element_blank(), 
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  xlab(expression(paste("Relative Location (in ",mu,m,")", sep=""))) + 
  ggtitle("In-phase sample")

diff.shift <- -(365-338)*1/.645 + sigs2.min - sigs1.min + 70 

p2 <- sigs_graphics[[1]] %>% filter(land_id %in% c(1, 276)) %>% 
  ggplot() + 
  geom_segment(aes(x = xmin, y=ymin, xend = xmin, yend=ymax), 
               colour="grey50", linetype=3, data = rects) +
  geom_segment(aes(x = xmax, y=ymin, xend = xmax, yend=ymax), 
               colour="grey50", linetype=3, data = rects) +
  geom_rect(aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), 
            data=rects, fill= "grey90") +
  geom_line(aes(x = y-diff.shift*I(land_id==276), 
                y=l30-5*I(land_id==276), group=land_id)) + 
  theme_bw() +
  theme(axis.title.y = element_blank(), 
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  xlab(expression(paste("Relative Location (in ",mu,m,")", sep=""))) + 
  ggtitle("Out-of-phase sample")

grid.arrange(p1,p2, ncol=2)
```

```{r win-comparison-2, fig.width=8, fig.height=5, out.width='\\textwidth', fig.cap=" The two plots on the left show how the same shift behaves in case of a matching pair and the two plots on the right show how the different shift behaves in case of a matching pair.", eval=FALSE}

asd<- all %>% filter(wo==120, wv==50, match == TRUE, land1_id == 1)

sigs_graphics<- readRDS("../data/sigs_generate_window.rds")

sigs_graphics.match<- as.data.frame(sigs_graphics[1])
sigs_graphics.nonmatch<- as.data.frame(sigs_graphics[2])
colnames(sigs_graphics.match)<- gsub("^.*\\.","", colnames(sigs_graphics.match))
colnames(sigs_graphics.nonmatch)<- gsub("^.*\\.","", colnames(sigs_graphics.nonmatch))

# Matching signature
#gi<-
#sigs_graphics.match<- sigs_graphics.match %>% filter(land_id == c(1,9))  
#sigs_graphics.match<- sigs_graphics.match %>% select(y, l30, land_id)
d1<- sigs_graphics.match[which(sigs_graphics.match$land_id==1,arr.ind = TRUE),]
d2<- sigs_graphics.match[which(sigs_graphics.match$land_id==19,arr.ind = TRUE),]
data1<-d1 %>%select(l30)
#data1<- data1$y
data2<- d2 %>%select(l30)
#data2<- data2$y
window_opt = 120 
window_val = 50
coarse = 1
data1<- matrix(unlist(data1))
data2<- matrix(unlist(data2))

  unity <- function(x) {x / sqrt(sum(x^2))} ## normalize columns of a matrix to make correlation computation faster
  
  ####################################################
  ##Clean the marks and compute the smooth residuals##
  ####################################################
  
  data1 <- matrix(data1[round((0.01*nrow(data1))):round(0.99*nrow(data1)),], ncol = 1)
  data2 <- matrix(data2[round((0.01*nrow(data2))):round(0.99*nrow(data2)),], ncol = 1)
  
  ##Normalize the tool marks
  y1 <- data1 - lowess(y = data1,  x = 1:nrow(data1), f= coarse)$y
  y2 <- data2 - lowess(y = data2,  x = 1:nrow(data2), f= coarse)$y
  
  
  ############################################
  ##Compute the observed maximum correlation##
  ############################################
  
  #####################
  ##Optimization step##
  #####################
  ##Each column in these matrices corresponds to a window in the respective tool mark
  y1_mat_opt <- matrix(NA, ncol = length(1:(length(y1) - (window_opt - 1))), nrow = window_opt)
  for(l in 1:(length(y1) - (window_opt - 1))){
    y1_mat_opt[,l] <- y1[l:(l+(window_opt - 1))]
  }
  y2_mat_opt <- matrix(NA, ncol = length(1:(length(y2) - (window_opt - 1))), nrow = window_opt)
  for(l in 1:(length(y2) - (window_opt - 1))){
    y2_mat_opt[,l] <- y2[l:(l+(window_opt - 1))]
  }
  
  ##Compute the correlation between all pairs of windows for the two marks
  ##Rows in the following matrix are mark 2, columns are mark 1
  y2_mat_opt <- apply(scale(y2_mat_opt), 2, unity)
  y1_mat_opt <- apply(scale(y1_mat_opt), 2, unity)
  corr_mat_opt <- t(y2_mat_opt) %*% y1_mat_opt ##correlation matrix
  max_corr_opt_loc <- which(corr_mat_opt == max(corr_mat_opt), arr.ind = TRUE) ##pair of windows maximizing the correlation
  

  s1.rect.opt <- data.frame(xmin=max_corr_opt_loc[1,2], xmax= max_corr_opt_loc[1,2]+window_opt, ymin=-Inf, ymax=Inf)
    s1.rect.sameshift1 <- data.frame(xmin=max_corr_opt_loc[1,2]-window_val, xmax= max_corr_opt_loc[1,2]- 2*window_val, ymin=-Inf, ymax=Inf)
       s1.rect.sameshift2 <- data.frame(xmin=max_corr_opt_loc[1,2]+window_opt+ window_val, xmax= max_corr_opt_loc[1,2]+ window_opt+2*window_val, ymin=-Inf, ymax=Inf)
       
  s2.rect.opt <- data.frame(xmin=max_corr_opt_loc[1,1], xmax= max_corr_opt_loc[1,1]-window_opt, ymin=-Inf, ymax=Inf)
    s2.rect.sameshift1 <- data.frame(xmin=max_corr_opt_loc[1,1]-window_opt-window_val, xmax= max_corr_opt_loc[1,1]-window_opt- 2*window_val, ymin=-Inf, ymax=Inf)
       s2.rect.sameshift2 <- data.frame(xmin=max_corr_opt_loc[1,1]+ window_val, xmax= max_corr_opt_loc[1,1]+ 2*window_val, ymin=-Inf, ymax=Inf)
       
p1<-  ggplot(data = d1, aes(x = y, y = l30)) + geom_line() + 
  geom_rect(data=s1.rect.opt, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "black",alpha=0.5, inherit.aes = FALSE)+
    geom_rect(data=s1.rect.sameshift1, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "red",alpha=0.1, inherit.aes = FALSE)+
    geom_rect(data=s1.rect.sameshift2, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "blue",alpha=0.1, inherit.aes = FALSE)+
  theme_bw() + labs(y = "Signature Land Id = 1")
  
 p2<-   ggplot(data = d2, aes(x = y, y = l30)) + geom_line() + 
   geom_rect(data=s2.rect.opt, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "black",alpha=0.5, inherit.aes = FALSE)+
    geom_rect(data=s2.rect.sameshift1, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "red",alpha=0.1, inherit.aes = FALSE)+
    geom_rect(data=s2.rect.sameshift2, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "blue",alpha=0.1, inherit.aes = FALSE)+
  theme_bw() + labs(y = "Signature Land Id = 9")
 ##########
 
 d1.rect.opt <- data.frame(xmin=max_corr_opt_loc[1,2], xmax= max_corr_opt_loc[1,2]+window_opt, ymin=-Inf, ymax=Inf)
    d1.rect.sameshift1 <- data.frame(xmin=max_corr_opt_loc[1,2]-2*window_opt, xmax= max_corr_opt_loc[1,2]-2*window_opt-window_val, ymin=-Inf, ymax=Inf)
       d1.rect.sameshift2 <- data.frame(xmin=max_corr_opt_loc[1,2]+window_opt+ window_val, xmax= max_corr_opt_loc[1,2]+ window_opt+2*window_val, ymin=-Inf, ymax=Inf)
       
  d2.rect.opt <- data.frame(xmin=max_corr_opt_loc[1,1], xmax= max_corr_opt_loc[1,1]-window_opt, ymin=-Inf, ymax=Inf)
    d2.rect.sameshift1 <- data.frame(xmin=max_corr_opt_loc[1,1]-window_opt-window_val, xmax= max_corr_opt_loc[1,1]-window_opt- 2*window_val, ymin=-Inf, ymax=Inf)
       d2.rect.sameshift2 <- data.frame(xmin=max_corr_opt_loc[1,1]+ 4*window_val, xmax= max_corr_opt_loc[1,1]+ 5*window_val, ymin=-Inf, ymax=Inf)
       
p3<-  ggplot(data = d1, aes(x = y, y = l30)) + geom_line() + 
  geom_rect(data=d1.rect.opt, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "black",alpha=0.5, inherit.aes = FALSE)+
    geom_rect(data=d1.rect.sameshift1, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "red",alpha=0.1, inherit.aes = FALSE)+
    geom_rect(data=d1.rect.sameshift2, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "blue",alpha=0.1, inherit.aes = FALSE)+
  theme_bw() + labs(y = "Signature Land Id = 1")
  
 p4<-   ggplot(data = d2, aes(x = y, y = l30)) + geom_line() + 
   geom_rect(data=d2.rect.opt, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "black",alpha=0.5, inherit.aes = FALSE)+
    geom_rect(data=d2.rect.sameshift1, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "red",alpha=0.1, inherit.aes = FALSE)+
    geom_rect(data=d2.rect.sameshift2, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "blue",alpha=0.1, inherit.aes = FALSE)+
  theme_bw() + labs(y = "Signature Land Id = 9")
  

multiplot(p1, p2, p3, p4, cols=2)
```


For both samples the correlations between the pairs of markings is then calculated. 
The intuition here is that for two markings from the same source the correlation for the in-phase sample should be high, while the correlations of the out-of-phase sample  provide a measure for the base-level correlation for non-matching marks of a given length $w_v$. The Chumbley score is then computed as a Mann Whitney U statistic to compare between in-phase sample and out-of-phase sample. 
In the original method proposed in \citet{chumbley} both in-phase and out-of-phase sample are extracted randomly, whereas \citet{hadler} proposed deterministic rules for both samples to make the resulting score deterministic while simultaneously avoiding overlaps within selected marks to ensure independence.



<!--
The similarity in the two markings is then judged by the algorithm on the basis of cross-correlation of a fixed and constant number consecutive pixels (say $k$) taken from the two markings. This can for example be $k$ taken from the two indexed marking $x(t_1)$ and $y(t_2)$ such that in theory $k$ remains smaller than length of the two striae or marks. Depending on what stage of the algorithm we are in, matching of different pixel lengths and locations is done. This in the end, effectively compares all possible windows that would guarantee in quantifying the two marks or striae as coming from the same source or not. -->

<!-- The word 'pixel' refers to the resolution of the confocal light microscope. In the case for bullet signatures extracted from NIST ballistics database \citep{nist} of the Hamby study, a pixel corresponds to approximately 1.56 microns. -->

<!--
The algorithm works in two phases, namely, an optimization step and a validation step, at the end of which a Mann Whitney U statistic is calculated as a measure to differentiate between matches and non-matches. A pre-processing step to the algorithm is to choose a coarseness value which  is used as a parameter to the lowess smoothing function. The coarseness essentially gives the proportion of points which influence the smooth at each value, which means larger values lead to more smoothness. The lowess smoothing is applied to each of two sets of vectorized striae or marks $x(t_1)$ and $y(t_2)$, before proceeding to the algorithm.-->
<!-- The algorithm starts with the optimization step where the area of best agreement in the two markings being compared is identified. Comparison window size is predefined. Each window of the first marking is compared with all windows of the second marking. A maximum correlation statistic is used to identify the region of best agreement, with the maximum usually seen being near 1 for both cases which is what is intuitively expected for matches, and not expected intuitively for non matches. 
\citet{hadler} in their paper proposed an improvement to this algorithm by trying to remove mutual dependence of parameters. The mututal dependence was due to the serial correlation in surface depth values of a marking. Also a random sampling sub step in the validation phase makes a group of pixels to be chosen more than once and hence introduces lack of independence in certain steps. The reason for removing the mutual dependence was the Mann-Whitney U statistic works under the assumption of independence of parameters.-->
<!--
The validation step builds on this with two sub-steps namely, same shift and different shift. As the aim is to come up with a non-parameteric U statistic, \citet{hadler} proposed a normalization procedure in the validation step which goes to some extent to address the issue of mutual dependence. A series of windows are chosen in both the same and different shift sub-steps for the purpose of comparison. Both substeps were modified by \citet{hadler} who introduced a deterministic rule for sampling of windows in the same-shift and different shift as opposed to the originally proposed random sampling by \citet{chumbley}.
In th same shift substep the chosen series of windows are at a common distance (rigid-shifts) from the window earlier identified as the region of best agreement in the optimization step. The correlation of these windows generally turn out to be lower than the maximum correlation window. The significance is that these same shift windows still have large enough correlation values for the two markings being compared that are in reality a match. Any choice of maximum correlation windows (where correlation values are often near 1) for non-matches in the optimization step are also validated by this sub-step. This is because the same-shift correlations would not be anywhere as large for same-shift windows when all windows in the sub-step are compared.
The different shift substep on the other hand gives perspective to the correlation values of the same shift window correlation values. There are no rigid-shifts but different shifts where distance from the maximum correlation window are chosen randomly by \citet{chumbley} and deterministically as per \citet{hadler}. This means that there is an equal possibilty of comparing a trace segment from one marking to any trace segment or window in the second marking.

Neither of above sets of correlation in the two sub-steps are allowed to include the maximum correlation window as identified earlier. Therefore the assumption is that if two markings match each other, the same-shift correlations would be larger than the different-shift windows.
And if they are not a match the correlations in the two sets will be very similar. The U-statistic tests for the null hypothesis that the two markings are not a match and are therefore not made by the same source.  As given by \citet{hadler}, it is computed from the joint rank of all correlations of both the same and different shift samples. 
-->
<!--
##################

#### endedit

\hh{end of intro: remaining paper is structured as follows: introduce to the data we get from confocal microscopy, introduce to profiles and signatures.
Introduce to chumbley score method, apply chumbley and discuss results ....
}

## Scans for land engraved areas

- scans available: NIST database (citation), Hamby 44 and Hamby 252 (Hamby citation)
- move figure up
- discuss cross section, profiles and signatures
-->

<!-- Compairing pairs of toolmarks with the intention of matching it to a tool has been studied many times in the past. Extensive examples can be found in literature for tools and toolmark research ranging from screwdrivers \citep{manytoolmarks1, manytoolmarks2, chumbley} to groove pliers \citep{manytoolmarks2} to slip-joint pliers \citep{afte-chumbley} and many more. In comparison to this, same source matching of bullets to firearms has not been examined as prominently as that of toolmarks. Even less information is available on validity of methods and error rates associated with firearms examination. The National Academy of Sciences in its report in 2009 \citep{NAS:2009} discussed the need for determining error rates in methods proposed for firearms examination. As seen in the case of most forensic applications, the first step for same source matching and error rate determination involves identification of unique features that are characteristic of the object at hand. For the case of bullets and firearms, striation marks on the surface of the bullet are considered to be such markings that can be used in methods for same source matching. These marks are often a product of rifling and impurities and defects due to manufacturing in the barrel of the gun, which leads to engravings on the bullet surface \citep{afte-article1992}. In current practice, firearm examiners invariably make visual comparisons of bullet striae and use visual assesment tools to dignify bullets as being matches and non-matches. One way of accomplishing anykind of comparison between bullets is to do a comparison between surface marking of two or more bullet lands. Bullet Lands are considered to be areas between grooves made by the rifling action of the barrel and the markings on them are considered to be unique. The land engraved markings or sometimes termed as Bullet profiles \citep{aoas,ma2004} are striation marks made on Bullet lands and often used for these land to land comparisons. Bullet Signatures is another word used in literature as seen in the work of \citet{chu2013} and \citet{aoas}. In our context bullet signatures refer to a processed version of the raw land engraved markings or profiles. The generation of bullet signatures involves first extraction of a bullet profile by taking the cross-sectional of the surface at a given height and then using loess fits to model the structure. The residuals of this fit are called signatures, which are considered to be noise free and a good reflection of the class charecteristics and unique features of a bullet. A more detailed version of the extraction technique of signatures is discussed by \citet{aoas}, where comprehensive details about the height at which profile is to be selected, removing curvature, smoothing, identifying groove locations are explained. Figure \ref{fig:rgl} shows us how the extracted signature from a bullet land lines up with the image of the land from which it was extracted. We can see also see in the figure how the depth and relative position of the striation markings seen in the image are interpreted as the signature. -->

<!-- As mentioned earlier, subject bias and error rate determination have been a long standing issue in firearm examination \citep{NAS:2009}. This issue of removing subjectivity and making firearm analysis objective  was also raised in the \citet{pcast2016} report. For an objective analysis, scientific principle testability and error rate determination are considered to be defining aspects.  Identification of the uncertainities associated with a method of comparison and coming up with a systemized method of quantification, is therefore important. Associating mathematical probabilites in a systematic manner to these methods are one such way of quantification.  -->
<!-- In a previous study conducted by \citet{aoas} a machine learning based algorithm was developed for same source matching of bullets and error rates were discussed using the database from the Hamby Study \citep{hamby}. The method proposed by \citet{chumbley} (and later improved by \citet{hadler}) also provides a means to determine error rates and claims to reduce subject bias. Therefore, giving us a strong motivation to explore the adaptibility of the Chumbley score methodology to bullets. In this paper, we take this toolmark based methodology and try to apply and adapt it to bullets. After this, we consequently discuss about the efforts in doing so, along with the associated error rates. The data used in this paper also belongs to the Hamby Study \citep{hamby}. This gives us a common platform for comparing the performance of the chumbley method on bullets with the already existing method proposed by \citet{aoas} for bullets. As a brief overview, \citet{chumbley}, in their paper, compare two toolmarks with the intention of determining if it comes from the same tool.  They use an empirical based setup to validate their proposed algorithm and quantitative method which calculates a U-statisic for the purpose of classification of toolmarks as matching or non-matching. The data for their study was obtained from 50 sequentially manufactured screwdriver tips, and preselected comparison window sizes were given as inputs to the algorithm. The algorithm then compares the two toolmarks and comes up with a Mann-Whitney U-statistic and an associated p-value to designate them as matches or non-matches. The performance for every 100 comparisons, of the algorithm proposed by \citet{chumbley} and the improvement proposed by \citet{hadler} are listed in the table below. -->


<!-- ```{r, results='asis', include=FALSE} -->
<!-- library(knitr) -->
<!-- library(kableExtra) -->
<!-- library(xtable) -->

<!-- a1<- matrix(NA, ncol = 3, nrow = 2) -->
<!-- toolmark1.confusion<- data.frame(a1) -->
<!-- colnames(toolmark1.confusion)<- c("Classification", "Match", "Non-Match") -->
<!-- toolmark1.confusion$Classification<- c("Match", "Non-Match") -->
<!-- # toolmark1.confusion$Match<- c(41,2) -->
<!-- # toolmark1.confusion$`Non-Match`<- c(9,48) -->
<!-- toolmark1.confusion$Match<- c(91,2) -->
<!-- toolmark1.confusion$`Non-Match`<- c(9,98) -->

<!-- a2<- matrix(NA, ncol = 3, nrow = 2) -->
<!-- toolmark2.confusion<- data.frame(a2) -->
<!-- colnames(toolmark2.confusion)<- c("Classification", "Match", "Non-Match") -->
<!-- toolmark2.confusion$Classification<- c("Match", "Non-Match") -->
<!-- toolmark2.confusion$Match<- c(47,0) -->
<!-- toolmark2.confusion$`Non-Match`<- c(3,50) -->

<!-- chumbley.confusion<-knitr::kable(toolmark1.confusion, digits=0, format = "latex" ,booktabs=TRUE) #%>% kableExtra::kable_styling(latex_options = c("hold")) -->
<!-- hadler.confusion<-knitr::kable(toolmark2.confusion, digits=0, format = "latex" ,booktabs=TRUE) #%>% kableExtra::kable_styling(latex_options = c("hold")) -->



<!-- cat(c("\\begin{table}[!htb] -->
<!--     \\begin{minipage}{.5\\linewidth} -->
<!--       \\caption{Chumbley et al. 2010} -->
<!--       \\centering", -->
<!--         chumbley.confusion, -->
<!--     "\\end{minipage}% -->
<!--     \\begin{minipage}{.5\\linewidth} -->
<!--       \\centering -->
<!--         \\caption{Hadler et. al. (2017)}", -->
<!--        hadler.confusion, -->
<!--     "\\end{minipage}  -->
<!-- \\end{table}" -->
<!-- )) -->

<!-- ``` -->
<!-- Striation marks are used to determine same source for bullets. Current standards ask for a visual inspection of the marks under a comparison microscope by a firearms examiner. One of the problems raised by the NAS report in 2009 is the subjectivity involved in this comparison and the need for determining error rates \citet{NAS:2009}. The issue of subjectivity of a firearms examiner has been reviewed by many authors where things like scientific principle testability and error rates are considered to be defining aspects for an objective analysis such that without identifying the uncertainities associated with a method of comparison, it is inconclusive to regards an analysis complete. This means that with the usual method of visual comparison that is adopted by firearms examiners, lies the problem of lack of any systemized quantification and as such there seems no certain way of associating it with any kind of mathematical probability. Since determining error rates has been duly noted as a fundamental problem in forensic science \citet{NAS:2009}, there is a need of methods that address this problem. Some of the methods that have been used to estimate error rates in firearm examinations include Automatic cartridge case comparison where same source identification of cartridge cases and associated error rates have been provided by \citet{riva}, although as noted by \citet{aoas}, in this case alignments of striae involves roation of planes, which cannot be generalized for bullets. In case of methods that use machine learning algorithms some methods which are bootsrap based methods often tend to give over-estimated error rates \citet{efron}, but there are alternate error estimation techniques as described by \citet{aoas}, \citet{efron} and  \citet{vorburger2016} which give better results. -->

<!-- In case of methods that do not use machine learning algorithm, matching two striation marks with each other in order to identify if it is from the same source, can also be done using a well defined comparative statistical algorithm. The statistical algorithm is therefore expected  to go through a step by step procedure of identifying the class characteristics i.e striation marks for bullets, choosing the striae, and then follow a systematic procedure of comparing two striae with each other on the basis of criteria like cross-correlation (or others). Such a methodology would make it possible to determine error rates too and give definite results regarding what proportion of cases would the analysis be successful in identifying a match correctly and in how many cases would it end up being a false positive. -->

<!-- Identification criteria like consecutively matching striae (CMS) as first seen by Biasotti 1959 \citet{biasotti} and later mentioned in \citet{chu2013} more often than not include subject bias and are error prone and as seen in the work of \citet{miller} the number of CMS may turn out to be high even when the bullet is not fired by the same firearm. Parameters like cross-correlation factor as seen in the extensive comparisons made by \citep{aoas} seem to perform much better for matching purposes. -->

<!-- An important aspect of same source matching in firearms is identification of bullet microtopographies that are unique to be chosen for a match, Bullet profiles and signatures prove to be such features. The choice of a bullet signature to uniquely define a bullet depends on finding first, a stable region on a Bullet land which minimal noise and many pronounced striation marks. \citet{aoas} -->

<!-- The method employed by \citet{aoas} uses the cross correlation factor as a means to identify the stable region. The markings or striae in this region are supposed to have a high CCF with each other. Therefore a Bullet profile is chosen from this region by taking the cross section at a given height. A loess fit on the bullet profile produces  residuals which are termed as signatures and can be used as a unique identifier that can be used while matching two bullets to a firearm. -->

<!-- Compairing pairs of toolmarks on the otherhand, with the intention of matching it to a tool has been studied relatively more in the past as compared to bullets, and \citet{chumbley} have described in their paper an algorithm and analytic method that compares two toolmarks and come to the conclusion if they are from the same tool or not. The method also determines the error rates, reduces subject bias and designate the two toolmarks as matches or non-matches with respect to a source. \citet{chumbley} used an empirical based setup to validate their proposed analytic and quantitative algorithm. -->

<!-- \citet{chumbley} found that the algorithm gives false-positives in slightly over 2 cases (or 2%) of the time and about 9 false-negatives (or 9%) for every 100 comparisons. \citet{hadler} on the other hand, in the improved version of the algorithm first described by \citet{chumbley} found false-positives for 0 cases and 3 false-negatives (or 6%) for 50 comparisons of knwon matches. -->
<!-- ```{r} -->
<!-- a<- matrix(NA, ncol = 3, nrow = 2) -->
<!-- toolmark.falsepositive<- data.frame(a) -->
<!-- colnames(toolmark.falsepositive)<- c("Classification", "Match", "Non-Match") -->
<!-- toolmark.falsepositive$Classification<- c("Match", "Non-Match") -->
<!-- toolmark.falsepositive$Match<- c(47,0) -->
<!-- toolmark.falsepositive$`Non-Match`<- c(3,50) -->

<!-- knitr::kable(toolmark.falsepositive, digits=0, format = "latex" ,booktabs=TRUE) %>% kableExtra::kable_styling(latex_options = c("hold")) -->
<!-- ``` -->

<!-- ## Potential limitations of the Chumbley Score adaptation to bullets -->
<!-- Bullet striations are different from toolmarks. Bullet markings are typically produced by the barrel on the bullets, while toolmarks can be made by tool tips on different surfaces. Tools like screw driver tips as used by \citet{chumbley}, produce longer and pronounced markings. Bullets on the other hand are much smaller in length, width, and curved in the cross-sectional topography. This means the markings made by barrels as opposed to markings made by tools, may have a problem in distinctiveness. The majority of Bullet profiles and signatures of the Hamby study \citep{hamby} used in this paper are extracted by procedures mentioned by  \citet{aoas}. The Ruger pistol barrels used in the  Hamby study have a 9mm caliber. This means that bullets used in these pistols have land markings of almost $(1/6)^{th}$ the size of screwdriver toolmarks used by \citet{chumbley}.  Also, striations on bullets are made on their curved surfaces, whereas the algorithm developed by \citet{chumbley} and \citet{hadler} has only been tested for flatter and wider surfaces which have negligible curvature. Therefore, using methods proposed for toolmarks may need adaptation in order to give tangible results for bullets. Moreover, in order to get flat bullet signatures and remove the curvatures some kind of smoothing needs to be applied as a pre-step wihch needs further investigation as to whether the level of smoothing does effect the working of the algorithm on Bullets. -->

<!-- The Chumbley method and algorithm works on the premise of comparison windows which are supposed to represent small segments of pixels of equal sizes in the two markings that are to be compared in a predetermined fashion. They term them as windows of optimization and validation. The optimization window is selected in the first step called the optimization step where areas/windows of best agreement between the the two markings are found. The window of optimization for bullet striations are shorter as bullet signatures are smaller compared to toolmarks . The idea is to keep the number of windows of optimization sufficiently large. This means we have shorter trace segments that lets us compare smaller segments of one signature to another than those for toolmarks. Here, trace segments are the partitions of marking with the length of the partition equal to the size of window of optimization. This introduces a problem as, if we go too small in the window of optimization, the unique features of the trace segments are lost and seem similar, while too large sizes vastly reduces the weight of small features that would otherwise uniquely classify a signature and hence identify the region of agreement. The problem of distinctiveness in the structure of bullet striations may also lead to potential failure of the algorithm to identify correct windows of agreements or not be able to identify these windows at all. This may lead to incorrect classification of bullets as matches or non-matches. -->

<!-- Thus, the comaparison windows have a direct influence on the false positives and false negatives, making identification of the optimum sizes of these windows very important. This raises important questions about how and what parameter settings need to be chosen for bullet land comparison. Something similar is done in \citet{afte-chumbley} for toolmark comparisons of slip-joint pliers where optimum window sizes are determined. There is a need to figure out the best parameter settings which minimizes the errors for unsmoothed markings or profiles and pre-processed signatures. An analysis of these error rates and comparison with other methods will help us understand the adaptibility of the chumbley score to bullets.   -->




<!-- ## The Chumbley Score Test -->

<!-- The Chumbley score algorithm takes input as two vectorized processes.  Here the processes are of the form $z(t)$ which is a spatial process for some location indexed with $t$. This means that while $z(t)$ represents any striae, $z(t_1)$ is a realization of the spatial process. $t$ here denotes a vector of equally spaced pixel locations for the striation marks under consideration. The word 'pixel' refers to the resolution of the confocal light microscope. In the case for bullet signatures extracted from NIST ballistics database \citep{nist} of the Hamby study, a pixel corresponds to approximately 1.56 microns. The vectorized processes representing two sets of striaes are therefore shown as $x(t_1)$, $t_1 = 1,2,...T_1$ and $y(t_2)$, $t_2 = 1,2...T_2$. Here $x$ and $y$ denote two instances of the process $z(*)$ which means two striation marks, while the indexing $t$ or pixel range is shown as $t_1$ and $t_2$ for $x$ and $y$ respectively. This representation of $t$ as $t_1$ and $t_2$ lets us identify them as either of same or different lengths. The striation marks under consideration are potentially from two different bullets whose source needs to be identified as being same or different. $T_1$ and $T_2$, as represented above, are the final pixel indexes of each marking and therefore give the respective lengths of the markings. The similarity in the two markings is then judged by the algorithm on the basis of cross-correlation of a fixed and constant number consecutive pixels (say $k$) taken from the two markings. This can for example be $k$ taken from the two indexed marking $x(t_1)$ and $y(t_2)$ such that in theory $k$ remains smaller than length of the two striae or marks. Depending on what stage of the algorithm we are in, matching of different pixel lengths and locations is done. This in the end, effectively compares all possible windows that would guarantee in quantifying the two marks or striae as coming from the same source or not. -->

<!-- The algorithm works in two phases, namely, an optimization step and a validation step, at the end of which a Mann Whitney U statistic is calculated as a measure to differentiate between matches and non-matches. A pre-processing step to the algorithm is to choose a coarseness value which  is used as a parameter to the lowess smoothing function. The coarseness essentially gives the proportion of points which influence the smooth at each value, which means larger values lead to more smoothness. The lowess smoothing is applied to each of two sets of vectorized striae or marks $x(t_1)$ and $y(t_2)$, before proceeding to the algorithm. -->
<!-- The algorithm starts with the optimization step where the area of best agreement in the two markings being compared is identified. Comparison window size is predefined. Each window of the first marking is compared with all windows of the second marking. A maximum correlation statistic is used to identify the region of best agreement, with the maximum usually seen being near 1 for both cases which is what is intuitively expected for matches, and not expected intuitively for non matches.  -->
<!-- \citet{hadler} in their paper proposed an improvement to this algorithm by trying to remove mutual dependence of parameters. The mututal dependence was due to the serial correlation in surface depth values of a marking. Also a random sampling sub step in the validation phase makes a group of pixels to be chosen more than once and hence introduces lack of independence in certain steps. The reason for removing the mutual dependence was the Mann-Whitney U statistic works under the assumption of independence of parameters. -->
<!-- The validation step builds on this with two sub-steps namely, same shift and different shift. As the aim is to come up with a non-parameteric U statistic, \citet{hadler} proposed a normalization procedure in the validation step which goes to some extent to address the issue of mutual dependence. A series of windows are chosen in both the same and different shift sub-steps for the purpose of comparison. Both substeps were modified by \citet{hadler} who introduced a deterministic rule for sampling of windows in the same-shift and different shift as opposed to the originally proposed random sampling by \citet{chumbley}. -->
<!-- In th same shift substep the chosen series of windows are at a common distance (rigid-shifts) from the window earlier identified as the region of best agreement in the optimization step. The correlation of these windows generally turn out to be lower than the maximum correlation window. The significance is that these same shift windows still have large enough correlation values for the two markings being compared that are in reality a match. Any choice of maximum correlation windows (where correlation values are often near 1) for non-matches in the optimization step are also validated by this sub-step. This is because the same-shift correlations would not be anywhere as large for same-shift windows when all windows in the sub-step are compared. -->
<!-- The different shift substep on the other hand gives perspective to the correlation values of the same shift window correlation values. There are no rigid-shifts but different shifts where distance from the maximum correlation window are chosen randomly by \citet{chumbley} and deterministically as per \citet{hadler}. This means that there is an equal possibilty of comparing a trace segment from one marking to any trace segment or window in the second marking. -->

<!-- Neither of above sets of correlation in the two sub-steps are allowed to include the maximum correlation window as identified earlier. Therefore the assumption is that if two markings match each other, the same-shift correlations would be larger than the different-shift windows. -->
<!-- And if they are not a match the correlations in the two sets will be very similar. The U-statistic tests for the null hypothesis that the two markings are not a match and are therefore not made by the same source.  As given by \citet{hadler}, it is computed from the joint rank of all correlations of both the same and different shift samples.  -->

<!-- The data that is used by \citet{chumbley} is generated by a surface profilometer that gives the height in terms of distance along a linear trace. This is taken perpendicular to the striations present in the toolmark. Two such trace are then compared using the algorithm. -->

<!-- ## Detailed algorithm -->

<!-- ###Optimization step -->
<!-- The idea behind this step is to first identify the area of best agreement in the two toolmark data. Comparison window size is predefined by the user, which in case of screwdriver toolmarks was chosen by Chumbley et al and Hadler et al as around 10 percent of the length of the toolmarks, which was around 500. This window is henceforth referred to as Window of optimization. -->

<!-- A maximum correlation statistic is used to identify the region of best agreement, with the maximum usually seen being near 1 for both cases which is what we intuitively expect for matches, and something which we do not intuitively expect for non matches  -->

<!-- First, there are a very large number of cross-correlations calculated for the two series of striae or marks $x(t_1)$ and $y(t_2)$, for eg if the window of optimization was defined as 200 and toolmark pixel length is around 1000 then we have 1200-(200-1) = 1001.  -->

<!-- This is the number of windows we have for one toolmark and each window is compared with all windows of the second toolmark (the number of windows is again similar), and the window with the maximum correlation is identified to be the region where the toolmarks are in maximum agreement with each other -->


<!-- ###Validation Step -->

<!-- ####Same-shift:  -->
<!-- In this step a series of windows are chosen at random (originally by \citet{chumbley}) and deterministically (by \citet{hadler}), but at a common distance (rigid-shift) from the window identified as the region of best agreement in the vaildation step. -->
<!-- The correlation of these windows would be as intuitively assumed i.e. lower than the maximum correlation window (Optimization step), but the significance is that these same shift windows will still have large enough correlation values for two toolmarks or signatures that are in reality a match. -->

<!-- This also validates that if in the optimization step the maximum correlation window chosen (with correlation value near 1) was by accident (like in case of signatures that in reality are a non-match), all these same- shift correlations (A fixed number of these trace segments are identified) would not be anywhere large enough for all same-shift windows. -->




<!-- ####Different Shift: -->
<!-- Primary reason for this substep is to give perspective to the correlation values of the same shift window correlation values. -->

<!-- This time there are no rigid-shifts but different shifts (distance from Window of Opt with max correlation) chosen randomly by \citet{chumbley} and deterministically by Hadler et al. such that there is an equal possibilty of comparing a trace segment from one signature or toolmark to any one in the second signature or toolamrk. -->

<!-- Neither of above sets of correlation are allowed to include the maximum correlation window as identified earlier. -->

<!-- Therefore the assumption is that if two toolmarks or signatures match each other the same-shift correlations would be larger than the different-shift windows, -->
<!-- and if they are not a match the correlations in the two sets will be very similar. -->

<!-- ###U Statistic: -->

<!-- This is computed from the joint rank of all correlations of both the same and different shift samples. As given by \citet{hadler} -->

<!-- Null Hypothesis: If the toolmarks were not match i.e not made by the same tool. -->

<!-- Let ns and nd be the number of same shift and different shift windows -->
<!-- $$N = n_{s} + n_{d}$$ -->

<!-- The mann whitney U statistic is given by -->
<!-- $$U =\sum^{ns}_{i=1}R_{s}\left( i\right)$$ -->

<!-- with the standardized version which includes provision for rank ties -->

<!-- $$\overline{U}= \dfrac{U-M}{\sqrt{V}}$$ -->

<!-- where prior to normalization the U-statistic has the mean as -->

<!-- $$M = n_{s}\left(\dfrac{N+1}{2}\right)$$ -->

<!-- and variance  -->

<!-- $$V = \dfrac{n_{s}n_{d}}{N\left(N-1\right)}\left[\Sigma^{n_{s}}R_{s}\left(i\right)^{2}+\Sigma^{n_{d}}R_{d}\left(j\right)^{2}\right] -\dfrac{n_{s} n_{d}\left(N+1\right)^{2}}{4\left(N-1\right)}$$ -->


# Testing setup

## The Data

\hh{Introduce profiles and signatures, as shown in figure \ref{fig:sigs-profiles}.}

```{r sigs-profiles, fig.width = 8, fig.height = 2.5, fig.cap="Bullet land profile (left) and the corresponding signature (right) for one of the lands of Hamby-44."}
p1 <- sigs_graphics[[1]] %>% filter(land_id==19) %>%
  ggplot(aes(x = y, y = value)) + geom_line() +
  theme_bw() +
  ylab(expression(paste("Depth (in ",mu,m,")", sep=""))) + 
  xlab(expression(paste("Relative Location (in ",mu,m,")", sep=""))) + 
  ggtitle("Profile")

p2 <- sigs_graphics[[1]] %>% filter(land_id==19) %>%
  ggplot(aes(x = y, y = l30)) + 
  geom_line(aes(y=resid), size=.25) +
  geom_line(aes(y=l30)) +
  theme_bw() +
  ylab(expression(paste("Depth (in ",mu,m,")", sep=""))) + 
  xlab(expression(paste("Relative Location (in ",mu,m,")", sep=""))) + 
  ggtitle("Signature")

grid.arrange(p1,p2, ncol=2)
```

Lands for all Hamby-44 and Hamby-252 scans are made available through the NIST ballistics database \citep{nist} and are considered, here. Both of these sets of scans are part of the larger Hamby study \citep{hamby}. Each set consists of twenty known bullets (two each from ten consecutively rifled Ruger P85 barrels) and fifteen questioned bullets (each matching one of the ten barrels). Ground truth for both of these Hamby sets is known and was used to assess correctness of the tests results. 

\hh{Discuss CSAFE scans}

Profiles and signatures were extracted from all scans as described in \citet{aoas}.


## Setup
\hh{XXX the description below needs to be updated.}

\hh{using (a) signatures and (b) profiles, run chumbley score across scans from NIST and CSAFE for various settings of $w_o$ and $w_v$ (and coarseness $c$ for profiles).}


We used the adjusted Chumbley method as proposed in \citet{hadler} and implemented in the R package `toolmaRk` \citep{toolmark} on all pairwise land-to-land comparisons of the Hamby scans provided by NIST for a total of 85,491 land-to-land comparisons. 

<!--The settings for optimizing and validating window sizes, $w_o$ and $w_v$, ranged from $w_o \in [50, 280]$ and $w_v \in \{30, 50\}$, see also figure \ref{fig:type2}.-->

## Failed Tests

Initially, the default settings suggested in \citet{hadler} were used: $w_o = 120$ pixels or about 190 $\mu m$ (ten percent of the average length of profiles) and coarseness $c = 0.25$. \autoref{fig:fails} shows the percentage of failed tests among the 86k land-to-land comparisons of the NIST data for different values of the validation window size $w_v$. For same-source lands up to 12.5 percent of the tests fail. 

```{r fails, echo=FALSE, fig.height = 3, fig.width = 5, out.width='0.7\\textwidth', fig.cap="Percent of failed land-to-land comparisons using an optimization window $w_o = 120$ and a coarseness of $c = 0.25$. With an increase in the size of the validation window  a higher percentage of tests fails. The percentage of failures is higher for same-source lands than for different source lands."}
fails <- allp %>% filter(coarse == 0.25) %>% 
  group_by(wo, wv) %>% summarize(
  failed = sum(is.na(p_value)),
  n=n()
)

allp %>% filter(coarse == 0.25) %>% 
  group_by(wo, wv, match) %>% summarize(
  failed = sum(is.na(p_value)),
  n=n()
) %>% 
  ggplot(aes(x = wv, y = failed/n*100, colour=factor(match), shape=factor(match))) +
  geom_point(size=3) +
  xlab("Size of validation window in pixels") +
  ylab("Percent of failed tests") +
  theme_bw() +
  scale_colour_brewer("Same-source", palette="Set1") +
  scale_shape_discrete("Same-source") +
  ylim(c(0,13))
```


\hh{diagnose problem: use of optimal location in getting the pairs for different-shift correlations}
\hh{figure: number of diff shift pairs and location}


# A modified test

\hh{fix the algorithm - all of the other properties in Hadler and morris still hold.}

\hh{number of failed tests down. }


# Comparison of Results

## Profiles

\hh{When dealing with profiles, coarseness is an additional parameter that has to be considered in the matching.}

\hh{Figure \ref{fig:coarse} shows the type II error rates for profiles using an optimization window $w_o= 120$ and a validation window $w_v = 30$ for varying level of coarseness. The type II error for all nominal levels of  $\alpha$ are the lowest for a coarseness range of 0.20 to 0.30, with a minim in type II reached at a coarseness $c = 0.25$. }
\hh{However, for an optimal value of coarseness other aspects have to be condsidered.}
```{r coarse, fig.cap="Type II error with respect to coarseness parameter over profiles, $w_o = 120$, $w_v = 30$. Optimal values for coarseness are around $c \\approx 0.30$", fig.width=5, fig.height = 3, out.width='.6\\textwidth'}
e1<-errors2 %>% filter(wv==30)%>% ggplot(aes(x = coarse, y=beta, colour=factor(alpha))) +
  geom_point() + geom_line() +
  theme_bw() +
  scale_colour_brewer(expression("Nominal\ntype I error"~alpha), palette="Set2") + xlab("Profile Coarseness")+
  ylab("Type II error") 
e1
```

\hh{\autoref{fig:profile-sketch} gives an overview of the effect of different coarseness parameters: from left to right, coarseness levels $c$ are 0.2, 0.25, and 0.3, respectively. The top row shows the resulting signature after smoothing the profile shown in Figure~\ref{fig:sigs-profiles} with the coarseness specified. The histograms in the bottom row show the relative optimal location $t^o$ (optimal location $t^o$ divided by the length of the corresponding profile). Optimal locations should be distributed uniformly. However, for coarseness values of $c = 0.25$ and $c=0.30$ we see very distinct boundary effects: optimal locations $t^o$ are found at the very extreme ends of a profile more often than one would expect based on a uniform location.}

```{r profile-sketch, fig.height = 5, fig.width='\\textwidth', fig.cap="Overview of the effect of different coarseness parameters $c$ on the profile shown in Figure \\ref{fig:sigs-profiles} (top). The bottom row shows histograms of  the (relative) optimal locations $t^o$ identified in the optimization step for different values of the coarseness parameter $c$. "}
profile <- sigs_graphics[[1]] %>% filter(land_id==19)
profile$lw20 <- lowess(x = profile$y, y = profile$value, f=.20)$y
profile$lw25 <- lowess(x = profile$y, y = profile$value, f=.25)$y
profile$lw30 <- lowess(x = profile$y, y = profile$value, f=.30)$y
#profile$lw50 <- lowess(x = profile$y, y = profile$value, f=.5)$y
#profile$lw75 <- lowess(x = profile$y, y = profile$value, f=.75)$y
#profile$lw100 <- lowess(x = profile$y, y = profile$value, f=1)$y

profiles <- tidyr::gather(profile, coarseness, values, lw20:lw30)
profiles <- profiles %>% mutate(
  c = readr::parse_number(coarseness)/100
)
p1 <- profiles %>% 
  ggplot(aes(x = y, y = value-values)) + geom_line() +
  theme_bw() +
  ylab(expression(paste("Depth (in ",mu,m,")", sep=""))) + 
  xlab(expression(paste("Relative Location (in ",mu,m,")", sep=""))) +
  facet_grid(~c, labeller="label_both") 

profc20 <- readRDS("../data/profiles-coarseness/chumbley-csafe-profiles-inv-wo_160-wv_30-c_20.rds")
profc20$coarseness <- 0.2
profc25 <- readRDS("../data/profiles-coarseness/chumbley-csafe-profiles-inv-wo_160-wv_30-c_25.rds")
profc30 <- readRDS("../data/profiles-coarseness/chumbley-csafe-profiles-inv-wo_160-wv_30.rds")
profc30$coarseness <- .30
profc <- bind_rows(profc20, profc25, profc30)
profc <- profc %>% mutate(
  locations = chumbley %>% purrr::map_int(.f = function(x) {
    res <- NULL
    try(res <- x$locations[1], silent = TRUE)
    res
  })
)
profsumm <- read.csv("../data/profiles-summary.csv")
profsumm <- profsumm %>% select(-run_id)
profc <- profc %>% left_join(profsumm, by=c("land2_id"="land_id", "profile2_id"="profile_id"))

p2 <- profc %>% 
  ggplot(aes(x = locations/(length-wo))) +
  geom_histogram(binwidth = 0.025) +
  facet_grid(.~coarseness, labeller="label_both") + 
  theme_bw() +
  xlab(expression("Relative optimal location "~t^"*")) +
  ylab("Number of profiles")
 
grid.arrange(p1, p2, ncol=1)
```




## Results

\hh{For signatures from NIST scans we see three problems: 
\begin{enumerate}
\item type-2 error rate is at best 30\% for a type-1 error rate of 5\%, which is well above the error rates we see for tool marks from screw drivers, see figure \ref{fig:type2};
\item the observed type-1 error, which generally close to the nominal type-1 error rate, depends on the size of the optimization window: as the window size increases, the observed type-1 error decreases, see figure \ref{fig:type1};
\item the Chumbley-score fails to provide a result for up to 3\% of the cases. The number of failed tests increases linearly in the size used for the window in the optimization step. The rate of failed tests is considerably higher when the two lands are from same source than when the lands are from different sources, see figure \ref{fig:missings}. 
\end{enumerate}
}


Figure \ref{fig:missings} gives an overview of the number of failed tests, i.e. tests in which a particular parameter setting did not return a valid result. This happens e.g.\ when the shift to align two markings is so large, that the remaining overlap is too small to accommodate  windows for validation. The problem is therefore exacerbated by a larger validation window. Figure \ref{fig:missings}(left) also shows that the number of failed tests is approximately linear in the size of the optimization window.
Tests also fail at a higher rate than expected when the markings are from the same source (right). This difference is the least pronounced around an optimized window size $w_o$ of around 120. However, even in this scenario, the number of failed tests for markings from the same source is about twice as high as expected given the number of same source and different source pairings in the data set.



<!--
Following on similar lines to the setup of toolmarks, the first step here is to first identify what difference does different window sizes of optimization and the validation step have, when adapting the toolmark method to bullets.

The marking made on bullets are smaller than toolmarks and is also less wider. The idea is to find out possible areas of error while adapting the score based method proposed for toolmarks, using cross-validation setup to identify appropriate parameter settings for (a) signatures and (b) profiles directly

### Signatures
-->

```{r type2, fig.width=8, fig.height=5, out.width='.8\\textwidth', fig.cap="Type II error rates observed across a range of window sizes for optimization $w_o$. For a window size of $w_o = 120$ we see a drop in type II error rate across all type I rates considered. Smaller validation sizes $w_v$ are typically associated with a smaller type II error."}
errors %>% filter(wv %in% c(30,50)) %>%
  ggplot(aes(x = wo, y = beta, colour=factor(alpha))) + 
  geom_point(aes(shape=factor(wv)), size=3.5) +
#  facet_grid(.~wo, labeller="label_both") + 
  geom_smooth(aes(group=alpha), se=FALSE, method="loess", size=.7) +
  theme_bw() +
  scale_colour_brewer(expression("Nominal\ntype I error"~alpha), palette="Set2") +
  scale_shape_discrete(expression("Size of\nvalidation window "~w[v])) +
  xlab(expression("Window size for optimization "~w[o])) +
  ylab("Type II error rate")


labels = expression(P[M1](tilde(z)>0),P[M0](tilde(z)>0))
```

```{r type1, fig.width=8, fig.height=5, out.width='\\textwidth', fig.cap="Comparison of observed and nominal type I error rates  across a range of window sizes for optimization $wo$. The horizontal line in each facet indicates the nominal type I error rate."}
errors %>% filter(wv %in% c(30,50)) %>%
  ggplot(aes(x = wo, y = actual, colour=factor(alpha))) + 
  geom_hline(aes(yintercept=alpha), colour="grey30", size=.5) +
  geom_point(aes(shape=factor(wv)), size=2.5) +
  #  facet_grid(.~wo, labeller="label_both") + 
  geom_smooth(aes(group=alpha), size=.8, se=FALSE, method="lm", alpha=.9) +
  theme_bw() +
#  scale_y_log10(breaks=c(0.001,.005, 0.01, 0.05)) +
  scale_shape_discrete(expression("Size of validation window "~w[v])) +
  ylab("Observed type I error rate") +
  xlab(expression("Window size for optimization, "~w[o])) +
  scale_colour_brewer(expression("Nominal type I error "~alpha), palette="Set2") +
  facet_wrap(~alpha, labeller="label_both", scales="free") +
  theme(legend.position="bottom")
  
```


```{r missings, out.width='\\textwidth', fig.width=8, fig.height=4, fig.cap="The number of failed tests increases with an increase in the size of the optimization window (left). Unfortunately there is also a dependency between failed tests and ground truth. The plot on the right shows the ratio of the number of land pairs from same sources and different sources for failed tests. For small optimization windows and large windows the number of failed tests for same-source land-to-land comparisons is increasing. Even in the minimum, same-source land-to-land comparisons fail at twice the rate that they are expected to based on the ratio of the number of known matches and known non-matches (horizontal line).",fig.align = "center"}
#ns1<- ns[which(ns$wv == c(30,50), arr.ind = T),]

fails <- all %>% group_by(wv, wo) %>% summarize(
  miss = sum(is.na(p_value)),
  missperc = miss/n()*100
)

failsbymatch <- all %>% group_by(wv, wo, match) %>% summarize(
  fails = sum(is.na(p_value))
) %>% mutate(
    ratio = fails[match==TRUE]/fails[match==FALSE],
    match = c("Different Source", "Same Source")[as.numeric(as.logical(match)) + 1]
    ) 

missings <- fails %>% filter(wv %in% c(30, 50)) %>%
  ggplot(aes(x = wo, y = missperc, 
             colour = factor(wv),
             shape=factor(wv)), size=.3) + 
  geom_smooth(method="lm", size=.5, se=FALSE) +
  geom_point(size=2.5) + 
  theme_bw() +
  ylab("Percent of failed tests.") +
  xlab(expression("Size of optimization window "~w[o])) +
  scale_colour_brewer(expression("Size of validation window "~w[v]), palette="Set1") +
  scale_shape(expression("Size of validation window "~w[v])) +
    theme(legend.position="bottom")


missingsbymatch <- failsbymatch %>% 
  filter(wv %in% c(30, 50),
         match=="Same Source") %>%
  ggplot(aes(x = wo, y = ratio, 
             colour = factor(wv),
             shape=factor(wv)), size=.3) + 
  geom_hline(yintercept = 43960/2948225, size=0.5, colour="grey30") +
  geom_smooth(method="loess", span=0.8, size=.5, se=FALSE) +
  geom_point(size=2.5) + 
  theme_bw() +
  ylab("Ratio of same/different source\namong failed tests") +
  xlab(expression("Size of optimization window "~w[o])) +
  scale_colour_brewer(expression("Size of validation window "~w[v]), palette="Set1") +
  scale_shape(expression("Size of validation window "~w[v])) +
    theme(legend.position="bottom") +
  scale_y_continuous(breaks=c(0.015, 0.03, 0.06, 0.09))



gridExtra::grid.arrange(missings, missingsbymatch, ncol=2)


```

<!--
Signatures of lands for all Hamby-44 and Hamby-252 scans made available through the NIST ballistics database \citep{nist} were considered. Both of these sets of scans are part of the larger Hamby study \citep{hamby} and each consist of twenty known bullets (two each from ten consecutively rifled Ruger P85 barrels) and fifteen questioned bullets (each matching one of the ten barrels). Ground truth for both of these Hamby sets is known and was used to assess correctness of the tests results. 

Bullet signatures being compared at this time are therefore from the Hamby 44 and Hamby 252 data. The database setup and pre-processing system used for choosing the Bullet signatures are as described by \citet{aoas}. In order to choose the bullet signatures we first filter out Land_id for Profiles from the Hamby 44 and Hamby 252 data and remove all NA values. Then run_id = 3 is chosen as the signatures generated from this run_id give the closest match. Different run_id's have some different settings for generating the signatures.(The level of smoothing does not seem to be one of them)

The bullet signatures when generated by this process already includes a loess smoothing. Therefore, the coarseness factor is set to 1 while running the chumbley non random algorithm for comparing different optimization windows.The algorithm generates the same_shift, different_shift, U-Stat and P_value parameters which are then used to calculate the errors associated with different sets of window sizes.

### Profiles

The profiles are cross-sectional values of the the bullet striation mark which are chosen at an optimum height (x as used by \citet{aoas}). This x or height is not a randomly chosen level. The rationale behind the choice has been explained by \citet{aoas}. A region is first chosen where the cross-correlation seems to change very less and in this region an optimum height is chosen. The profiles generally resemble a curve which is more or less similar to a quadratic curve (a quadratic fit to the raw data values of the profile is not an exact fit but it does show a similar trend). Profiles are the set of raw values representing the striation marks, and signatures are generated from these by removal of the inherent curvature and applying some smoothing (the signatures generated by \citet{aoas} use a loess function for smoothing). 

Similar to signatures the run_id = 3 was used when applying the chumbley algorithm using the database setup given by \citet{aoas} of Hamby-44 and Hamby-252 datasets, on the profiles. The run_id not only defines the level of smoothing but also signifies the chosen height at which the profiles were selected initially. Another important aspect is the range of horizontal values (which is referred to as the y values in \citet{aoas}) in the signatures. These have already been pre-processed in the database to not include any grooves.

Therefore for the sake of comparison the run_id = 3 is still chosen so as to ensure that the horizontal values remain the same as that of the signatures. This also gives us profiles with the grooves removed.

The idea therefore is to first use these raw values of the profile directly in the chumbley algorithm, and see how the algorithm performs for different coarseness values (smoothing parameter as referred in the function lowess used in the chumbley algorithm).
-->





```{r, include =F}
wo120_sig <- errors %>% filter(wo==120, wv %in% c(30, 50))
wo120_pr <- errors2 %>% filter(wo==120, wv %in% c(30, 50), coarse == .25)

dt <- rbind(wo120_sig, wo120_pr) %>% 
  ungroup() %>% 
  mutate(beta = round(100*beta, 1)) %>%
  select(wv, type, alpha, beta) %>% 
  spread(alpha, beta)
print(xtable(dt), include.rownames=FALSE)
```

\begin{table}[ht]
\caption{\label{tab:type2} Type II error rates for profiles and signatures of bullet lands. For profiles, a coarseness value of $c = 0.25$ is used to remove bullet curvature.}
\centering
\begin{tabular}{rlrrrr}
  \hline
validation && \multicolumn{4}{l}{Nominal type I error rate $\alpha$}\\  
window $w_v$ & source & 0.001 & 0.005 & 0.01 & 0.05 \\ 
  \hline
 30 & profiles & 54.40 & 43.80 & 39.70 & 30.00 \\ 
   30 & signatures & 55.00 & 45.40 & 41.40 & 31.10 \\ \hline
   50 & profiles & 58.50 & 44.40 & 40.70 & 28.70 \\ 
   50 & signatures & 62.60 & 49.60 & 44.20 & 30.50 \\ 
   \hline
\end{tabular}
\end{table}


# Conclusion

The results suggest that the Nominal type I error $\alpha$ value shows dependence on the size of the window of optimization. For a given window of optimization the actual Type I error is comparable to the nominal level for only a select few validation window sizes and for comparable validation window sizes of 30 and 50 as done here, the actual type I error does not seem to vary as much as it varies with the optimization window sizes .
A Test Fail, i.e. tests in which a particular parameter setting did not return a valid result, happens, when the shift to align two signatures is so large, that the remaining overlap is too small to accommodate  windows for validation, depends on whether known-match or known non-matches has predictive value, with test results from different sources having a much higher chance to fail. On conducting an analysis of all known bullet lands using the adjusted chumbley algorithm, Type II error was identified to be least bad for window of validation 30 and window of optimization 120. In case of unsmoothed raw marks (profiles), Type II error increases with the amount of smoothing and least for LOWESS smoothing coarseness value about 0.25 or 0.3. In an effort to identify the level of adaptiveness of the algorithm, comparisons were made between signatures and profiles. Their comparison with respect to validation window size for a fixed optimization window size suggested that, profiles have a total error (i.e all incorrect classification of known-matches and known non-matches) greater than or equal to the total error of signatures for all sizes of validation window. Profiles also fail more number of times than signatures in a test fail (for different coarseness keeping windows fixed and also for different validation windows keeping coarseness fixed) which lets us conclude that the behaviour of the algorithm for the profiles instead of pre-processed signatures is not better. Finally it should be noted that the current version of the adjusted chumbley algorithm seems to fall short when compared to other machine-learning based methods \citet{aoas}, and some level of modification to the deterministic algorithm needs to be identified and tested that would reduce the number of incorrect classifications.

```{r wo120-profile, fig.width = 6, fig.height=4, fig.cap="The figure shows the Type II error for different validation window sizes and different chosen nominal alpha levels when the size of the optimization window = 120", eval = FALSE}
wo120_profile <- errors2 %>% filter(wo==120, coarse == 0.25) %>% 
  #gather(type, error, actual:beta) %>% 
  gather(type, error, beta) %>% 
  ggplot(aes(x = wv, y = error, colour = factor(alpha)))  + 
  geom_line() + geom_point() + facet_wrap(~type, scales="free") +
    theme_bw()+ 
  theme(legend.position="bottom") + ggtitle("Profile" , subtitle = "Varying validation window sizes, Optimization window = 120, Coarseness = 0.25")

#combined120 <- full_join(errors2 %>% filter(wo==120, coarse == 0.25), wo120 %>% 

wo120_1<- wo120_profile %>% gather(type, error, beta) %>%  
  ggplot(aes(x = wv, y = error, colour = factor(alpha)))+
  geom_line(linetype=5)+  geom_point(shape= 24) + facet_wrap(~type, scales="free") +
  theme_bw()+ 
  theme(legend.position="bottom") + ggtitle("Signatures" , subtitle = "Varying validation window sizes, Optimization window = 120")

combined120<- 
  ggplot(data = wo120_profile %>% gather(type, error, beta), aes(x = wv, y = error, colour = factor(alpha)))+
  geom_line(aes(linetype="Signatures"))+  geom_point(shape= 24) + 
  geom_line(data =  errors2 %>% filter(wo==120, coarse == 0.25) %>% 
  gather(type, error, beta), aes(x = wv, y = error, colour = factor(alpha), linetype = "Profiles")) +       geom_point(data =  errors2 %>% filter(wo==120, coarse == 0.25) %>% 
  gather(type, error, beta)) +
  facet_wrap(~type, scales="free") +
  scale_linetype_manual("Lines",values=c("Signatures"=2,"Profiles"=1))+
  guides(fill = guide_legend(keywidth = 1, keyheight = 1),
    linetype=guide_legend(keywidth = 3, keyheight = 1),
    colour=guide_legend(keywidth = 3, keyheight = 1))+
  theme_bw()+ 
  theme(legend.position="right") + ggtitle("Type II error with Validation window" , subtitle = "Profiles at coarseness 0.25") + xlab("Type II error")
    
combined120
#wo120_profile
#wo120_1
#multiplot(wo120_1, wo120_profile1)
```
```{r failtest-coarseness, fig.width = 6, fig.height=4, fig.cap="The figure on the left shows the actual Type I error while the figure on the right shows the Type II error for different validation window sizes and different chosen nominal alpha levels when the size of the optimization window = 120", eval= FALSE}

miss_1
```
```{r failtest-wv, fig.width = 6, fig.height=4, fig.cap="The figure on the left shows the actual Type I error while the figure on the right shows the Type II error for different validation window sizes and different chosen nominal alpha levels when the size of the optimization window = 120", eval= FALSE}

miss_2
```
```{r failtest-wo, fig.width = 6, fig.height=4, fig.cap="The figure on the left shows the actual Type I error while the figure on the right shows the Type II error for different validation window sizes and different chosen nominal alpha levels when the size of the optimization window = 120", eval= FALSE}

miss_2
```

<!-- # Algorithm Modification -->

<!-- The proposed algorithm modification is at the same shift step which comes after the shift distance is identified by finding two windows in the two markings that have the highest correlation. -->

<!-- The modification allows for a "wiggle" room in the second marking of the two sets of markings being compared. This means that each set of same shift windows that are being compared, the second marking will window that is under consideration is allowd to move a little towards the left and a little towards the right. -->

<!-- This gives us a new set of 4 windows, 2 to the left and 2 to the right of the original comparison window. Then the correlations for each one of these 5 windows with the window under consideration of the 1st marking is retrieved (from the validation step correlation matrix) or computed. -->

<!-- Then the window that has the maximum correlation (from the set of 5 windows in the 2nd marking) with the window of the 1st marking is chosen. -->

<!-- This new maximum correlation is then used to compute the U-statistics using the same method as before. -->

<!-- ## Expected advantage of modification -->

<!-- We are expecting that this modification would improve the type I and II errors for the good. Less number of false negatives and false positives. -->
<!-- The reason for this expectation is many a times the bullet markings are not made at the same distances for two or more bullets because of the way it comes out of the barrel. -->

<!-- Therefore rigid same shifts might not necessarily compare the right set of windows. In the same shift step for one set of windows, allowing the window in the 2nd marking to wiggle left and right and finding the best match to the window in the 1st marking, lets us adjust for situations where markings are compressed or elongated. -->

<!-- ## Dependence on the delta of new windows from the original same shift window -->

<!-- From initial tests the amount of movement to get new windows left and right of the original same shift window seems to directly influence the -->
<!-- the pvalue and U statistic that we look at.  -->

<!-- ### other questions -->
<!-- How much movement is reasonable and should be used? Is there a way to understand if the 2nd marking in comparison to the 1st marking is compressed or elongated or neither? Should the amount of movement selected depend on this compression or elongation? or using a static delta movement value justifieable. -->
<!--
# Appendix
\begin{appendix}

On the other hand Figure \ref{fig:prof_missings} (b) shows if the coarseness level set in the chumbley agorithm has any effect on the signatures, which are pre-processed and already smoothed to a certain extent. From Figure \ref{fig:prof_missings} (b) we can notice that for different nominal $\alpha$ levels, the type II error fluctuates slightly but does not change much, thereby helping us conclude that the coarseness levels set in the LOWESS smoothing in the chumbley alggorithm does effect the type II error much for signatures.

\subsection{Comparison of profiles and signatures}

Another reason for failed tests can be incorrect identification of maximum correlation windows in the optimization step as seen in figure \ref{fig:prof_missings}(d) because of the level of smoothing, as too much smoothing would subdue intricate features that might otherwise help in the correlation calculations and correct identification of maximum correlation windows irrespective of the size. This would again cause a simiar effect as explained for figure \ref{fig:missings} with validation windows, irrespective of size, during the shifts end up at the ends of the markings resulting in an invalid calculation and failed comparison attempt. 

In figure \ref{fig:prof_missings}(d) and (f), we compare profiles and signatures on the basis of number of failed tests. The profiles chosen for figure \ref{fig:prof_missings}(f) have a constant coarseness of 0.25 and window of optimization as 120. The signatures in this case are not smoothed using the chumbley algorithm step of LOWESS smoothing. Instead signatures are used as calculated by \citet{aoas}. The smoothing in these signatures were determined and fixed on the basis of their performance in the random forest based algorithm proposed by \citet{aoas}. The comparison of profiles and signatures with variation of validation window size therefore is made on even footing. The trends are similar to figure \ref{fig:missings} in the sense that for known non-matches the number of failed tests are more for both signatures and profiles and increasing linearly with the validation window size. The problem is however, worse for profiles which has higher number of failed tests than signatures for all validation windows.

The total error for different validation window sizes for signatures and profiles can be seen in figure \ref{fig:prof_missings} (e).The optimization window size is 120 and profiles are calculated at a default 0.25 coarseness level while signatures as before are not smoothed again in the modified chumbley algorithm. We can see that the total error is always higher for profiles as compared to signatures for all sizes of validation window. 

```{r, fig.cap="Type II errors for different levels of coarseness."}
e1<-errors2 %>% filter(wv==30)%>% ggplot(aes(x = coarse, y=beta, colour=factor(alpha))) +
  geom_point() + geom_line() +
  theme_bw() +
  scale_colour_brewer(expression("Nominal\ntype I error"~alpha), palette="Set2") + xlab("Profile Coarseness")+
  ylab("Type II error") + ggtitle(label = "(a)")

e2<- errors_sig_c %>% filter(wv==30)%>% ggplot(aes(x = coarse, y=beta, colour=factor(alpha))) +
  geom_point() + geom_line() +
  theme_bw() +
  scale_colour_brewer(expression("Nominal\ntype I error"~alpha), palette="Set2")+ xlab("Signature Coarseness") + ylab("Type II error") + ggtitle(label = "(b)")

#multiplot(e1,e2, cols = 1)

```



```{r, eval=FALSE}
#errors %>% ggplot(aes(x = wv, y=total, colour=factor(alpha), shape="signatures")) + geom_point() + facet_grid(~wo) +geom_point(data=errors2, aes(shape="profiles"))

total1<- errors_sig_c %>% ggplot(aes(x = coarse, y=total, colour=factor(alpha), shape="signatures")) + geom_point() + geom_line()  + geom_point(data= errors2 %>% filter(wv==30), aes(shape="profiles")) + geom_line(data=errors2 %>% filter(wv==30), aes(shape="profiles")) + theme_bw() +
  scale_colour_brewer(expression("Nominal\ntype I error"~alpha), palette="Set2") + xlab("Coarseness") +ylab("Total error") + ggtitle(label = "(c)")

total2<- wo120_profile %>% filter(wv>10) %>% ggplot(aes(x = wv, y=total, colour=factor(alpha), shape="signatures")) + geom_point() + geom_line() +geom_point(data=errors2 %>% filter(wv>10) %>% filter(coarse==0.25), aes(shape="profiles")) + geom_line(data=errors2 %>% filter(wv>10)%>% filter(coarse==0.25), aes(shape="profiles")) + theme_bw() +
  scale_colour_brewer(expression("Nominal\ntype I error"~alpha), palette="Set2") + ylab("Total error") + ggtitle(label = "(e)")


```

```{r prof_missings, fig.width=10, fig.height=10, fig.cap="Row 3:  Total error and Number of failed tests by the window validation size, wv, and ground truth, Row 2: Total error and Number of failed tests with Coarseness for both profiles and signatures, Row 1: Type II error for different coarseness levels as used in the modified chumbley algorithm for profiles and signatures"}


#ns1<- ns[which(ns$wv == c(30,50), arr.ind = T),]
miss_1<- nsp %>% filter(wv == 30) %>%
#ns %>% filter(wv == c(30, 50)) %>%
  ggplot(aes(x = coarse, y = missperc, shape= "profile", colour = factor(match))) + 
  #geom_smooth(method="auto", size=.5, se=FALSE) +
  geom_point(size = 2) + 
  theme_bw() +
  ylab("Percent of failed tests.") +
  xlab("Coarseness") +
  scale_colour_brewer("Match", palette="Set1") + geom_point(data=ns_sig_c  %>% filter(wv %in% c(30,50)) , aes(shape="signature"), size = 2) + ggtitle(label = "(d)")
# + scale_colour_brewer("Match", palette="Set2") #+geom_smooth(method="auto", size=.5, se=FALSE) 
  #scale_shape_discrete("Size of \nvalidation\nwindow, #wv")

miss_2<- nsp %>% filter(coarse == 0.25) %>%
#ns %>% filter(wv == c(30, 50)) %>%
  ggplot(aes(x = wv, y = missperc, shape= "profile", colour = factor(match))) + 
  #geom_smooth(method="auto", size=.5, se=FALSE) +
  geom_point(size = 2) + 
  theme_bw() +
  ylab("Percent of failed tests.") +
  xlab("wv") +
  scale_colour_brewer("Match", palette="Set1") + geom_point(data=ns %>% filter(wo == 120)  %>% filter(coarse == 1) , aes(shape="signature"), size = 2) + ggtitle(label = "(f)")#+ scale_colour_brewer("Match", palette="Set2")

#multiplot(plotlist = list(total1, total2, miss_1, miss_2, e1, e2), cols = 2)#, matrix(c(1,2,3,3), nrow=2, byrow=TRUE))
multiplot(plotlist = list(e1, total1, total2,e2, miss_1, miss_2), cols = 2)
```
\end{appendix} 
-->