---
title: "Adaption of the Chumbley Score to matching of bullet striation marks"
authors:
- affiliation: Department of Statistics, Iowa State University 
  name: Ganesh Krishnan
  thanks: The authors gratefully acknowledge ...
- affiliation: Department of Statistics and CSAFE, Iowa State University 
  name: Heike Hofmann
biblio-style: apsr
output:
  pdf_document:
    citation_package: natbib
    fig_caption: yes
    keep_tex: yes
    latex_engine: pdflatex
    template: template.tex
  html_document: default
blinded: 0
keywords:
- 3 to 6 keywords
- that do not appear in the title
bibliography: bibliography
abstract: null
---

\newcommand{\hh}[1]{{\textcolor{orange}{#1}}}
\newcommand{\gk}[1]{{\textcolor{green}{#1}}}
\newcommand{\cited}[1]{{\textcolor{red}{#1}}}

\tableofcontents
\newpage
```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
library(knitr)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  comment = "",
  fig.height = 6,
  fig.width = 6,
  fig.align = "center",
  out.width= '\\textwidth',
  cache = FALSE,
  fig.path='figures/',
  echo=FALSE,
  cache=TRUE
)
options(knitr.table.format = "latex")
library(tidyverse)
library(kableExtra)
```

```{r functions}
errorrate <- function(data, alpha) {
  summ <- data %>% filter(!is.na(p_value)) %>%
    mutate(signif = p_value < alpha) %>%
    group_by(wv, wo, match, coarse, signif) %>% tally()
  summ$error <- with(summ, match != signif)
  summ$alpha <- alpha
  
  rates <- summ %>% group_by(wv, wo, coarse, match, alpha) %>% summarize(
    rate = n[error==TRUE]/sum(n)
  )
  totals <- summ %>% group_by(wv, wo, coarse, alpha) %>% summarize(
    total= sum(n[error==TRUE])/sum(n)
  )
  rates <- rates %>% spread(match,rate) %>% rename(
    actual = `FALSE`,
    beta = `TRUE`
  )
  left_join(rates, totals)
}


# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
#  ref: http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
```

```{r data}
if (!file.exists("../data/all-sigs.rds")) {
  files <- dir("../data/signatures", pattern="csv")
  all <- data.frame()
  for (file in files) {
    tmp <- read.csv(file=file.path(path="../data/signatures", file)) 
    if (length(grep("x",names(tmp)) > 0)) tmp <- tmp %>% select(-x)
    all <- rbind(all, tmp)
  }
  all <- all %>% filter(land1_id < land2_id)
  saveRDS(all, file="../data/all-sigs.rds")
} else {
  all <- readRDS("../data/all-sigs.rds")
}

all$coarse <- 1
errors <- rbind(errorrate(all, 0.001), 
                errorrate(all, 0.005), 
                errorrate(all, 0.01), 
                errorrate(all, 0.05))
```

```{r data-profiles}
if (!file.exists("../data/all-profiles.rds")) {
  files <- dir("../data/profiles/", pattern="csv")
  allp <- data.frame()
  for (file in files) {
    tmp <- read.csv(file=file.path(path="../data/profiles", file)) 
    #print(names(tmp))
    if (length(grep("coarse",names(tmp))) == 0)
      tmp$coarse <- gsub(".*coar-(.*).csv", "\\1", file)
    if (length(grep("x",names(tmp)) > 0)) tmp <- tmp %>% select(-x)
    allp <- rbind(allp, tmp)
  }
  allp <- allp %>% filter(land1_id < land2_id)
  allp <- allp %>% mutate(coarse = as.numeric(gsub("pt", ".", coarse)))
  saveRDS(allp, file="../data/all-profiles.rds")
} else {
  allp <- readRDS("../data/all-profiles.rds")
}

errors2 <- rbind(errorrate(allp, 0.001), 
                errorrate(allp, 0.005), 
                errorrate(allp, 0.01), 
                errorrate(allp, 0.05))


```

```{r data-sig-coars}
if (!file.exists("../data/all-sig-c.rds")) {
  files <- dir("../data/sig_diff_coarseness/", pattern="csv")
  all_sig_c <- data.frame()
  for (file in files) {
    tmp <- read.csv(file=file.path(path="../data/sig_diff_coarseness/", file)) 
    #print(names(tmp))
    if (length(grep("coarse",names(tmp))) == 0)
      tmp$coarse <- gsub(".*coarse-(.*).csv", "\\1", file)
    if (length(grep("x",names(tmp)) > 0)) tmp <- tmp %>% select(-x)
    all_sig_c <- rbind(all_sig_c, tmp)
  }
  all_sig_c <- all_sig_c %>% filter(land1_id < land2_id)
  all_sig_c <- all_sig_c %>% mutate(coarse = as.numeric(gsub("pt", ".", coarse)))
  saveRDS(all_sig_c, file="../data/all-sig-c.rds")
} else {
  all_sig_c <- readRDS("../data/all-sig-c.rds")
}

errors_sig_c <- rbind(errorrate(all_sig_c, 0.001), 
                errorrate(all_sig_c, 0.005), 
                errorrate(all_sig_c, 0.01), 
                errorrate(all_sig_c, 0.05))


```


# Introduction and Background

## Motivation
Same source analyses are a major part of an Forensic Toolmark Examiner's job. In current practice examiners  make these comparisons visually under a comparison microscope and come to one of the following four conclusions: identification, inconclusive, elimination or unsuitable for examination \citep{afte-toolmarks1998}. These conclusions are made on the basis of "unique surface contours" of the two toolmarks being in "sufficient agreement" \citep{afte-toolmarks1998}. AFTE describes the term "sufficient agreement" as the possibility of another tool producing the markings under comparison, as practically impossible \citep{afte-toolmarks1998}. This subjectivity in the assessment as well as the lack of error rates are the main points of criticisms in the toolmark examination first raised by the National Research Council in 2009 \citep{NAS:2009} and later emphasized further by the President's Council of Advisors on Science and Technology \citep{pcast2016}.

Technological advances, such as profilometers and confocal microscopy allow to measure 3D surfaces \citep{vorburger2016} in a digitized form. This forms the basis of statistical analysis of toolmarks. A statistical approach based on data  removes both subjectivity from the assessment and allows a quantification of error rates for both false positive and false negative identifications.

Various toolmarks have been studied in the literature:  screwdriver marks  digitized using a profilometer have been analyzed in \citet{manytoolmarks1} and \citet{chumbley};  \citet{manytoolmarks2} have investigated 3D marks from screwdriver, tongue and groove pliers captured using a confocal microscope;  digitized marks from slip-joint pliers generated by a surface profilometer have been investigated by \citet{afte-chumbley}. 

\citet{manytoolmarks2} define a relative distance metric and use it as similarity measure between two toolmarks. \citet{manytoolmarks1} extract many small segments in the markings of two toolmarks and compare similiarity using a maximum pearson correlation coefficient. 
The Chumbley scoring method, first introduced by \citet{chumbley}, uses a similar but more extensive framework based on a Mann-Whitney U test of the resulting correlation coefficients. This approach was later improved by Hadler and Morris \citep{hadler} where they introduce a deterministic approach and address the issue of lack of independece between segements of striae.  In this paper, we are investigating the applicability of the Chumbley scoring method by \citet{hadler} to assess striation marks on bullet lands for same-source identification.

Striation marks on bullets are made by impurities in the barrel. As the bullet travels through the barrel, these imperfections leave "scratches" on the bullet surface. Typically, only striation marks in the land engraved areas (LEAs) are considered \citet{afte-article1992}. Bullet lands are depressed areas between the  grooves made by the rifling action of the barrel. Compared to toolmarks made by screwdrivers striation marks on bullets are typically much smaller, both in length and in width. Bullets also have a curved cross-sectional topography \hh{sketch?}. Figure \ref{fig:rgl} shows us how the signature from a bullet land lines up with the image of the land from which it was extracted. We can also see in the figure how the depth and relative position of the striation markings seen in the image are interpreted as the signature.

Bullet matching methods depend on the attribute being used for making comparisons. For single feature based matching \citet{chu2013} used an automatic method for counting consequtive matching striae (CMS). They reported an error rate 52% of the known same source lands comparisons as misidentified and zero false positives for known different source lands. \citet{ma2004} and \citet{vorburger2011} discuss about CCF (cross-correlation function) and its discriminating power and application but do not talk about matches and non-matches. \citet{aoas} use multiple attributes like CCF, CMS, D (distance measure) etc in a random forrest based method and compare every land against every other land of the Hamby dataset \citet{hamby}. They report an out-of-bag error rate of 0.0046.

The Chumbley score  provides us with another approach in the same-source assessment of bullet striation marks.\citet{chumbley}, in their paper, compare two toolmarks with the intention of determining if it comes from the same tool. The data for their study was obtained from 50 sequentially manufactured screwdriver tips.\citet{chumbley} reported error rates for marking made by the tips at differnt angles. For a 30 degree they report false negative error rate to be 0.023 and  false positive error rate be 0.09. For other angles the error rates remain the same for false negatives and 0.01 for false positives. The improvement proposed by \citet{hadler} on the other hand, did not consider effect of angles and gave 0.06 false  negative error rate and 0 false poisitive error rate for markings made at the same angle.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/B6-B2-L6-rescaled.png}
```{r, eval=FALSE, echo=FALSE, fig.width=8, fig.height=3, out.width='\\textwidth'}
land <- read.csv("../data/b6b2l6.csv")
land %>% ggplot(aes(x = y, y=resid)) +geom_line() +theme_bw() +
  ylab(expression(paste("Depth (in ",mu,m,")", sep=""))) + 
  xlab(expression(paste("Relative Location (in ",mu,m,")", sep="")))
```

\caption{\label{fig:rgl} Image of a bullet land from a confocal light microscope at 20 fold magnification (top) and a chart of the corresponding signature of the same land (bottom). The dotted lines connect some peaks visible in both visualizations.}

\end{figure}

## Scans for land engraved areas

The land engraved markings across the cross section of the land are termed as bullet profiles \citep{aoas,ma2004}. These are often used for making land to land comparisons. Bullet signatures on the other hand \citep{chu2013,aoas} refer to a processed version of the raw land engraved markings or profiles. The generation of bullet signatures involves first extraction of a bullet profile by taking the cross-sectional of the surface at a given height. Loess fits are then used to model the structure. The residuals of these fits are called signatures. They are considered to be noise free and a good reflection of the key attributes of the raw marking, and the unique features of a bullet. A more detailed version of the extraction technique of signatures is discussed by \citet{aoas}, where comprehensive details about the height at which profile is to be selected, removing curvature, smoothing, identifying groove locations are explained.

The majority of Bullet profiles and signatures of the Hamby study \citep{hamby} used in this paper are extracted by procedures mentioned by  \citet{aoas}. The scans for the Hamby study come from two sources namely, NIST scans \citep{nist} and scans produced at Iowa State University. Both scans were generated using confocal microscopes but at different resolutions. The NIST database contains scans of the bullet sets Hamby 44 and 252 with the resolution 1.5625 $\mu m$ per pixel. The Iowa State scans are available for only the Hamby 44 bullet set and at a resoultion of 0.065 $\mu m$ per pixel. The data used by \citet{chumbley, hadler} on the other hand, was produced by a profilometer at a resolution of about 0.73 $\mu m$ per pixel. The different resolutions indicate that the length of the digitized markings would be significantly different for each setting.

## Potential Challenges in Chumbley Score Adaptation

Markings made by Screw driver tips \citep{chumbley} are longer and pronounced and about 7 mm in length \citep{manytoolmarks1}, while the groove to groove length of the bullet lands from the Hamby study \citep{hamby} are about 2mm in length. The Ruger pistol barrels used in the  Hamby study have a 9mm caliber. This means that bullets used in these pistols have land markings of almost $(1/4)^{th}$ the size of screwdriver toolmarks used by \citet{chumbley, hadler}, therefore raising a problem of distinctiveness. One way to investigate this is to evaluate bullet striations at different resolutions. This gives us an incentive to use both the NIST and Iowa State scans and investigate the effect of length of the digitized mark on the working of the algorithm. Apart from this, the algorithm developed by \citet{hadler} has only been tested for flatter and wider surfaces which have negligible curvature, whereas striations on bullets are made on their curved surfaces. Therefore, using methods proposed for toolmarks need adaptation in order to give tangible results for bullets. Another challenge that arises from these topographical differences, is the understanding of that effect of different levels of smoothing. Especially investigating its effect on the algorithm performance when signatures are being compared and when raw markings or profiles are being compared.  

Some other challenges associated with the adaptation process call for a brief understanding of the methodology involved. The Chumbley method and algorithm works on the premise of comparison windows that are supposed to represent small segments of pixels of equal sizes in the two markings.  These windows are to be compared in a predetermined fashion and are termed as windows of optimization and validation. The optimization window is selected in the first step called the optimization step where areas/windows of best agreement between the the two markings are found. 

The window of optimization for bullet striations are shorter as bullet signatures in terms of absolute length (in mm), are smaller compared to toolmarks. Even in terms of digitized pixels, there is no straightforward way of establishing what window size is to be chosen. One important way to address this is to keep the number of windows of optimization sufficiently large or not too small. This means, if the number of windows are too small we inadvertently use bigger window sizes as we want to compare all segements of one mark with the segments of the other mark. This leads to not giving enough emphasis to smaller segements of the two marking that might be in high agreement. In terms of absolute length we end up comparing smaller segments between the two markings while dealing with bullet lands, than those being compared for toolmarks. 

For too large window sizes, the weight of small features that would otherwise uniquely classify a signature and hence identify the region of agreement is vastly reduced. On the other hand, this introduces another problem as, if we go too small in the window of optimization, the unique features of the trace segments are lost and all seem similar. The problem of distinctiveness in the structure of bullet striations may also lead to potential failure of the algorithm to identify correct windows of agreements or not be able to identify these windows at all. This may lead to incorrect classification of bullets as coming from the same-source (or matches) or from different source (or non-matches). 

Thus, the comaparison windows have a direct influence on the false positives and false negatives, making identification of the optimum sizes of these windows very important. This raises important questions about how and what parameter settings need to be chosen for bullet land comparison. Something similar is done in \citet{afte-chumbley} for toolmark comparisons of slip-joint pliers where optimum window sizes are determined. There is a need to figure out the best parameter settings which minimizes the errors for unsmoothed markings or profiles and pre-processed signatures. An analysis of these error rates and comparison with other methods will help us understand the adaptibility of the chumbley score to bullets.  

## The Chumbley Score Test

The Chumbley score algorithm takes input as two vectorized processes.  Here the processes are of the form $z(t)$ which is a spatial process for some location indexed with $t$. This means that while $z(t)$ represents any striae, $z(t_1)$ is a realization of the spatial process. $t$ here denotes a vector of equally spaced pixel locations for the striation marks under consideration. The word 'pixel' refers to the resolution of the confocal light microscope. In the case for bullet signatures extracted from NIST ballistics database \citep{nist} of the Hamby study, a pixel corresponds to approximately 1.56 microns. The vectorized processes representing two sets of striaes are therefore shown as $x(t_1)$, $t_1 = 1,2,...T_1$ and $y(t_2)$, $t_2 = 1,2...T_2$. Here $x$ and $y$ denote two instances of the process $z(*)$ which means two striation marks, while the indexing $t$ or pixel range is shown as $t_1$ and $t_2$ for $x$ and $y$ respectively. This representation of $t$ as $t_1$ and $t_2$ lets us identify them as either of same or different lengths. The striation marks under consideration are potentially from two different bullets whose source needs to be identified as being same or different. $T_1$ and $T_2$, as represented above, are the final pixel indexes of each marking and therefore give the respective lengths of the markings. The similarity in the two markings is then judged by the algorithm on the basis of cross-correlation of a fixed and constant number consecutive pixels (say $k$) taken from the two markings. This can for example be $k$ taken from the two indexed marking $x(t_1)$ and $y(t_2)$ such that in theory $k$ remains smaller than length of the two striae or marks. Depending on what stage of the algorithm we are in, matching of different pixel lengths and locations is done. This in the end, effectively compares all possible windows that would guarantee in quantifying the two marks or striae as coming from the same source or not.

The algorithm works in two phases, namely, an optimization step and a validation step, at the end of which a Mann Whitney U statistic is calculated as a measure to differentiate between matches and non-matches. A pre-processing step to the algorithm is to choose a coarseness value which  is used as a parameter to the lowess smoothing function. The coarseness essentially gives the proportion of points which influence the smooth at each value, which means larger values lead to more smoothness. The lowess smoothing is applied to each of two sets of vectorized striae or marks $x(t_1)$ and $y(t_2)$, before proceeding to the algorithm.
The algorithm starts with the optimization step where the area of best agreement in the two markings being compared is identified. Comparison window size is predefined. Each window of the first marking is compared with all windows of the second marking. A maximum correlation statistic is used to identify the region of best agreement, with the maximum usually seen being near 1 for both cases which is what is intuitively expected for matches, and not expected intuitively for non matches. 
\citet{hadler} in their paper proposed an improvement to this algorithm by trying to remove mutual dependence of parameters. The mututal dependence was due to the serial correlation in surface depth values of a marking. Also a random sampling sub step in the validation phase makes a group of pixels to be chosen more than once and hence introduces lack of independence in certain steps. The reason for removing the mutual dependence was the Mann-Whitney U statistic works under the assumption of independence of parameters.
The validation step builds on this with two sub-steps namely, same shift and different shift. As the aim is to come up with a non-parameteric U statistic, \citet{hadler} proposed a normalization procedure in the validation step which goes to some extent to address the issue of mutual dependence. A series of windows are chosen in both the same and different shift sub-steps for the purpose of comparison. Both substeps were modified by \citet{hadler} who introduced a deterministic rule for sampling of windows in the same-shift and different shift as opposed to the originally proposed random sampling by \citet{chumbley}.
In th same shift substep the chosen series of windows are at a common distance (rigid-shifts) from the window earlier identified as the region of best agreement in the optimization step. The correlation of these windows generally turn out to be lower than the maximum correlation window. The significance is that these same shift windows still have large enough correlation values for the two markings being compared that are in reality a match. Any choice of maximum correlation windows (where correlation values are often near 1) for non-matches in the optimization step are also validated by this sub-step. This is because the same-shift correlations would not be anywhere as large for same-shift windows when all windows in the sub-step are compared.
The different shift substep on the other hand gives perspective to the correlation values of the same shift window correlation values. There are no rigid-shifts but different shifts where distance from the maximum correlation window are chosen randomly by \citet{chumbley} and deterministically as per \citet{hadler}. This means that there is an equal possibilty of comparing a trace segment from one marking to any trace segment or window in the second marking.

Neither of above sets of correlation in the two sub-steps are allowed to include the maximum correlation window as identified earlier. Therefore the assumption is that if two markings match each other, the same-shift correlations would be larger than the different-shift windows.
And if they are not a match the correlations in the two sets will be very similar. The U-statistic tests for the null hypothesis that the two markings are not a match and are therefore not made by the same source.  As given by \citet{hadler}, it is computed from the joint rank of all correlations of both the same and different shift samples. 


##################

#### endedit

\hh{end of intro: remaining paper is structured as follows: introduce to the data we get from confocal microscopy, introduce to profiles and signatures.
Introduce to chumbley score method, apply chumbley and discuss results ....
}

## Scans for land engraved areas

- scans available: NIST database (citation), Hamby 44 and Hamby 252 (Hamby citation)
- move figure up
- discuss cross section, profiles and signatures

<!-- Compairing pairs of toolmarks with the intention of matching it to a tool has been studied many times in the past. Extensive examples can be found in literature for tools and toolmark research ranging from screwdrivers \citep{manytoolmarks1, manytoolmarks2, chumbley} to groove pliers \citep{manytoolmarks2} to slip-joint pliers \citep{afte-chumbley} and many more. In comparison to this, same source matching of bullets to firearms has not been examined as prominently as that of toolmarks. Even less information is available on validity of methods and error rates associated with firearms examination. The National Academy of Sciences in its report in 2009 \citep{NAS:2009} discussed the need for determining error rates in methods proposed for firearms examination. As seen in the case of most forensic applications, the first step for same source matching and error rate determination involves identification of unique features that are characteristic of the object at hand. For the case of bullets and firearms, striation marks on the surface of the bullet are considered to be such markings that can be used in methods for same source matching. These marks are often a product of rifling and impurities and defects due to manufacturing in the barrel of the gun, which leads to engravings on the bullet surface \citep{afte-article1992}. In current practice, firearm examiners invariably make visual comparisons of bullet striae and use visual assesment tools to dignify bullets as being matches and non-matches. One way of accomplishing anykind of comparison between bullets is to do a comparison between surface marking of two or more bullet lands. Bullet Lands are considered to be areas between grooves made by the rifling action of the barrel and the markings on them are considered to be unique. The land engraved markings or sometimes termed as Bullet profiles \citep{aoas,ma2004} are striation marks made on Bullet lands and often used for these land to land comparisons. Bullet Signatures is another word used in literature as seen in the work of \citet{chu2013} and \citet{aoas}. In our context bullet signatures refer to a processed version of the raw land engraved markings or profiles. The generation of bullet signatures involves first extraction of a bullet profile by taking the cross-sectional of the surface at a given height and then using loess fits to model the structure. The residuals of this fit are called signatures, which are considered to be noise free and a good reflection of the class charecteristics and unique features of a bullet. A more detailed version of the extraction technique of signatures is discussed by \citet{aoas}, where comprehensive details about the height at which profile is to be selected, removing curvature, smoothing, identifying groove locations are explained. Figure \ref{fig:rgl} shows us how the extracted signature from a bullet land lines up with the image of the land from which it was extracted. We can see also see in the figure how the depth and relative position of the striation markings seen in the image are interpreted as the signature. -->

<!-- As mentioned earlier, subject bias and error rate determination have been a long standing issue in firearm examination \citep{NAS:2009}. This issue of removing subjectivity and making firearm analysis objective  was also raised in the \citet{pcast2016} report. For an objective analysis, scientific principle testability and error rate determination are considered to be defining aspects.  Identification of the uncertainities associated with a method of comparison and coming up with a systemized method of quantification, is therefore important. Associating mathematical probabilites in a systematic manner to these methods are one such way of quantification.  -->
<!-- In a previous study conducted by \citet{aoas} a machine learning based algorithm was developed for same source matching of bullets and error rates were discussed using the database from the Hamby Study \citep{hamby}. The method proposed by \citet{chumbley} (and later improved by \citet{hadler}) also provides a means to determine error rates and claims to reduce subject bias. Therefore, giving us a strong motivation to explore the adaptibility of the Chumbley score methodology to bullets. In this paper, we take this toolmark based methodology and try to apply and adapt it to bullets. After this, we consequently discuss about the efforts in doing so, along with the associated error rates. The data used in this paper also belongs to the Hamby Study \citep{hamby}. This gives us a common platform for comparing the performance of the chumbley method on bullets with the already existing method proposed by \citet{aoas} for bullets. As a brief overview, \citet{chumbley}, in their paper, compare two toolmarks with the intention of determining if it comes from the same tool.  They use an empirical based setup to validate their proposed algorithm and quantitative method which calculates a U-statisic for the purpose of classification of toolmarks as matching or non-matching. The data for their study was obtained from 50 sequentially manufactured screwdriver tips, and preselected comparison window sizes were given as inputs to the algorithm. The algorithm then compares the two toolmarks and comes up with a Mann-Whitney U-statistic and an associated p-value to designate them as matches or non-matches. The performance for every 100 comparisons, of the algorithm proposed by \citet{chumbley} and the improvement proposed by \citet{hadler} are listed in the table below. -->


<!-- ```{r, results='asis', include=FALSE} -->
<!-- library(knitr) -->
<!-- library(kableExtra) -->
<!-- library(xtable) -->

<!-- a1<- matrix(NA, ncol = 3, nrow = 2) -->
<!-- toolmark1.confusion<- data.frame(a1) -->
<!-- colnames(toolmark1.confusion)<- c("Classification", "Match", "Non-Match") -->
<!-- toolmark1.confusion$Classification<- c("Match", "Non-Match") -->
<!-- # toolmark1.confusion$Match<- c(41,2) -->
<!-- # toolmark1.confusion$`Non-Match`<- c(9,48) -->
<!-- toolmark1.confusion$Match<- c(91,2) -->
<!-- toolmark1.confusion$`Non-Match`<- c(9,98) -->

<!-- a2<- matrix(NA, ncol = 3, nrow = 2) -->
<!-- toolmark2.confusion<- data.frame(a2) -->
<!-- colnames(toolmark2.confusion)<- c("Classification", "Match", "Non-Match") -->
<!-- toolmark2.confusion$Classification<- c("Match", "Non-Match") -->
<!-- toolmark2.confusion$Match<- c(47,0) -->
<!-- toolmark2.confusion$`Non-Match`<- c(3,50) -->

<!-- chumbley.confusion<-knitr::kable(toolmark1.confusion, digits=0, format = "latex" ,booktabs=TRUE) #%>% kableExtra::kable_styling(latex_options = c("hold")) -->
<!-- hadler.confusion<-knitr::kable(toolmark2.confusion, digits=0, format = "latex" ,booktabs=TRUE) #%>% kableExtra::kable_styling(latex_options = c("hold")) -->



<!-- cat(c("\\begin{table}[!htb] -->
<!--     \\begin{minipage}{.5\\linewidth} -->
<!--       \\caption{Chumbley et al. 2010} -->
<!--       \\centering", -->
<!--         chumbley.confusion, -->
<!--     "\\end{minipage}% -->
<!--     \\begin{minipage}{.5\\linewidth} -->
<!--       \\centering -->
<!--         \\caption{Hadler et. al. (2017)}", -->
<!--        hadler.confusion, -->
<!--     "\\end{minipage}  -->
<!-- \\end{table}" -->
<!-- )) -->

<!-- ``` -->
<!-- Striation marks are used to determine same source for bullets. Current standards ask for a visual inspection of the marks under a comparison microscope by a firearms examiner. One of the problems raised by the NAS report in 2009 is the subjectivity involved in this comparison and the need for determining error rates \citet{NAS:2009}. The issue of subjectivity of a firearms examiner has been reviewed by many authors where things like scientific principle testability and error rates are considered to be defining aspects for an objective analysis such that without identifying the uncertainities associated with a method of comparison, it is inconclusive to regards an analysis complete. This means that with the usual method of visual comparison that is adopted by firearms examiners, lies the problem of lack of any systemized quantification and as such there seems no certain way of associating it with any kind of mathematical probability. Since determining error rates has been duly noted as a fundamental problem in forensic science \citet{NAS:2009}, there is a need of methods that address this problem. Some of the methods that have been used to estimate error rates in firearm examinations include Automatic cartridge case comparison where same source identification of cartridge cases and associated error rates have been provided by \citet{riva}, although as noted by \citet{aoas}, in this case alignments of striae involves roation of planes, which cannot be generalized for bullets. In case of methods that use machine learning algorithms some methods which are bootsrap based methods often tend to give over-estimated error rates \citet{efron}, but there are alternate error estimation techniques as described by \citet{aoas}, \citet{efron} and  \citet{vorburger2016} which give better results. -->

<!-- In case of methods that do not use machine learning algorithm, matching two striation marks with each other in order to identify if it is from the same source, can also be done using a well defined comparative statistical algorithm. The statistical algorithm is therefore expected  to go through a step by step procedure of identifying the class characteristics i.e striation marks for bullets, choosing the striae, and then follow a systematic procedure of comparing two striae with each other on the basis of criteria like cross-correlation (or others). Such a methodology would make it possible to determine error rates too and give definite results regarding what proportion of cases would the analysis be successful in identifying a match correctly and in how many cases would it end up being a false positive. -->

<!-- Identification criteria like consecutively matching striae (CMS) as first seen by Biasotti 1959 \citet{biasotti} and later mentioned in \citet{chu2013} more often than not include subject bias and are error prone and as seen in the work of \citet{miller} the number of CMS may turn out to be high even when the bullet is not fired by the same firearm. Parameters like cross-correlation factor as seen in the extensive comparisons made by \citep{aoas} seem to perform much better for matching purposes. -->

<!-- An important aspect of same source matching in firearms is identification of bullet microtopographies that are unique to be chosen for a match, Bullet profiles and signatures prove to be such features. The choice of a bullet signature to uniquely define a bullet depends on finding first, a stable region on a Bullet land which minimal noise and many pronounced striation marks. \citet{aoas} -->

<!-- The method employed by \citet{aoas} uses the cross correlation factor as a means to identify the stable region. The markings or striae in this region are supposed to have a high CCF with each other. Therefore a Bullet profile is chosen from this region by taking the cross section at a given height. A loess fit on the bullet profile produces  residuals which are termed as signatures and can be used as a unique identifier that can be used while matching two bullets to a firearm. -->

<!-- Compairing pairs of toolmarks on the otherhand, with the intention of matching it to a tool has been studied relatively more in the past as compared to bullets, and \citet{chumbley} have described in their paper an algorithm and analytic method that compares two toolmarks and come to the conclusion if they are from the same tool or not. The method also determines the error rates, reduces subject bias and designate the two toolmarks as matches or non-matches with respect to a source. \citet{chumbley} used an empirical based setup to validate their proposed analytic and quantitative algorithm. -->

<!-- \citet{chumbley} found that the algorithm gives false-positives in slightly over 2 cases (or 2%) of the time and about 9 false-negatives (or 9%) for every 100 comparisons. \citet{hadler} on the other hand, in the improved version of the algorithm first described by \citet{chumbley} found false-positives for 0 cases and 3 false-negatives (or 6%) for 50 comparisons of knwon matches. -->
<!-- ```{r} -->
<!-- a<- matrix(NA, ncol = 3, nrow = 2) -->
<!-- toolmark.falsepositive<- data.frame(a) -->
<!-- colnames(toolmark.falsepositive)<- c("Classification", "Match", "Non-Match") -->
<!-- toolmark.falsepositive$Classification<- c("Match", "Non-Match") -->
<!-- toolmark.falsepositive$Match<- c(47,0) -->
<!-- toolmark.falsepositive$`Non-Match`<- c(3,50) -->

<!-- knitr::kable(toolmark.falsepositive, digits=0, format = "latex" ,booktabs=TRUE) %>% kableExtra::kable_styling(latex_options = c("hold")) -->
<!-- ``` -->

<!-- ## Potential limitations of the Chumbley Score adaptation to bullets -->
<!-- Bullet striations are different from toolmarks. Bullet markings are typically produced by the barrel on the bullets, while toolmarks can be made by tool tips on different surfaces. Tools like screw driver tips as used by \citet{chumbley}, produce longer and pronounced markings. Bullets on the other hand are much smaller in length, width, and curved in the cross-sectional topography. This means the markings made by barrels as opposed to markings made by tools, may have a problem in distinctiveness. The majority of Bullet profiles and signatures of the Hamby study \citep{hamby} used in this paper are extracted by procedures mentioned by  \citet{aoas}. The Ruger pistol barrels used in the  Hamby study have a 9mm caliber. This means that bullets used in these pistols have land markings of almost $(1/6)^{th}$ the size of screwdriver toolmarks used by \citet{chumbley}.  Also, striations on bullets are made on their curved surfaces, whereas the algorithm developed by \citet{chumbley} and \citet{hadler} has only been tested for flatter and wider surfaces which have negligible curvature. Therefore, using methods proposed for toolmarks may need adaptation in order to give tangible results for bullets. Moreover, in order to get flat bullet signatures and remove the curvatures some kind of smoothing needs to be applied as a pre-step wihch needs further investigation as to whether the level of smoothing does effect the working of the algorithm on Bullets. -->

<!-- The Chumbley method and algorithm works on the premise of comparison windows which are supposed to represent small segments of pixels of equal sizes in the two markings that are to be compared in a predetermined fashion. They term them as windows of optimization and validation. The optimization window is selected in the first step called the optimization step where areas/windows of best agreement between the the two markings are found. The window of optimization for bullet striations are shorter as bullet signatures are smaller compared to toolmarks . The idea is to keep the number of windows of optimization sufficiently large. This means we have shorter trace segments that lets us compare smaller segments of one signature to another than those for toolmarks. Here, trace segments are the partitions of marking with the length of the partition equal to the size of window of optimization. This introduces a problem as, if we go too small in the window of optimization, the unique features of the trace segments are lost and seem similar, while too large sizes vastly reduces the weight of small features that would otherwise uniquely classify a signature and hence identify the region of agreement. The problem of distinctiveness in the structure of bullet striations may also lead to potential failure of the algorithm to identify correct windows of agreements or not be able to identify these windows at all. This may lead to incorrect classification of bullets as matches or non-matches. -->

<!-- Thus, the comaparison windows have a direct influence on the false positives and false negatives, making identification of the optimum sizes of these windows very important. This raises important questions about how and what parameter settings need to be chosen for bullet land comparison. Something similar is done in \citet{afte-chumbley} for toolmark comparisons of slip-joint pliers where optimum window sizes are determined. There is a need to figure out the best parameter settings which minimizes the errors for unsmoothed markings or profiles and pre-processed signatures. An analysis of these error rates and comparison with other methods will help us understand the adaptibility of the chumbley score to bullets.   -->




<!-- ## The Chumbley Score Test -->

<!-- The Chumbley score algorithm takes input as two vectorized processes.  Here the processes are of the form $z(t)$ which is a spatial process for some location indexed with $t$. This means that while $z(t)$ represents any striae, $z(t_1)$ is a realization of the spatial process. $t$ here denotes a vector of equally spaced pixel locations for the striation marks under consideration. The word 'pixel' refers to the resolution of the confocal light microscope. In the case for bullet signatures extracted from NIST ballistics database \citep{nist} of the Hamby study, a pixel corresponds to approximately 1.56 microns. The vectorized processes representing two sets of striaes are therefore shown as $x(t_1)$, $t_1 = 1,2,...T_1$ and $y(t_2)$, $t_2 = 1,2...T_2$. Here $x$ and $y$ denote two instances of the process $z(*)$ which means two striation marks, while the indexing $t$ or pixel range is shown as $t_1$ and $t_2$ for $x$ and $y$ respectively. This representation of $t$ as $t_1$ and $t_2$ lets us identify them as either of same or different lengths. The striation marks under consideration are potentially from two different bullets whose source needs to be identified as being same or different. $T_1$ and $T_2$, as represented above, are the final pixel indexes of each marking and therefore give the respective lengths of the markings. The similarity in the two markings is then judged by the algorithm on the basis of cross-correlation of a fixed and constant number consecutive pixels (say $k$) taken from the two markings. This can for example be $k$ taken from the two indexed marking $x(t_1)$ and $y(t_2)$ such that in theory $k$ remains smaller than length of the two striae or marks. Depending on what stage of the algorithm we are in, matching of different pixel lengths and locations is done. This in the end, effectively compares all possible windows that would guarantee in quantifying the two marks or striae as coming from the same source or not. -->

<!-- The algorithm works in two phases, namely, an optimization step and a validation step, at the end of which a Mann Whitney U statistic is calculated as a measure to differentiate between matches and non-matches. A pre-processing step to the algorithm is to choose a coarseness value which  is used as a parameter to the lowess smoothing function. The coarseness essentially gives the proportion of points which influence the smooth at each value, which means larger values lead to more smoothness. The lowess smoothing is applied to each of two sets of vectorized striae or marks $x(t_1)$ and $y(t_2)$, before proceeding to the algorithm. -->
<!-- The algorithm starts with the optimization step where the area of best agreement in the two markings being compared is identified. Comparison window size is predefined. Each window of the first marking is compared with all windows of the second marking. A maximum correlation statistic is used to identify the region of best agreement, with the maximum usually seen being near 1 for both cases which is what is intuitively expected for matches, and not expected intuitively for non matches.  -->
<!-- \citet{hadler} in their paper proposed an improvement to this algorithm by trying to remove mutual dependence of parameters. The mututal dependence was due to the serial correlation in surface depth values of a marking. Also a random sampling sub step in the validation phase makes a group of pixels to be chosen more than once and hence introduces lack of independence in certain steps. The reason for removing the mutual dependence was the Mann-Whitney U statistic works under the assumption of independence of parameters. -->
<!-- The validation step builds on this with two sub-steps namely, same shift and different shift. As the aim is to come up with a non-parameteric U statistic, \citet{hadler} proposed a normalization procedure in the validation step which goes to some extent to address the issue of mutual dependence. A series of windows are chosen in both the same and different shift sub-steps for the purpose of comparison. Both substeps were modified by \citet{hadler} who introduced a deterministic rule for sampling of windows in the same-shift and different shift as opposed to the originally proposed random sampling by \citet{chumbley}. -->
<!-- In th same shift substep the chosen series of windows are at a common distance (rigid-shifts) from the window earlier identified as the region of best agreement in the optimization step. The correlation of these windows generally turn out to be lower than the maximum correlation window. The significance is that these same shift windows still have large enough correlation values for the two markings being compared that are in reality a match. Any choice of maximum correlation windows (where correlation values are often near 1) for non-matches in the optimization step are also validated by this sub-step. This is because the same-shift correlations would not be anywhere as large for same-shift windows when all windows in the sub-step are compared. -->
<!-- The different shift substep on the other hand gives perspective to the correlation values of the same shift window correlation values. There are no rigid-shifts but different shifts where distance from the maximum correlation window are chosen randomly by \citet{chumbley} and deterministically as per \citet{hadler}. This means that there is an equal possibilty of comparing a trace segment from one marking to any trace segment or window in the second marking. -->

<!-- Neither of above sets of correlation in the two sub-steps are allowed to include the maximum correlation window as identified earlier. Therefore the assumption is that if two markings match each other, the same-shift correlations would be larger than the different-shift windows. -->
<!-- And if they are not a match the correlations in the two sets will be very similar. The U-statistic tests for the null hypothesis that the two markings are not a match and are therefore not made by the same source.  As given by \citet{hadler}, it is computed from the joint rank of all correlations of both the same and different shift samples.  -->

<!-- The data that is used by \citet{chumbley} is generated by a surface profilometer that gives the height in terms of distance along a linear trace. This is taken perpendicular to the striations present in the toolmark. Two such trace are then compared using the algorithm. -->

<!-- ## Detailed algorithm -->

<!-- ###Optimization step -->
<!-- The idea behind this step is to first identify the area of best agreement in the two toolmark data. Comparison window size is predefined by the user, which in case of screwdriver toolmarks was chosen by Chumbley et al and Hadler et al as around 10 percent of the length of the toolmarks, which was around 500. This window is henceforth referred to as Window of optimization. -->

<!-- A maximum correlation statistic is used to identify the region of best agreement, with the maximum usually seen being near 1 for both cases which is what we intuitively expect for matches, and something which we do not intuitively expect for non matches  -->

<!-- First, there are a very large number of cross-correlations calculated for the two series of striae or marks $x(t_1)$ and $y(t_2)$, for eg if the window of optimization was defined as 200 and toolmark pixel length is around 1000 then we have 1200-(200-1) = 1001.  -->

<!-- This is the number of windows we have for one toolmark and each window is compared with all windows of the second toolmark (the number of windows is again similar), and the window with the maximum correlation is identified to be the region where the toolmarks are in maximum agreement with each other -->


<!-- ###Validation Step -->

<!-- ####Same-shift:  -->
<!-- In this step a series of windows are chosen at random (originally by \citet{chumbley}) and deterministically (by \citet{hadler}), but at a common distance (rigid-shift) from the window identified as the region of best agreement in the vaildation step. -->
<!-- The correlation of these windows would be as intuitively assumed i.e. lower than the maximum correlation window (Optimization step), but the significance is that these same shift windows will still have large enough correlation values for two toolmarks or signatures that are in reality a match. -->

<!-- This also validates that if in the optimization step the maximum correlation window chosen (with correlation value near 1) was by accident (like in case of signatures that in reality are a non-match), all these same- shift correlations (A fixed number of these trace segments are identified) would not be anywhere large enough for all same-shift windows. -->

```{r win-comparison, fig.width=8, fig.height=5, out.width='\\textwidth', fig.cap=" The two plots on the left show how the same shift behaves in case of a matching pair and the two plots on the right show how the different shift behaves in case of a matching pair."}

asd<- all %>% filter(wo==120, wv==50, match == TRUE, land1_id == 1)

sigs_graphics<- readRDS("../data/sigs_generate_window.rds")

sigs_graphics.match<- as.data.frame(sigs_graphics[1])
sigs_graphics.nonmatch<- as.data.frame(sigs_graphics[2])
colnames(sigs_graphics.match)<- gsub("^.*\\.","", colnames(sigs_graphics.match))
colnames(sigs_graphics.nonmatch)<- gsub("^.*\\.","", colnames(sigs_graphics.nonmatch))

# Matching signature
#gi<-
#sigs_graphics.match<- sigs_graphics.match %>% filter(land_id == c(1,9))  
#sigs_graphics.match<- sigs_graphics.match %>% select(y, l30, land_id)
d1<- sigs_graphics.match[which(sigs_graphics.match$land_id==1,arr.ind = TRUE),]
d2<- sigs_graphics.match[which(sigs_graphics.match$land_id==19,arr.ind = TRUE),]
data1<-d1 %>%select(l30)
#data1<- data1$y
data2<- d2 %>%select(l30)
#data2<- data2$y
window_opt = 120 
window_val = 50
coarse = 1
data1<- matrix(unlist(data1))
data2<- matrix(unlist(data2))

  unity <- function(x) {x / sqrt(sum(x^2))} ## normalize columns of a matrix to make correlation computation faster
  
  ####################################################
  ##Clean the marks and compute the smooth residuals##
  ####################################################
  
  data1 <- matrix(data1[round((0.01*nrow(data1))):round(0.99*nrow(data1)),], ncol = 1)
  data2 <- matrix(data2[round((0.01*nrow(data2))):round(0.99*nrow(data2)),], ncol = 1)
  
  ##Normalize the tool marks
  y1 <- data1 - lowess(y = data1,  x = 1:nrow(data1), f= coarse)$y
  y2 <- data2 - lowess(y = data2,  x = 1:nrow(data2), f= coarse)$y
  
  
  ############################################
  ##Compute the observed maximum correlation##
  ############################################
  
  #####################
  ##Optimization step##
  #####################
  ##Each column in these matrices corresponds to a window in the respective tool mark
  y1_mat_opt <- matrix(NA, ncol = length(1:(length(y1) - (window_opt - 1))), nrow = window_opt)
  for(l in 1:(length(y1) - (window_opt - 1))){
    y1_mat_opt[,l] <- y1[l:(l+(window_opt - 1))]
  }
  y2_mat_opt <- matrix(NA, ncol = length(1:(length(y2) - (window_opt - 1))), nrow = window_opt)
  for(l in 1:(length(y2) - (window_opt - 1))){
    y2_mat_opt[,l] <- y2[l:(l+(window_opt - 1))]
  }
  
  ##Compute the correlation between all pairs of windows for the two marks
  ##Rows in the following matrix are mark 2, columns are mark 1
  y2_mat_opt <- apply(scale(y2_mat_opt), 2, unity)
  y1_mat_opt <- apply(scale(y1_mat_opt), 2, unity)
  corr_mat_opt <- t(y2_mat_opt) %*% y1_mat_opt ##correlation matrix
  max_corr_opt_loc <- which(corr_mat_opt == max(corr_mat_opt), arr.ind = TRUE) ##pair of windows maximizing the correlation
  

  s1.rect.opt <- data.frame(xmin=max_corr_opt_loc[1,2], xmax= max_corr_opt_loc[1,2]+window_opt, ymin=-Inf, ymax=Inf)
    s1.rect.sameshift1 <- data.frame(xmin=max_corr_opt_loc[1,2]-window_val, xmax= max_corr_opt_loc[1,2]- 2*window_val, ymin=-Inf, ymax=Inf)
       s1.rect.sameshift2 <- data.frame(xmin=max_corr_opt_loc[1,2]+window_opt+ window_val, xmax= max_corr_opt_loc[1,2]+ window_opt+2*window_val, ymin=-Inf, ymax=Inf)
       
  s2.rect.opt <- data.frame(xmin=max_corr_opt_loc[1,1], xmax= max_corr_opt_loc[1,1]-window_opt, ymin=-Inf, ymax=Inf)
    s2.rect.sameshift1 <- data.frame(xmin=max_corr_opt_loc[1,1]-window_opt-window_val, xmax= max_corr_opt_loc[1,1]-window_opt- 2*window_val, ymin=-Inf, ymax=Inf)
       s2.rect.sameshift2 <- data.frame(xmin=max_corr_opt_loc[1,1]+ window_val, xmax= max_corr_opt_loc[1,1]+ 2*window_val, ymin=-Inf, ymax=Inf)
       
p1<-  ggplot(data = d1, aes(x = y, y = l30)) + geom_line() + 
  geom_rect(data=s1.rect.opt, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "black",alpha=0.5, inherit.aes = FALSE)+
    geom_rect(data=s1.rect.sameshift1, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "red",alpha=0.1, inherit.aes = FALSE)+
    geom_rect(data=s1.rect.sameshift2, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "blue",alpha=0.1, inherit.aes = FALSE)+
  theme_bw() + labs(y = "Signature Land Id = 1")
  
 p2<-   ggplot(data = d2, aes(x = y, y = l30)) + geom_line() + 
   geom_rect(data=s2.rect.opt, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "black",alpha=0.5, inherit.aes = FALSE)+
    geom_rect(data=s2.rect.sameshift1, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "red",alpha=0.1, inherit.aes = FALSE)+
    geom_rect(data=s2.rect.sameshift2, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "blue",alpha=0.1, inherit.aes = FALSE)+
  theme_bw() + labs(y = "Signature Land Id = 9")
 ##########
 
 d1.rect.opt <- data.frame(xmin=max_corr_opt_loc[1,2], xmax= max_corr_opt_loc[1,2]+window_opt, ymin=-Inf, ymax=Inf)
    d1.rect.sameshift1 <- data.frame(xmin=max_corr_opt_loc[1,2]-2*window_opt, xmax= max_corr_opt_loc[1,2]-2*window_opt-window_val, ymin=-Inf, ymax=Inf)
       d1.rect.sameshift2 <- data.frame(xmin=max_corr_opt_loc[1,2]+window_opt+ window_val, xmax= max_corr_opt_loc[1,2]+ window_opt+2*window_val, ymin=-Inf, ymax=Inf)
       
  d2.rect.opt <- data.frame(xmin=max_corr_opt_loc[1,1], xmax= max_corr_opt_loc[1,1]-window_opt, ymin=-Inf, ymax=Inf)
    d2.rect.sameshift1 <- data.frame(xmin=max_corr_opt_loc[1,1]-window_opt-window_val, xmax= max_corr_opt_loc[1,1]-window_opt- 2*window_val, ymin=-Inf, ymax=Inf)
       d2.rect.sameshift2 <- data.frame(xmin=max_corr_opt_loc[1,1]+ 4*window_val, xmax= max_corr_opt_loc[1,1]+ 5*window_val, ymin=-Inf, ymax=Inf)
       
p3<-  ggplot(data = d1, aes(x = y, y = l30)) + geom_line() + 
  geom_rect(data=d1.rect.opt, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "black",alpha=0.5, inherit.aes = FALSE)+
    geom_rect(data=d1.rect.sameshift1, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "red",alpha=0.1, inherit.aes = FALSE)+
    geom_rect(data=d1.rect.sameshift2, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "blue",alpha=0.1, inherit.aes = FALSE)+
  theme_bw() + labs(y = "Signature Land Id = 1")
  
 p4<-   ggplot(data = d2, aes(x = y, y = l30)) + geom_line() + 
   geom_rect(data=d2.rect.opt, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "black",alpha=0.5, inherit.aes = FALSE)+
    geom_rect(data=d2.rect.sameshift1, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "red",alpha=0.1, inherit.aes = FALSE)+
    geom_rect(data=d2.rect.sameshift2, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "blue",alpha=0.1, inherit.aes = FALSE)+
  theme_bw() + labs(y = "Signature Land Id = 9")
  

multiplot(p1, p2, p3, p4, cols=2)
```


<!-- ####Different Shift: -->
<!-- Primary reason for this substep is to give perspective to the correlation values of the same shift window correlation values. -->

<!-- This time there are no rigid-shifts but different shifts (distance from Window of Opt with max correlation) chosen randomly by \citet{chumbley} and deterministically by Hadler et al. such that there is an equal possibilty of comparing a trace segment from one signature or toolmark to any one in the second signature or toolamrk. -->

<!-- Neither of above sets of correlation are allowed to include the maximum correlation window as identified earlier. -->

<!-- Therefore the assumption is that if two toolmarks or signatures match each other the same-shift correlations would be larger than the different-shift windows, -->
<!-- and if they are not a match the correlations in the two sets will be very similar. -->

<!-- ###U Statistic: -->

<!-- This is computed from the joint rank of all correlations of both the same and different shift samples. As given by \citet{hadler} -->

<!-- Null Hypothesis: If the toolmarks were not match i.e not made by the same tool. -->

<!-- Let ns and nd be the number of same shift and different shift windows -->
<!-- $$N = n_{s} + n_{d}$$ -->

<!-- The mann whitney U statistic is given by -->
<!-- $$U =\sum^{ns}_{i=1}R_{s}\left( i\right)$$ -->

<!-- with the standardized version which includes provision for rank ties -->

<!-- $$\overline{U}= \dfrac{U-M}{\sqrt{V}}$$ -->

<!-- where prior to normalization the U-statistic has the mean as -->

<!-- $$M = n_{s}\left(\dfrac{N+1}{2}\right)$$ -->

<!-- and variance  -->

<!-- $$V = \dfrac{n_{s}n_{d}}{N\left(N-1\right)}\left[\Sigma^{n_{s}}R_{s}\left(i\right)^{2}+\Sigma^{n_{d}}R_{d}\left(j\right)^{2}\right] -\dfrac{n_{s} n_{d}\left(N+1\right)^{2}}{4\left(N-1\right)}$$ -->


# Simulation setup

Following on similar lines to the setup of toolmarks, the first step here is to first identify what difference does different window sizes of optimization and the validation step have, when adapting the toolmark method to bullets.

The marking made on bullets are smaller than toolmarks and is also less wider. The idea is to find out possible areas of error while adapting the score based method proposed for toolmarks, using cross-validation setup to identify appropriate parameter settings for (a) signatures and (b) profiles directly

### Signatures

Signatures of lands for all Hamby-44 and Hamby-252 scans made available through the NIST ballistics database \citep{nist} were considered. Both of these sets of scans are part of the larger Hamby study \citep{hamby} and each consist of twenty known bullets (two each from ten consecutively rifled Ruger P85 barrels) and fifteen questioned bullets (each matching one of the ten barrels). Ground truth for both of these Hamby sets is known and was used to assess correctness of the tests results. 

Bullet signatures being compared at this time are therefore from the Hamby 44 and Hamby 252 data. The database setup and pre-processing system used for choosing the Bullet signatures are as described by \citet{aoas}. In order to choose the bullet signatures we first filter out Land_id for Profiles from the Hamby 44 and Hamby 252 data and remove all NA values. Then run_id = 3 is chosen as the signatures generated from this run_id give the closest match. Different run_id's have some different settings for generating the signatures.(The level of smoothing does not seem to be one of them)

The bullet signatures when generated by this process already includes a loess smoothing. Therefore, the coarseness factor is set to 1 while running the chumbley non random algorithm for comparing different optimization windows.The algorithm generates the same_shift, different_shift, U-Stat and P_value parameters which are then used to calculate the errors associated with different sets of window sizes.

### Profiles

The profiles are cross-sectional values of the the bullet striation mark which are chosen at an optimum height (x as used by \citet{aoas}). This x or height is not a randomly chosen level. The rationale behind the choice has been explained by \citet{aoas}. A region is first chosen where the cross-correlation seems to change very less and in this region an optimum height is chosen. The profiles generally resemble a curve which is more or less similar to a quadratic curve (a quadratic fit to the raw data values of the profile is not an exact fit but it does show a similar trend). Profiles are the set of raw values representing the striation marks, and signatures are generated from these by removal of the inherent curvature and applying some smoothing (the signatures generated by \citet{aoas} use a loess function for smoothing). 

Similar to signatures the run_id = 3 was used when applying the chumbley algorithm using the database setup given by \citet{aoas} of Hamby-44 and Hamby-252 datasets, on the profiles. The run_id not only defines the level of smoothing but also signifies the chosen height at which the profiles were selected initially. Another important aspect is the range of horizontal values (which is referred to as the y values in \citet{aoas}) in the signatures. These have already been pre-processed in the database to not include any grooves.

Therefore for the sake of comparison the run_id = 3 is still chosen so as to ensure that the horizontal values remain the same as that of the signatures. This also gives us profiles with the grooves removed.

The idea therefore is to first use these raw values of the profile directly in the chumbley algorithm, and see how the algorithm performs for different coarseness values (smoothing parameter as referred in the function lowess used in the chumbley algorithm).

\pagebreak

# Results
We used the adjusted Chumbley method as proposed in \citet{hadler} and implemented in the R package `toolmaRk` \citep{toolmark} on all pairwise land-to-land comparisons of the Hamby scans (a total of 85,491 comparisons) with the pairwise sets for the comparisons given in the table \ref{tab:param}.
```{r param}
param_setting<- all %>% select(wo, wv) %>% group_by(wo, wv) %>% tally() %>% select(wo,wv)
#param_setting<- data.frame(matrix(param_setting, nrow = 16))
#t(param_setting)
#param_setting %>% tally() 
library(kableExtra)
#knitr::kable(t(param_setting)[,1:17], digits=0, caption = "Overview of parameter settings used for optimization and validation windows for bullet land signatures.", format = "latex" ,booktabs=TRUE) %>% kableExtra::kable_styling(latex_options = c("scale_down", "hold"))

par_tab1<-t(param_setting)[,1:18]
par_tab2<-t(param_setting)[,19:33]
knitr::kable(list(par_tab1,par_tab2), digits=0,caption = "Overview of parameter settings used for optimization and validation windows for bullet land signatures.", format = "latex" ,booktabs=TRUE) %>% kableExtra::kable_styling(latex_options = c("scale_down","hold"))%>% kableExtra::kable_styling(latex_options = c("scale_down","hold")) #%>% group_rows("",1,2) %>% group_rows("",3,4)

```

```{r confusion}
library(kableExtra)
a27<- all %>% filter(wo==280, wv==30) %>% xtabs(data= ., ~signif+match)
a27<- as.data.frame(a27)
a27$Type<- c("True Negative","False Positive (Type I)","False Negative (Type II)","True Positive")

a28<- all %>% filter(wo==120, wv==30) %>% xtabs(data= ., ~signif+match)
a28<- as.data.frame(a28)
a28$Type<- c("True Negative","False Positive (Type I)","False Negative (Type II)","True Positive")

a29<- all %>% filter(wo==80, wv==30) %>% xtabs(data= ., ~signif+match)
a29<- as.data.frame(a29)
a29$Type<- c("True Negative","False Positive (Type I)","False Negative (Type II)","True Positive")

knitr::kable(list(a27,a28,a29), digits=0, caption = "Confusion Table for different optimization window sizes with validation window size as 30.", format = "latex" ,booktabs=TRUE)%>% kableExtra::kable_styling(latex_options = c("hold"), "striped", full_width = F) %>% group_rows("Size of Optimization Window = 280",1,5) %>% group_rows("Size of Optimization Window = 120", 6, 10) %>%  group_rows("Size of Optimization Window = 80", 11,14)
```


## Signatures


Figure \ref{fig:type2} gives an overview of type II error rates observed when varying the window size in the optimization step. Two levels of validation window size 30 and 50 were chosen as to compare the error rates for different nominal type I errors. We notice that the trends for these nominal type I errors are similar and in most cases a validation window of 50 has higher type II error than for 30. A change in this trend is seen for a 0.05 $\alpha$ level, although the difference between the two windows is very small for this case. We can also notice an obvious trend of increase in the Type II error as the window of optimization increases and see a minimum around the optimization window size of 120 pixels. Hence we are inclined to choose a smaller validation window size and optimization window as 120.


Table \ref{tab:confusion} shows the confusion tables with the classification of type I and type II errors and how the numbers change with a change in the optimization window. The windows represent areas to the left of the window with minimum type II, near the minimum type II window and to the far right of the minimum type II error

```{r type2, fig.width=8, fig.height=5, out.width='\\textwidth', fig.cap="Type II error rates observed across a range of window sizes for optimization $wo$. For a window size of $wo = 120$ we see a drop in type II error rate across all type I rates considered. Smaller validation sizes $wv$ are typically associated with a smaller type II error."}
errors %>% filter(wv %in% c(30,50)) %>%
  ggplot(aes(x = wo, y = beta, colour=factor(alpha))) + 
  geom_point(aes(shape=factor(wv))) +
#  facet_grid(.~wo, labeller="label_both") + 
  geom_smooth(aes(group=alpha), se=FALSE, method="loess") +
  theme_bw() +
  scale_colour_brewer(expression("Nominal\ntype I error"~alpha), palette="Set2") +
  scale_shape_discrete("Size of validation\nwindow, wv") +
  xlab(expression("Window size for optimization, "~wo)) +
  ylab("Type II error rate")


labels = expression(P[M1](tilde(z)>0),P[M0](tilde(z)>0))
```

Figure \ref{fig:type1} compares nominal (fixed) type I error and actually observed type I errors for the parameter settings in table \ref{tab:param}. With an increasing size of the window used in the optimization step the observed type I error rate decreases (slighty). This means as the optimization window increase the observed type I error rate gets smaller. A smaller validation window on the other hand, tends to be associated with a higher type I error rate. This can be better imagined for a given window of optimization, where the actual Type I error is comparable to the nominal level for only a select few validation window sizes. For these comparable validation window sizes of 30 and 50 as done here, the actual type I error increases very slightly and can be seen in Figure \ref{fig:type1}. This increase is not as much when compared to the variation seen with the optimization window sizes. This effect might be related to the increasing number of tests that fail for larger optimization window sizes, in particular for non-matching striae (see fig \ref{fig:missings}).

```{r type1, fig.width=8, fig.height=5, out.width='\\textwidth', fig.cap="Comparison of observed and nominal type I error rates  across a range of window sizes for optimization $wo$. The horizontal line in each facet indicates the nominal type I error rate."}
errors %>% filter(wv %in% c(30,50)) %>%
  ggplot(aes(x = wo, y = actual, colour=factor(alpha))) + 
  geom_hline(aes(yintercept=alpha), colour="grey30") +
  geom_point(aes(shape=factor(wv))) +
  #  facet_grid(.~wo, labeller="label_both") + 
  geom_smooth(aes(group=alpha), se=FALSE, method="lm") +
  theme_bw() +
#  scale_y_log10(breaks=c(0.001,.005, 0.01, 0.05)) +
  scale_shape_discrete("Size of validation window, wv") +
  ylab("Observed type I error rate") +
  xlab(expression("Window size for optimization, "~wo)) +
  scale_colour_brewer(expression("Nominal type I error "~alpha), palette="Set2") +
  facet_wrap(~alpha, labeller="label_both", scales="free") +
  theme(legend.position="bottom")
  
```


```{r wo120, fig.width = 4, fig.height=4, fig.cap="The figure on the left shows the actual Type I error while the figure on the right shows the Type II error for different validation window sizes and different chosen nominal alpha levels when the size of the optimization window = 120"}
wo120 <- errors %>% filter(wo==120)
wo120 %>% gather(type, error, beta) %>%#actual:beta) %>%
  ggplot(aes(x = wv, y = error, colour = factor(alpha)))  +
  geom_line() + geom_point() + facet_wrap(~type, scales="free") +
  theme_bw()+
  theme(legend.position="bottom") +  ggtitle("Signatures" , subtitle = "Varying validation window sizes, Optimization window = 120")
```

The actual type I error and type II error for signatures were also compared for different validation window sizes. Figure \ref{fig:wo120} shows the actual rates for different nominal $\alpha$ levels. We can see that the type II error rises with higher validation windows for the smaller nominal $\alpha$ levels while for the nominal $\alpha$ = 0.05 its almost constant.
```{r }
ns <- all %>% group_by(wv, wo, match) %>% summarize(
  n_ss = mean(same_shift_n),
  n_ds = mean(diff_shift_n),
  zero_ss = sum(same_shift_n==0),
  zero_ds = sum(diff_shift_n==0),
  miss = sum(is.na(p_value)),
  missperc = miss/85491*100
)

nsp <- allp %>% group_by(wv, wo, match, coarse) %>% summarize(
  n_ss = mean(same_shift_n),
  n_ds = mean(diff_shift_n),
  zero_ss = sum(same_shift_n==0),
  zero_ds = sum(diff_shift_n==0),
  miss = sum(is.na(p_value)),
  missperc = miss/85491*100
)

ns_sig_c <- all_sig_c %>% group_by(wv, wo, match, coarse) %>% summarize(
  n_ss = mean(same_shift_n),
  n_ds = mean(diff_shift_n),
  zero_ss = sum(same_shift_n==0),
  zero_ds = sum(diff_shift_n==0),
  miss = sum(is.na(p_value)),
  missperc = miss/85491*100
)

```

```{r missings, fig.width=6, fig.height=4, fig.cap="Number of failed tests by the window optimization size, wo, and ground truth.",fig.align = "center"}
#ns1<- ns[which(ns$wv == c(30,50), arr.ind = T),]
missings<- ns %>% filter(wv %in% c(30, 50)) %>%
#ns %>% filter(wv == c(30, 50)) %>%
  ggplot(aes(x = wo, y = missperc, shape=factor(wv), colour = factor(match)), size=.3) + 
  geom_smooth(method="lm", size=.2, se=FALSE) +
  geom_point() + 
  theme_bw() +
  ylab("Percent of failed tests.") +
  xlab("Size of optimization window, wo") +
  scale_colour_brewer("Match", palette="Set1") +
  scale_shape_discrete("Size of \nvalidation\nwindow, wv")
missings
```

```{r lms, results='asis'}
nsnest <- ns %>% filter(wv %in% c(30,50)) %>% group_by(match, wv) %>% nest()
nsnest <- nsnest %>% mutate(
  model = data %>% purrr::map(.f = function(d) lm(I(missperc*100)~wo, data=d)),
  coefs = model %>% purrr::map(.f = broom::tidy)
)
tab <- nsnest %>% unnest(coefs) %>% filter(term=="wo") %>% arrange(match) %>%
  select(-term, -statistic, -p.value)
knitr::kable(tab, digits=3, caption = "Estimates of the increase in percent of failed tests corresponding to a 100 point increase in the optimization window.", booktabs=TRUE)
```
Figure \ref{fig:missings} gives an overview of the number of failed tests, i.e. tests in which a particular parameter setting did not return a valid result. This happens, when the shift to align two signatures is so large, that the remaining overlap is too small to accommodate  windows for validation. The problem is therefore exacerbated by a larger validation window. Figure \ref{fig:missings} also shows that the number of failed tests is approximately linear in the size of the optimization window. Test results from different sources have a much higher chance to fail, raising the question, whether failed tests should be treated as rejections of the null hypothesis of same source. For known non-matches there is a higher possibility that in the optimization pair of windows where cross-correlations are maximum are too far apart, and same shifts of this order hit the end of the signature. 

## Profiles

Figure \ref{fig:prof_missings} (a) shows the type II error rates for profiles for the optimization window 120 and validation window 30 with varying level of coarseness. We can see that the type II error for all the nominal $\alpha$ levels is lowest in the range of 0.20 to 0.35. Therefore, a value of 0.25 can be used keeping in mind it keeps the type II error lowest while runnning simulations. Thus for comparisons of different window sizes etc as seen in the differnt parts of Figure \ref{fig:prof_missings} this coarseness value is used.

On the other hand Figure \ref{fig:prof_missings} (b) shows if the coarseness level set in the chumbley agorithm has any effect on the signatures, which are pre-processed and already smoothed to a certain extent. From Figure \ref{fig:prof_missings} (b) we can notice that for different nominal $\alpha$ levels, the type II error fluctuates slightly but does not change much, thereby helping us conclude that the coarseness levels set in the LOWESS smoothing in the chumbley alggorithm does effect the type II error much for signatures.

### Comparison of profiles and signatures

Another reason for failed tests can be incorrect identification of maximum correlation windows in the optimization step as seen in figure \ref{fig:prof_missings}(d) because of the level of smoothing, as too much smoothing would subdue intricate features that might otherwise help in the correlation calculations and correct identification of maximum correlation windows irrespective of the size. This would again cause a simiar effect as explained for figure \ref{fig:missings} with validation windows, irrespective of size, during the shifts end up at the ends of the markings resulting in an invalid calculation and failed comparison attempt. 

In figure \ref{fig:prof_missings}(d) and (f), we compare profiles and signatures on the basis of number of failed tests. The profiles chosen for figure \ref{fig:prof_missings}(f) have a constant coarseness of 0.25 and window of optimization as 120. The signatures in this case are not smoothed using the chumbley algorithm step of LOWESS smoothing. Instead signatures are used as calculated by \citet{aoas}. The smoothing in these signatures were determined and fixed on the basis of their performance in the random forest based algorithm proposed by \citet{aoas}. The comparison of profiles and signatures with variation of validation window size therefore is made on even footing. The trends are similar to figure \ref{fig:missings} in the sense that for known non-matches the number of failed tests are more for both signatures and profiles and increasing linearly with the validation window size. The problem is however, worse for profiles which has higher number of failed tests than signatures for all validation windows.

The total error for different validation window sizes for signatures and profiles can be seen in figure \ref{fig:prof_missings} (e).The optimization window size is 120 and profiles are calculated at a default 0.25 coarseness level while signatures as before are not smoothed again in the modified chumbley algorithm. We can see that the total error is always higher for profiles as compared to signatures for all sizes of validation window. 

```{r, fig.cap="Type II errors for different levels of coarseness."}
e1<-errors2 %>% filter(wv==30)%>% ggplot(aes(x = coarse, y=beta, colour=factor(alpha))) +
  geom_point() + geom_line() +
  theme_bw() +
  scale_colour_brewer(expression("Nominal\ntype I error"~alpha), palette="Set2") + xlab("Profile Coarseness")+
  ylab("Type II error") + ggtitle(label = "(a)")

e2<- errors_sig_c %>% filter(wv==30)%>% ggplot(aes(x = coarse, y=beta, colour=factor(alpha))) +
  geom_point() + geom_line() +
  theme_bw() +
  scale_colour_brewer(expression("Nominal\ntype I error"~alpha), palette="Set2")+ xlab("Signature Coarseness") + ylab("Type II error") + ggtitle(label = "(b)")

#multiplot(e1,e2, cols = 1)

```



```{r}
#errors %>% ggplot(aes(x = wv, y=total, colour=factor(alpha), shape="signatures")) + geom_point() + facet_grid(~wo) +geom_point(data=errors2, aes(shape="profiles"))

total1<- errors_sig_c %>% ggplot(aes(x = coarse, y=total, colour=factor(alpha), shape="signatures")) + geom_point() + geom_line()  + geom_point(data= errors2 %>% filter(wv==30), aes(shape="profiles")) + geom_line(data=errors2 %>% filter(wv==30), aes(shape="profiles")) + theme_bw() +
  scale_colour_brewer(expression("Nominal\ntype I error"~alpha), palette="Set2") + xlab("Coarseness") +ylab("Total error") + ggtitle(label = "(c)")

total2<- wo120 %>% filter(wv>10) %>% ggplot(aes(x = wv, y=total, colour=factor(alpha), shape="signatures")) + geom_point() + geom_line() +geom_point(data=errors2 %>% filter(wv>10) %>% filter(coarse==0.25), aes(shape="profiles")) + geom_line(data=errors2 %>% filter(wv>10)%>% filter(coarse==0.25), aes(shape="profiles")) + theme_bw() +
  scale_colour_brewer(expression("Nominal\ntype I error"~alpha), palette="Set2") + ylab("Total error") + ggtitle(label = "(e)")


```

```{r prof_missings, fig.width=10, fig.height=10, fig.cap="Row 3:  Total error and Number of failed tests by the window validation size, wv, and ground truth, Row 2: Total error and Number of failed tests with Coarseness for both profiles and signatures, Row 1: Type II error for different coarseness levels as used in the modified chumbley algorithm for profiles and signatures"}


#ns1<- ns[which(ns$wv == c(30,50), arr.ind = T),]
miss_1<- nsp %>% filter(wv == 30) %>%
#ns %>% filter(wv == c(30, 50)) %>%
  ggplot(aes(x = coarse, y = missperc, shape= "profile", colour = factor(match))) + 
  #geom_smooth(method="auto", size=.5, se=FALSE) +
  geom_point(size = 2) + 
  theme_bw() +
  ylab("Percent of failed tests.") +
  xlab("Coarseness") +
  scale_colour_brewer("Match", palette="Set1") + geom_point(data=ns_sig_c  %>% filter(wv %in% c(30,50)) , aes(shape="signature"), size = 2) + ggtitle(label = "(d)")
# + scale_colour_brewer("Match", palette="Set2") #+geom_smooth(method="auto", size=.5, se=FALSE) 
  #scale_shape_discrete("Size of \nvalidation\nwindow, #wv")

miss_2<- nsp %>% filter(coarse == 0.25) %>%
#ns %>% filter(wv == c(30, 50)) %>%
  ggplot(aes(x = wv, y = missperc, shape= "profile", colour = factor(match))) + 
  #geom_smooth(method="auto", size=.5, se=FALSE) +
  geom_point(size = 2) + 
  theme_bw() +
  ylab("Percent of failed tests.") +
  xlab("wv") +
  scale_colour_brewer("Match", palette="Set1") + geom_point(data=ns %>% filter(wo == 120)  %>% filter(coarse == 1) , aes(shape="signature"), size = 2) + ggtitle(label = "(f)")#+ scale_colour_brewer("Match", palette="Set2")

#multiplot(plotlist = list(total1, total2, miss_1, miss_2, e1, e2), cols = 2)#, matrix(c(1,2,3,3), nrow=2, byrow=TRUE))
multiplot(plotlist = list(e1, total1, total2,e2, miss_1, miss_2), cols = 2)
```
 \pagebreak

## Conclusion

The results suggest that the Nominal type I error $\alpha$ value shows dependence on the size of the window of optimization. For a given window of optimization the actual Type I error is comparable to the nominal level for only a select few validation window sizes and for comparable validation window sizes of 30 and 50 as done here, the actual type I error does not seem to vary as much as it varies with the optimization window sizes .
A Test Fail, i.e. tests in which a particular parameter setting did not return a valid result, happens, when the shift to align two signatures is so large, that the remaining overlap is too small to accommodate  windows for validation, depends on whether known-match or known non-matches has predictive value, with test results from different sources having a much higher chance to fail. On conducting an analysis of all known bullet lands using the adjusted chumbley algorithm, Type II error was identified to be least bad for window of validation 30 and window of optimization 120. In case of unsmoothed raw marks (profiles), Type II error increases with the amount of smoothing and least for LOWESS smoothing coarseness value about 0.25 or 0.3. In an effort to identify the level of adaptiveness of the algorithm, comparisons were made between signatures and profiles. Their comparison with respect to validation window size for a fixed optimization window size suggested that, profiles have a total error (i.e all incorrect classification of known-matches and known non-matches) greater than or equal to the total error of signatures for all sizes of validation window. Profiles also fail more number of times than signatures in a test fail (for different coarseness keeping windows fixed and also for different validation windows keeping coarseness fixed) which lets us conclude that the behaviour of the algorithm for the profiles instead of pre-processed signatures is not better. Finally it should be noted that the current version of the adjusted chumbley algorithm seems to fall short when compared to other machine-learning based methods \citet{aoas}, and some level of modification to the deterministic algorithm needs to be identified and tested that would reduce the number of incorrect classifications.

```{r wo120-profile, fig.width = 6, fig.height=4, fig.cap="The figure shows the Type II error for different validation window sizes and different chosen nominal alpha levels when the size of the optimization window = 120", eval = FALSE}
wo120_profile <- errors2 %>% filter(wo==120, coarse == 0.25) %>% 
  #gather(type, error, actual:beta) %>% 
  gather(type, error, beta) %>% 
  ggplot(aes(x = wv, y = error, colour = factor(alpha)))  + 
  geom_line() + geom_point() + facet_wrap(~type, scales="free") +
    theme_bw()+ 
  theme(legend.position="bottom") + ggtitle("Profile" , subtitle = "Varying validation window sizes, Optimization window = 120, Coarseness = 0.25")

#combined120 <- full_join(errors2 %>% filter(wo==120, coarse == 0.25), wo120 %>% 

wo120_1<- wo120 %>% gather(type, error, beta) %>%  
  ggplot(aes(x = wv, y = error, colour = factor(alpha)))+
  geom_line(linetype=5)+  geom_point(shape= 24) + facet_wrap(~type, scales="free") +
  theme_bw()+ 
  theme(legend.position="bottom") + ggtitle("Signatures" , subtitle = "Varying validation window sizes, Optimization window = 120")

combined120<- 
  ggplot(data = wo120 %>% gather(type, error, beta), aes(x = wv, y = error, colour = factor(alpha)))+
  geom_line(aes(linetype="Signatures"))+  geom_point(shape= 24) + 
  geom_line(data =  errors2 %>% filter(wo==120, coarse == 0.25) %>% 
  gather(type, error, beta), aes(x = wv, y = error, colour = factor(alpha), linetype = "Profiles")) +       geom_point(data =  errors2 %>% filter(wo==120, coarse == 0.25) %>% 
  gather(type, error, beta)) +
  facet_wrap(~type, scales="free") +
  scale_linetype_manual("Lines",values=c("Signatures"=2,"Profiles"=1))+
  guides(fill = guide_legend(keywidth = 1, keyheight = 1),
    linetype=guide_legend(keywidth = 3, keyheight = 1),
    colour=guide_legend(keywidth = 3, keyheight = 1))+
  theme_bw()+ 
  theme(legend.position="right") + ggtitle("Type II error with Validation window" , subtitle = "Profiles at coarseness 0.25") + xlab("Type II error")
    
combined120
#wo120_profile
#wo120_1
#multiplot(wo120_1, wo120_profile1)
```
```{r failtest-coarseness, fig.width = 6, fig.height=4, fig.cap="The figure on the left shows the actual Type I error while the figure on the right shows the Type II error for different validation window sizes and different chosen nominal alpha levels when the size of the optimization window = 120", eval= FALSE}

miss_1
```
```{r failtest-wv, fig.width = 6, fig.height=4, fig.cap="The figure on the left shows the actual Type I error while the figure on the right shows the Type II error for different validation window sizes and different chosen nominal alpha levels when the size of the optimization window = 120", eval= FALSE}

miss_2
```
```{r failtest-wo, fig.width = 6, fig.height=4, fig.cap="The figure on the left shows the actual Type I error while the figure on the right shows the Type II error for different validation window sizes and different chosen nominal alpha levels when the size of the optimization window = 120", eval= FALSE}

miss_2
```

<!-- # Algorithm Modification -->

<!-- The proposed algorithm modification is at the same shift step which comes after the shift distance is identified by finding two windows in the two markings that have the highest correlation. -->

<!-- The modification allows for a "wiggle" room in the second marking of the two sets of markings being compared. This means that each set of same shift windows that are being compared, the second marking will window that is under consideration is allowd to move a little towards the left and a little towards the right. -->

<!-- This gives us a new set of 4 windows, 2 to the left and 2 to the right of the original comparison window. Then the correlations for each one of these 5 windows with the window under consideration of the 1st marking is retrieved (from the validation step correlation matrix) or computed. -->

<!-- Then the window that has the maximum correlation (from the set of 5 windows in the 2nd marking) with the window of the 1st marking is chosen. -->

<!-- This new maximum correlation is then used to compute the U-statistics using the same method as before. -->

<!-- ## Expected advantage of modification -->

<!-- We are expecting that this modification would improve the type I and II errors for the good. Less number of false negatives and false positives. -->
<!-- The reason for this expectation is many a times the bullet markings are not made at the same distances for two or more bullets because of the way it comes out of the barrel. -->

<!-- Therefore rigid same shifts might not necessarily compare the right set of windows. In the same shift step for one set of windows, allowing the window in the 2nd marking to wiggle left and right and finding the best match to the window in the 1st marking, lets us adjust for situations where markings are compressed or elongated. -->

<!-- ## Dependence on the delta of new windows from the original same shift window -->

<!-- From initial tests the amount of movement to get new windows left and right of the original same shift window seems to directly influence the -->
<!-- the pvalue and U statistic that we look at.  -->

<!-- ### other questions -->
<!-- How much movement is reasonable and should be used? Is there a way to understand if the 2nd marking in comparison to the 1st marking is compressed or elongated or neither? Should the amount of movement selected depend on this compression or elongation? or using a static delta movement value justifieable. -->
