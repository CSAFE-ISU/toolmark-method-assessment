---
title: "Adaption of the Chumbley Score to matching of bullet striation marks"
authors:
- affiliation: Department of Statistics, Iowa State University 
  name: Ganesh Krishnan
  thanks: The authors gratefully acknowledge ...
- affiliation: Department of Statistics and CSAFE, Iowa State University 
  name: Heike Hofmann
biblio-style: apsr
output:
  pdf_document:
    citation_package: natbib
    fig_caption: yes
    keep_tex: yes
    latex_engine: pdflatex
    template: template.tex
  html_document: default
blinded: 0
keywords:
- 3 to 6 keywords
- that do not appear in the title
bibliography: bibliography
abstract: null
---


\newcommand{\hh}[1]{{\textcolor{orange}{#1}}}
\newcommand{\gk}[1]{{\textcolor{green}{#1}}}
\newcommand{\cited}[1]{{\textcolor{red}{#1}}}
\setlength\parindent{0pt}

\tableofcontents
\newpage
```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
library(knitr)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  comment = "",
  fig.height = 6,
  fig.width = 6,
  fig.align = "center",
  out.width= '\\textwidth',
  cache = FALSE,
  fig.path='figures/',
  echo=FALSE,
  cache=TRUE
)
options(knitr.table.format = "latex")
library(tidyverse)
library(kableExtra)
library(xtable)
library(gridExtra)
```

```{r functions}
errorrate <- function(data, alpha) {
  summ <- data %>% filter(!is.na(p_value)) %>%
    mutate(signif = p_value < alpha) %>%
    group_by(wv, wo, match, coarse, signif) %>% tally()
  summ$error <- with(summ, match != signif)
  summ$alpha <- alpha
  
  rates <- summ %>% dplyr::group_by(wv, wo, coarse, match, alpha) %>% dplyr::summarize(
    rate = n[error==TRUE]/sum(n)
  )
  totals <- summ %>% dplyr::group_by(wv, wo, coarse, alpha) %>% dplyr::summarize(
    total= sum(n[error==TRUE])/sum(n)
  )
  rates <- rates %>% spread(match,rate) %>% dplyr::rename(
    actual = `FALSE`,
    beta = `TRUE`
  )
  left_join(rates, totals)
}


# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
#  ref: http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
```

```{r data}
if (!file.exists("../data/all-sigs.rds")) {
  files <- dir("../data/signatures", pattern="csv")
  all <- data.frame()
  for (file in files) {
    tmp <- read.csv(file=file.path(path="../data/signatures", file)) 
    if (length(grep("x",names(tmp)) > 0)) tmp <- tmp %>% dplyr::select(-x)
    all <- rbind(all, tmp)
  }
  all <- all %>% filter(land1_id < land2_id)
  saveRDS(all, file="../data/all-sigs.rds")
} else {
  all <- readRDS("../data/all-sigs.rds")
}

all$coarse <- 1
errors <- rbind(errorrate(all, 0.001), 
                errorrate(all, 0.005), 
                errorrate(all, 0.01), 
                errorrate(all, 0.05))
errors$type <- "signatures"

ns <- all %>% group_by(wv, wo, match) %>% summarize(
  n_ss = mean(same_shift_n),
  n_ds = mean(diff_shift_n),
  zero_ss = sum(same_shift_n==0),
  zero_ds = sum(diff_shift_n==0),
  miss = sum(is.na(p_value)),
  missperc = miss/n()*100
)
```

```{r data-profiles}
if (!file.exists("../data/all-profiles.rds")) {
  files <- dir("../data/profiles/", pattern="csv")
  allp <- data.frame()
  for (file in files) {
    tmp <- read.csv(file=file.path(path="../data/profiles", file)) 
    #print(names(tmp))
    if (length(grep("coarse",names(tmp))) == 0)
      tmp$coarse <- gsub(".*coar-(.*).csv", "\\1", file)
    if (length(grep("x",names(tmp)) > 0)) tmp <- tmp %>% dplyr::select(-x)
    allp <- rbind(allp, tmp)
  }
  allp <- allp %>% filter(land1_id < land2_id)
  allp <- allp %>% mutate(coarse = as.numeric(gsub("pt", ".", coarse)))
  saveRDS(allp, file="../data/all-profiles.rds")
} else {
  allp <- readRDS("../data/all-profiles.rds")
}

errors2 <- rbind(errorrate(allp, 0.001), 
                errorrate(allp, 0.005), 
                errorrate(allp, 0.01), 
                errorrate(allp, 0.05))
errors2$type <- "CS1"


if (!file.exists("../data/invprofiles.rds")) {
  files <- dir("../data/inverse-profiles/", pattern="rds")
  allinv <- data.frame()
  for (file in files) {
    tmp <- readRDS(file=file.path(path="../data/inverse-profiles", file)) 
    allinv <- rbind(allinv, tmp)
  }
  allinv <- allinv %>% filter(land1_id < land2_id)
  allinv <- allinv %>% dplyr::select(-coarseness)
  saveRDS(allinv, file="../data/invprofiles.rds")
} else {
  allinv <- readRDS("../data/invprofiles.rds")
}

errors3 <- rbind(errorrate(allinv, 0.001), 
                errorrate(allinv, 0.005), 
                errorrate(allinv, 0.01), 
                errorrate(allinv, 0.05))
errors3$type <- "CS2"


```

```{r data-sig-coars}
if (!file.exists("../data/all-sig-c.rds")) {
  files <- dir("../data/sig_diff_coarseness/", pattern="csv")
  all_sig_c <- data.frame()
  for (file in files) {
    tmp <- read.csv(file=file.path(path="../data/sig_diff_coarseness/", file)) 
    #print(names(tmp))
    if (length(grep("coarse",names(tmp))) == 0)
      tmp$coarse <- gsub(".*coarse-(.*).csv", "\\1", file)
    if (length(grep("x",names(tmp)) > 0)) tmp <- tmp %>% dplyr::select(-x)
    all_sig_c <- rbind(all_sig_c, tmp)
  }
  all_sig_c <- all_sig_c %>% filter(land1_id < land2_id)
  all_sig_c <- all_sig_c %>% mutate(coarse = as.numeric(gsub("pt", ".", coarse)))
  saveRDS(all_sig_c, file="../data/all-sig-c.rds")
} else {
  all_sig_c <- readRDS("../data/all-sig-c.rds")
}

errors_sig_c <- rbind(errorrate(all_sig_c, 0.001), 
                errorrate(all_sig_c, 0.005), 
                errorrate(all_sig_c, 0.01), 
                errorrate(all_sig_c, 0.05))


```


# Introduction and Background

## Motivation
Same source analyses are a major part of an Forensic Toolmark Examiner's job. In current practice examiners  make these comparisons by visual inspection under a comparison microscope and come to one of the following four conclusions: identification, inconclusive, elimination or unsuitable for examination~\citep{afte-toolmarks1998}. These conclusions are made on the basis of "unique surface contours" of the two toolmarks being in "sufficient agreement" \citep{afte-toolmarks1998}. AFTE describes the term "sufficient agreement" as the possibility of another tool producing the markings under comparison, as practically impossible \citep{afte-toolmarks1998}. Potential subject bias in the assessment as well as the lack of specified error rates are the main points of criticisms  first raised by the National Research Council in 2009 \citep{NAS:2009} and later emphasized further by the President's Council of Advisors on Science and Technology \citep{pcast2016}.

Technological advances, such as profilometers and confocal microscopy allow to measure 3D surfaces in a high-resolution digitized form. This technology has become more accessible over the last decade, and has made its way into topological images of ballistics evidence, such as bullet lands and breech faces \citep{DeKinder1, DeKinder2, Bachrach1, vorburger2016}.
Digitized images of 3D surfaces of form the data basis of statistical analysis of toolmarks. A statistical approach based on data  removes both subjectivity from the assessment and allows a quantification of error rates for both false positive and false negative identifications.

\hh{In the next page and a half it is easy to lose the red line. It might help to include a table with an overview.
The table should include the reference to the paper, the data used, the statistical method and the associated error rates.}
Various toolmarks have been studied in the literature:  \citet{manytoolmarks1} and \citet{chumbley} have been analyzing screwdriver marks  digitized using a profilometer;  \citet{manytoolmarks2} have investigated 3D marks from screwdriver, tongue and groove pliers captured using a confocal microscope; \citet{afte-chumbley} have been investigated digitized marks from slip-joint pliers generated by a surface profilometer. 

\hh{We need an additional sentence here to get from the data to the statistical methods ... }
\citet{manytoolmarks2} define a relative distance metric and use it as similarity measure between two toolmarks. \citet{manytoolmarks1} extract many small segments in the markings of two toolmarks and compare similiarity using a maximum pearson correlation coefficient. 
The Chumbley scoring method, first introduced by \citet{chumbley}, uses a similar but more extensive framework based on a Mann-Whitney U test of the resulting correlation coefficients. This approach is non-deterministic, because segments are chosen randomly.  \citep{hadler} make the score deterministic for each pair of toolmarks by choosing segments for comparison systematically. This approach also ensures independence between segements of striae.  In this paper, we are investigating the applicability of the Chumbley scoring method by \citet{hadler} to assess striation marks on bullet lands for same-source identification.

Striation marks on bullets are made by impurities in the barrel. As the bullet travels through the barrel, these imperfections leave "scratches" on the bullet surface. Typically, only striation marks in the land engraved areas (LEAs) are considered \citet{afte-article1992}. Bullet lands are depressed areas between the  grooves made by the rifling action of the barrel. Compared to toolmarks made by screwdrivers striation marks on bullets are typically much smaller, both in length and in width. Bullets also have a curved cross-sectional topography. Figure \ref{fig:rgl} shows us how the signature from a bullet land (bottom) lines up with the image of the land (top) from which it was extracted. We can also see in the figure how the depth and relative position of the striation markings seen in the image are interpreted as the signature.

Bullet matching methods are usually based on these associated signatures. \citet{chu2013} use an automatic method for counting consecutive matching striae (CMS). The authors report an error rate of 52% of the known same source lands comparisons as misidentified (false negative) and zero false positives for known different source lands. \citet{ma2004} and \citet{vorburger2011} discuss CCF (cross-correlation function) and its discriminating power and applicability for same-source analyses of bullets, but do not provide any error rates in their discussion. \citet{aoas} use multiple features like CCF, CMS, D (distance measure) etc in a random forest based method and compare every land against every other land of  digitised versions of Hamby 252 and Hamby 44 \citep{hamby} published on the NIST Ballistics Database \citep{nist}. The authors report an out-of-bag overall error rate of 0.46%, comprised of an error rate of 30.05% of  same-source pairs that were not identified and an error rate of 0.026% of different-source pairs that were incorrectly identified as same-source.

The Chumbley score  provides us with another approach in the same-source assessment of bullet striation marks. \citet{chumbley} compare two toolmarks for same-source. The data for this study was obtained from 50 sequentially manufactured screwdriver tips. \citet{chumbley} report error rates for markings made by the tips at different angles. For markings made at 30 degree the authors  report an average false negative error rate of 0.089  and an average false positive error rate of 0.023. For other angles of 60 and 85 degrees the false negatives error rate is 0.09 while the rate of false positives decreases to 0.01. The paper by \citet{hadler} is based on the same data but the authors focus on  markings made under the same angle. The error rates associated with the deterministic version of the score are  0.06 for false  negatives and a false positive error rate of 0.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/B6-B2-L6-rescaled.png}
```{r, eval=FALSE, echo=FALSE, fig.width=8, fig.height=3, out.width='\\textwidth'}
land <- read.csv("../data/b6b2l6.csv")
land %>% ggplot(aes(x = y, y=resid)) +geom_line() +theme_bw() +
  ylab(expression(paste("Depth (in ",mu,m,")", sep=""))) + 
  xlab(expression(paste("Relative Location (in ",mu,m,")", sep="")))
```

\caption{\label{fig:rgl} Image of a bullet land from a confocal light microscope at 20 fold magnification (top) and a chart of the corresponding signature of the same land (bottom). The dotted lines connect some peaks visible in both visualizations.}

\end{figure}

## Scans for land engraved areas

Comparisons of striae from bullets are usually based on comparisons of striae in land engraved areas, which are extracted in form of cross sections, called *profiles* \citep{aoas,ma2004}. From profiles bullet *signatures* \citep{chu2013,aoas} are extracted as residuals of a loess fit or Gaussian filter. This effectively removes topographic structure from the data in the attempt to increase the signal to noise ratio. The span of the loess fit was found using cross-validation, as described by \citet{aoas}.

\hh{don't split the discussion on the size. between the next paragraph and }
<!--The majority of Bullet profiles and signatures of the Hamby study \citep{hamby} used in this paper are extracted by procedures mentioned by  \citet{aoas}.--> 
There are two sources of scans for sets from the Hamby study available to us: scans of Hamby 44 and Hamby 252 are available from the NIST database \citep{nist}. Hamby 44 has also been made available to us and has been scanned locally for CSAFE at the Roy J.\ Carver High Resolution Microscopy Facility using a Sensofar confocal light microscope. 
Scans in the NIST database are made with a NanoFocus at 20x magnification. The resolutions of the two instruments are different: the NIST scans are taken at a resolution of 1.5625 $\mu m$ per pixel, while the CSAFE scans are available at a resolution of  0.645 $\mu m$ per pixel.
The length of an average bullet land from Hamby (9 mm Ruger P85) is about 2 millimeter, resulting in signatures of about 1200 pixels for NIST scans, and about 3000 pixels for CSAFE scans.

In comparison, scans from the profilometer used by \citet{chumbley, hadler} were taken at a resolution of about 0.73 $\mu m$ per pixel. The screw driver toolmarks are  about 7 mm in length \citep{manytoolmarks1}, for a total of over 9000 pixels for the width of these scans.

This severe limitation in the amount of available data poses the main challenge in adapting the Chumbley score to matching bullet lands, because of the resulting loss in power.



## The Chumbley Score Test


The Chumbley score algorithm takes input in form of two digitized toolmarks. The toolmark is in form of $z(t)$ which is a spatial process for location indexed by $t$. $t$ here denotes equally spaced pixel locations for the striation marks under consideration, $t = 1, ..., T$. Let further $z^s(t)$ denote a vector of markings of length $s$ starting in location  $t$. 

Let $x(t_1)$, $t_1 = 1,2,...T_1$ and $y(t_2)$, $t_2 = 1,2...T_2$ be two digitized toolmarks (where $T_1$ and $T_2$ are not necessarily equal). 
<!--The vectorized processes representing two sets of striaes are therefore shown as $x(t_1)$, $t_1 = 1,2,...T_1$ and $y(t_2)$, $t_2 = 1,2...T_2$. Here $x$ and $y$ denote two instances of the process $z(*)$ which means two striation marks, while the indexing $t$ or pixel range is shown as $t_1$ and $t_2$ for $x$ and $y$ respectively. This representation of $t$ as $t_1$ and $t_2$ lets us identify them as either of same or different lengths.--> 
The toolmarks under consideration are potentially from two different sources or the same source. $T_1$ and $T_2$, as represented above, are the final pixel indexes of each marking and therefore give the respective lengths of the markings. 

In a pre-processing step  the two markings are smoothed using a lowess \citep{lowess} with coarseness parameter $c$. 
 Originally, this smoothing is intended to remove drift and (sub)class characteristics from individual markings, however, in the setting of matching bullet striae, we can also make use of this mechanism to separate bullet curvature in profiles from signatures before matching signatures. 
 
 
After removing sub-class structure, 
the Chumbley scores is calculated in two steps: an optimization step and a validation step.
In the optimization step, the two markings are aligned horizontally such that within a pre-defined window of length $w_o$ the correlation between $x(t_1)$ and $y(t_2)$ is maximized:
\[
\left(t_1^o, t_2^o\right) = \mathop{\arg \max}\limits_{1 \le t_1 \le T_1, 1 \le t_2 \le T_2} \text{cor} \left(x^{w_o} (t_1), y^{w_o}(t_2) \right)
\]
This results in an optimal vertical (in-phase) shift of $t_1^o - t_2^o$ for aligning the two markings. \hh{We will denote the relative optimal locations as $t_1^*$ and $t_2^*$, where $t_i^* = t_i^o/(T_i-w_o)$ for $i=1,2$, such that $t_1^*, t_2^* \in [0,1]$. Once (sub-)class characteristics are removed, the relative optimal locations should be distributed according to a uniform distribution in $[0,1]$. }

In the validation step,  two sets of windows of size $w_v$ are chosen from both markings (see Figure \ref{fig:win-comparison}). In the first set, pairs of windows are extracted from the two markings using the optimal vertical shift as determined in the first step, whereas for the second set the windows are extracted using a different (out-of-phase) shift. 

More precisely, let us define starting points $s_i^{(k)}$ for each signature $k = 1, 2$ as 
\begin{eqnarray}\label{eq.start}
s^{(k)}_i = 
\begin{cases}
t_k^* + i w_v & \text{ for } i < 0 \\
t_k^* + w_ o + i w_v & \text{ for } i \ge 0,
\end{cases}
\end{eqnarray} for integer values of $i$ with $0 <  s^{(k)}_i \le T_k - w_v$.

Same-shift pairs of length $w_v$ are defined in \citet{hadler} as all 
pairs $(s_i^{(1)}, s_i^{(2)})$ for all integer values $i$ for which both $s_i^{(1)}$ and  $s_i^{(2)}$ are defined. Similarly, different-shift pairs are defined as $(s_i^{(1)}, s_{-i-1}^{(2)})$ for all $i$ where both $s_i^{(1)}$ and  $s_{-i-1}^{(2)}$ are defined (see \autoref{sketch-same-diff}).

\begin{figure}[hbtp]
\centering
\includegraphics[width=.7\textwidth]{images/sketch-same.png}

\includegraphics[width=.7\textwidth]{images/sketch-diff.png}
\caption{\label{sketch-same-diff}Sketch of same-shift pairings  (top) and different-shift pairings (bottom). Filled in rectangles show pairings resulting in correlations, unfilled rectangles are segments without a match.}
\end{figure}




```{r win-comparison, warning=FALSE, fig.width=8, fig.height=3.5, fig.cap="Two markings made by the same source. For convenience, the markings are moved into phase on the left and out-of phase on the right. In-phase (left) and out-of-phase (right) samples are shown by the light grey background. The Chumbley-score is based on a Mann-Whitney U test of the correlations derived from these two sets of samples."}
sigs_graphics <- readRDS("../data/sigs_generate_window.rds")
sigs_graphics$sigs_land_id_1_match <- sigs_graphics$sigs_land_id_1_match %>% group_by(land_id) %>% mutate(y = y-min(y, na.rm=T))

sigs_graphics.match<- as.data.frame(sigs_graphics[1])
sigs_graphics.nonmatch<- as.data.frame(sigs_graphics[2])
colnames(sigs_graphics.match)<- gsub("^.*\\.","", colnames(sigs_graphics.match))
colnames(sigs_graphics.nonmatch)<- gsub("^.*\\.","", colnames(sigs_graphics.nonmatch))

# Matching signature
d1<- sigs_graphics.match[which(sigs_graphics.match$land_id==1,arr.ind = TRUE),]
d2<- sigs_graphics.match[which(sigs_graphics.match$land_id==276,arr.ind = TRUE),]
data1<-d1 %>%dplyr::select(l30)
#data1<- data1$y
data2<- d2 %>%dplyr::select(l30)
#data2<- data2$y
window_opt = 120 
window_val = 50
coarse = 1
data1<- matrix(unlist(data1))
data2<- matrix(unlist(data2))

  unity <- function(x) {x / sqrt(sum(x^2))} ## normalize columns of a matrix to make correlation computation faster
  
  ####################################################
  ##Clean the marks and compute the smooth residuals##
  ####################################################
  
  data1 <- matrix(data1[round((0.01*nrow(data1))):round(0.99*nrow(data1)),], ncol = 1)
  data2 <- matrix(data2[round((0.01*nrow(data2))):round(0.99*nrow(data2)),], ncol = 1)
  
  ##Normalize the tool marks
  y1 <- data1 - lowess(y = data1,  x = 1:nrow(data1), f= coarse)$y
  y2 <- data2 - lowess(y = data2,  x = 1:nrow(data2), f= coarse)$y
  
  ############################################
  ##Compute the observed maximum correlation##
  ############################################
  
  #####################
  ##Optimization step##
  #####################
  ##Each column in these matrices corresponds to a window in the respective tool mark
  y1_mat_opt <- matrix(NA, ncol = length(1:(length(y1) - (window_opt - 1))), nrow = window_opt)
  for(l in 1:(length(y1) - (window_opt - 1))){
    y1_mat_opt[,l] <- y1[l:(l+(window_opt - 1))]
  }
  y2_mat_opt <- matrix(NA, ncol = length(1:(length(y2) - (window_opt - 1))), nrow = window_opt)
  for(l in 1:(length(y2) - (window_opt - 1))){
    y2_mat_opt[,l] <- y2[l:(l+(window_opt - 1))]
  }
  
  ##Compute the correlation between all pairs of windows for the two marks
  ##Rows in the following matrix are mark 2, columns are mark 1
  y2_mat_opt <- apply(scale(y2_mat_opt), 2, unity)
  y1_mat_opt <- apply(scale(y1_mat_opt), 2, unity)
  corr_mat_opt <- t(y2_mat_opt) %*% y1_mat_opt ##correlation matrix
  max_corr_opt_loc <- which(corr_mat_opt == max(corr_mat_opt), arr.ind = TRUE) ##pair of windows maximizing the correlation

sigs1.min = sigs_graphics[[1]] %>% filter(land_id==1) %>%
  dplyr::select(y) %>% min()
sigs2.min = sigs_graphics[[1]] %>% filter(land_id==276) %>% 
  dplyr::select(y) %>% min()
window_val <- 3*window_val*0.645

same.shift <- -(365-338)*1/.645 + sigs2.min - sigs1.min 
xs <- seq(100, 1800, length=10)
rects <- data.frame(ymin=-Inf, ymax= Inf, xmin = xs, xmax = xs+window_val)


p1 <- sigs_graphics[[1]] %>% filter(land_id %in% c(1, 276)) %>% 
  ggplot() + 
  geom_segment(aes(x = xmin, y=ymin, xend = xmin, yend=ymax), 
               colour="grey50", linetype=3, data = rects) +
  geom_segment(aes(x = xmax, y=ymin, xend = xmax, yend=ymax), 
               colour="grey50", linetype=3, data = rects) +
  geom_rect(aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), 
            data=rects, fill= "grey90") +
  geom_line(aes(x = y-same.shift*I(land_id==276), 
                y=l30-5*I(land_id==276), group=land_id)) + 
  theme_bw() +
  theme(axis.title.y = element_blank(), 
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  xlab(expression(paste("Relative Location (in ",mu,m,")", sep=""))) + 
  ggtitle("In-phase sample")

diff.shift <- -(365-338)*1/.645 + sigs2.min - sigs1.min + 70 

p2 <- sigs_graphics[[1]] %>% filter(land_id %in% c(1, 276)) %>% 
  ggplot() + 
  geom_segment(aes(x = xmin, y=ymin, xend = xmin, yend=ymax), 
               colour="grey50", linetype=3, data = rects) +
  geom_segment(aes(x = xmax, y=ymin, xend = xmax, yend=ymax), 
               colour="grey50", linetype=3, data = rects) +
  geom_rect(aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), 
            data=rects, fill= "grey90") +
  geom_line(aes(x = y-diff.shift*I(land_id==276), 
                y=l30-5*I(land_id==276), group=land_id)) + 
  theme_bw() +
  theme(axis.title.y = element_blank(), 
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  xlab(expression(paste("Relative Location (in ",mu,m,")", sep=""))) + 
  ggtitle("Out-of-phase sample")

grid.arrange(p1,p2, ncol=2)
```

```{r win-comparison-2, fig.width=8, fig.height=5, out.width='\\textwidth', fig.cap=" The two plots on the left show how the same shift behaves in case of a matching pair and the two plots on the right show how the different shift behaves in case of a matching pair.", eval=FALSE}

asd<- all %>% filter(wo==120, wv==50, match == TRUE, land1_id == 1)

sigs_graphics<- readRDS("../data/sigs_generate_window.rds")

sigs_graphics.match<- as.data.frame(sigs_graphics[1])
sigs_graphics.nonmatch<- as.data.frame(sigs_graphics[2])
colnames(sigs_graphics.match)<- gsub("^.*\\.","", colnames(sigs_graphics.match))
colnames(sigs_graphics.nonmatch)<- gsub("^.*\\.","", colnames(sigs_graphics.nonmatch))

# Matching signature
#gi<-
#sigs_graphics.match<- sigs_graphics.match %>% filter(land_id == c(1,9))  
#sigs_graphics.match<- sigs_graphics.match %>% select(y, l30, land_id)
d1<- sigs_graphics.match[which(sigs_graphics.match$land_id==1,arr.ind = TRUE),]
d2<- sigs_graphics.match[which(sigs_graphics.match$land_id==19,arr.ind = TRUE),]
data1<-d1 %>%dplyr::select(l30)
#data1<- data1$y
data2<- d2 %>%dplyr::select(l30)
#data2<- data2$y
window_opt = 120 
window_val = 50
coarse = 1
data1<- matrix(unlist(data1))
data2<- matrix(unlist(data2))

  unity <- function(x) {x / sqrt(sum(x^2))} ## normalize columns of a matrix to make correlation computation faster
  
  ####################################################
  ##Clean the marks and compute the smooth residuals##
  ####################################################
  
  data1 <- matrix(data1[round((0.01*nrow(data1))):round(0.99*nrow(data1)),], ncol = 1)
  data2 <- matrix(data2[round((0.01*nrow(data2))):round(0.99*nrow(data2)),], ncol = 1)
  
  ##Normalize the tool marks
  y1 <- data1 - lowess(y = data1,  x = 1:nrow(data1), f= coarse)$y
  y2 <- data2 - lowess(y = data2,  x = 1:nrow(data2), f= coarse)$y
  
  
  ############################################
  ##Compute the observed maximum correlation##
  ############################################
  
  #####################
  ##Optimization step##
  #####################
  ##Each column in these matrices corresponds to a window in the respective tool mark
  y1_mat_opt <- matrix(NA, ncol = length(1:(length(y1) - (window_opt - 1))), nrow = window_opt)
  for(l in 1:(length(y1) - (window_opt - 1))){
    y1_mat_opt[,l] <- y1[l:(l+(window_opt - 1))]
  }
  y2_mat_opt <- matrix(NA, ncol = length(1:(length(y2) - (window_opt - 1))), nrow = window_opt)
  for(l in 1:(length(y2) - (window_opt - 1))){
    y2_mat_opt[,l] <- y2[l:(l+(window_opt - 1))]
  }
  
  ##Compute the correlation between all pairs of windows for the two marks
  ##Rows in the following matrix are mark 2, columns are mark 1
  y2_mat_opt <- apply(scale(y2_mat_opt), 2, unity)
  y1_mat_opt <- apply(scale(y1_mat_opt), 2, unity)
  corr_mat_opt <- t(y2_mat_opt) %*% y1_mat_opt ##correlation matrix
  max_corr_opt_loc <- which(corr_mat_opt == max(corr_mat_opt), arr.ind = TRUE) ##pair of windows maximizing the correlation
  

  s1.rect.opt <- data.frame(xmin=max_corr_opt_loc[1,2], xmax= max_corr_opt_loc[1,2]+window_opt, ymin=-Inf, ymax=Inf)
    s1.rect.sameshift1 <- data.frame(xmin=max_corr_opt_loc[1,2]-window_val, xmax= max_corr_opt_loc[1,2]- 2*window_val, ymin=-Inf, ymax=Inf)
       s1.rect.sameshift2 <- data.frame(xmin=max_corr_opt_loc[1,2]+window_opt+ window_val, xmax= max_corr_opt_loc[1,2]+ window_opt+2*window_val, ymin=-Inf, ymax=Inf)
       
  s2.rect.opt <- data.frame(xmin=max_corr_opt_loc[1,1], xmax= max_corr_opt_loc[1,1]-window_opt, ymin=-Inf, ymax=Inf)
    s2.rect.sameshift1 <- data.frame(xmin=max_corr_opt_loc[1,1]-window_opt-window_val, xmax= max_corr_opt_loc[1,1]-window_opt- 2*window_val, ymin=-Inf, ymax=Inf)
       s2.rect.sameshift2 <- data.frame(xmin=max_corr_opt_loc[1,1]+ window_val, xmax= max_corr_opt_loc[1,1]+ 2*window_val, ymin=-Inf, ymax=Inf)
       
p1<-  ggplot(data = d1, aes(x = y, y = l30)) + geom_line() + 
  geom_rect(data=s1.rect.opt, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "black",alpha=0.5, inherit.aes = FALSE)+
    geom_rect(data=s1.rect.sameshift1, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "red",alpha=0.1, inherit.aes = FALSE)+
    geom_rect(data=s1.rect.sameshift2, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "blue",alpha=0.1, inherit.aes = FALSE)+
  theme_bw() + labs(y = "Signature Land Id = 1")
  
 p2<-   ggplot(data = d2, aes(x = y, y = l30)) + geom_line() + 
   geom_rect(data=s2.rect.opt, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "black",alpha=0.5, inherit.aes = FALSE)+
    geom_rect(data=s2.rect.sameshift1, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "red",alpha=0.1, inherit.aes = FALSE)+
    geom_rect(data=s2.rect.sameshift2, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "blue",alpha=0.1, inherit.aes = FALSE)+
  theme_bw() + labs(y = "Signature Land Id = 9")
 ##########
 
 d1.rect.opt <- data.frame(xmin=max_corr_opt_loc[1,2], xmax= max_corr_opt_loc[1,2]+window_opt, ymin=-Inf, ymax=Inf)
    d1.rect.sameshift1 <- data.frame(xmin=max_corr_opt_loc[1,2]-2*window_opt, xmax= max_corr_opt_loc[1,2]-2*window_opt-window_val, ymin=-Inf, ymax=Inf)
       d1.rect.sameshift2 <- data.frame(xmin=max_corr_opt_loc[1,2]+window_opt+ window_val, xmax= max_corr_opt_loc[1,2]+ window_opt+2*window_val, ymin=-Inf, ymax=Inf)
       
  d2.rect.opt <- data.frame(xmin=max_corr_opt_loc[1,1], xmax= max_corr_opt_loc[1,1]-window_opt, ymin=-Inf, ymax=Inf)
    d2.rect.sameshift1 <- data.frame(xmin=max_corr_opt_loc[1,1]-window_opt-window_val, xmax= max_corr_opt_loc[1,1]-window_opt- 2*window_val, ymin=-Inf, ymax=Inf)
       d2.rect.sameshift2 <- data.frame(xmin=max_corr_opt_loc[1,1]+ 4*window_val, xmax= max_corr_opt_loc[1,1]+ 5*window_val, ymin=-Inf, ymax=Inf)
       
p3<-  ggplot(data = d1, aes(x = y, y = l30)) + geom_line() + 
  geom_rect(data=d1.rect.opt, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "black",alpha=0.5, inherit.aes = FALSE)+
    geom_rect(data=d1.rect.sameshift1, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "red",alpha=0.1, inherit.aes = FALSE)+
    geom_rect(data=d1.rect.sameshift2, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "blue",alpha=0.1, inherit.aes = FALSE)+
  theme_bw() + labs(y = "Signature Land Id = 1")
  
 p4<-   ggplot(data = d2, aes(x = y, y = l30)) + geom_line() + 
   geom_rect(data=d2.rect.opt, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "black",alpha=0.5, inherit.aes = FALSE)+
    geom_rect(data=d2.rect.sameshift1, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "red",alpha=0.1, inherit.aes = FALSE)+
    geom_rect(data=d2.rect.sameshift2, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "blue",alpha=0.1, inherit.aes = FALSE)+
  theme_bw() + labs(y = "Signature Land Id = 9")
  

multiplot(p1, p2, p3, p4, cols=2)
```


For both same- and different-shift pairs correlations between the  markings are calculated. 
The intuition here is that for two markings from the same source the correlation for the in-phase sample should be high, while the correlations of the out-of-phase sample  provide a measure for the base-level correlation for non-matching marks of a given length $w_v$. The Chumbley score is then computed as a Mann Whitney U statistic to compare between in-phase sample and out-of-phase sample. 
In the original method proposed in \citet{chumbley} both in-phase and out-of-phase sample are extracted randomly, whereas \citet{hadler} proposed the above specified deterministic rules for both samples to make the resulting score deterministic while simultaneously avoiding overlaps within selected marks to ensure independence.

## A problem with failures and modification

Looking closer at \autoref{eq.start}, we see that by definition, some number of tests will fail to produce a result, either because the number of eligible same-shift pairs is zero, or the number of different-shift opairs is zero. \hh{XXX The problem of failed tests is first mentioned in XXX}

The number of same-shift pairs will be zero, if the optimal locations $t_1^{o}$ and $t_2^{o}$ are so far apart, that no segments of size $w_v$ are left on the same sides of the optimal locations, i.e. $t_1^{o} < w_v$ and $t_2^{o} > T_2-w_o-w_v$ or  $t_1^{o} < T_1- w_o - w_v$ and $t_2^{o} < w_v$, i.e. we have a failure rate of 
\[
P\left( t_1^{o} < w_v \ \cap \ t_2^{o} > T_2-w_o-w_v\right) + P\left( t_1^{o} < T_1- w_o - w_v \ \cap \ t_2^{o} < w_v\right).
\]
While we can assume that once (sub-)class characteristics are removed, optimal locations $t_i^{o}$ are uniformly distributed across the length of the profile, we cannot assume that $t_1^o$ and $t_2^o$ are independent of each other. In particular, for same-source profiles, we would expect a strong dependency between these locations, in which case a large difference between locations is unlikely. However, for different source matches, we can assume that  locations are independent.
In that case, we expect a test to fail with a probability of $\frac{2 w_v^2}{(T_1-w_o)(T_2-w_o)}$. For an average length of $T_i$ of 1200 pixels, $w_o = 120$ pixels and $w_v = 30$ pixels this probability is about 0.0015.

The number of possible different-shift pairs also depends on the location of  the  optimal locations $t_1^o$ and $t_2^o$. Whenever the optimal locations are close to the boundaries, the number of possible pairings decreases and reaches zero, if $t_i^{(o)} < w_v$ or  $t_i^{(o)} > T_i-w_o- w_v$. Assuming a correlation between optimal locations $t_1^o$ and $t_2^o$ of close to one for same-source profiles, this results in an expected rate of failure of $2 w_v / (T_i-w_o)$, or about 5.6\% for an average length of $T_i$ of 1200 pixels, $w_o = 120$ pixels and $w_v = 30$ pixels. Assuming independence in the optimal locations for different source profiles the expected probability for a failed test is, again, $\frac{2 w_v^2}{(T_1-w_o)(T_2-w_o)}$.

While failures due to missing correlations from same-shift pairs are unavoidable by definition of the Chumbley score, failures due to missing correlations from different-shift pairs can be prevented by using a different strategy in assigning pairs.

Using the same notation as in \autoref{eq.start}, we define same-shift pairs identical to \citet{hadler} as pairs $(s_i^{(1)}, s_i^{(2)})$ for all $i$ where the boundary conditions of both sequences are met simultaneously. Let us assume that this results in $I$ pairs. Define $s_{(j)}^{(k)}$ to be the $j$th starting location in sequence $k = 1, 2$, i.e.\ $s_{(1)}^{(k)} < s_{(2)}^{(k)} < ... < s_{(I)}^{(k)}$.  

We then define the pairs for different-shifts as
\begin{equation}\label{eq.diff2}
\left(s_{(j)}^{(1)}, s_{(I-j+1)}^{(1)} \right) \text{ for } j = 
\begin{cases}
1, ..., I & \text{ for even } I \\
1, ..., (I-1)/2, (I-1)/2 + 2, ..., I & \text{ for odd } I
\end{cases},
\end{equation}
i.e.\ for an odd number of same-shift correlations, we skip the middle pair for the different-shift correlations (see \autoref{sketch-diff-2}).
This pairing ensures that the number of different-shift pairings is the same or at most one less than the number of same-shift pairings in all tests.
\hh{In the remainder of the paper, we will refer to the algorithm defined by \citet{hadler} as {\bf (CS1)} and the suggested modified algorithm as {\bf (CS2)} and compare their performance on the available scans of the Hamby study.}
\begin{figure}[hbtp]
\centering
\includegraphics[width=.7\textwidth]{images/sketch-diff-2.png}
\caption{\label{sketch-diff-2}Sketch of adjusted different-shift pairings. At most one of the same-shift pairings can not be matched with a different-shift pair. }
\end{figure}



# Testing setup

## The Data

```{r sigs-profiles, fig.width = 8, fig.height = 2.5, fig.cap="Bullet land profile (left) and the corresponding signature (right) for one of the lands of Hamby-44."}
p1 <- sigs_graphics[[1]] %>% filter(land_id==19) %>%
  ggplot(aes(x = y, y = value)) + geom_line() +
  theme_bw() +
  ylab(expression(paste("Depth (in ",mu,m,")", sep=""))) + 
  xlab(expression(paste("Relative Location (in ",mu,m,")", sep=""))) + 
  ggtitle("Profile")

p2 <- sigs_graphics[[1]] %>% filter(land_id==19) %>%
  ggplot(aes(x = y, y = l30)) + 
  geom_line(aes(y=resid), size=.25) +
  geom_line(aes(y=l30)) +
  theme_bw() +
  ylab(expression(paste("Depth (in ",mu,m,")", sep=""))) + 
  xlab(expression(paste("Relative Location (in ",mu,m,")", sep=""))) + 
  ggtitle("Signature")

grid.arrange(p1,p2, ncol=2)
```

Lands for all Hamby-44 and Hamby-252 scans are made available through the NIST ballistics database \citep{nist} and are considered, here. Both of these sets of scans are part of the larger Hamby study \citep{hamby}. Each set consists of twenty known bullets (two each from ten consecutively rifled Ruger P85 barrels) and fifteen questioned bullets (each matching one of the ten barrels). Ground truth for both of these Hamby sets is known and was used to assess correctness of the tests results. 

Profiles for each bullet land were extracted from scans close to the heel of the bullet while avoiding break-off as described in \citet{aoas}.


## Setup
Both algorithms (CS1) and (CS2) are implemented in the R package `toolmaRk` \citep{toolmark}. We applied both methods to all pairwise land-to-land comparisons of the Hamby scans provided by NIST for a total of 85,491 land-to-land comparisons. 

# Results
## Failed Tests

Initially, the default settings suggested in \citet{hadler} were used: $w_o = 120$ pixels or about 190 $\mu m$ (ten percent of the average length of profiles) and coarseness $c = 0.25$. 
\autoref{fig:fails} shows the percentage of failed tests among the 85,491 land-to-land comparisons of the NIST data for different values of the validation window size $w_v$. For same-source lands up to 12.5 percent of the tests fail using CS1. The highest percentage of failed tests under CS2 is 1.3% for different source tests using a validation window size $w_v$ of 60 pixels.  \hh{Rates of expected failures are based on simulation runs using covariances between locations of same-source profiles of 0.854, and 0.120 for locations from different-source profiles, matching observed covariances for the Hamby scans. }\hh{ Observed failure rates are higher than expected. This might be due to remaining sub-class structure at a coarseness of 0.3 resulting in a distribution of optimal locations different from the assumed uniform. }

```{r simulation, echo=FALSE}
library(MASS) 
Sigsame <- matrix(c(1, 0.85, 0.85, 1), nrow = 2) 
Sigdiff <- matrix(c(1, 0.12, 0.12, 1), nrow = 2) 
set.seed(20140501)
simu <- 1:10 %>% purrr::map_df(.f = function(i) {
  X1 <- mvrnorm(10000, mu = c(0,0), Sigma = Sigsame) 
  X2 <- mvrnorm(10000, mu = c(0,0), Sigma = Sigdiff) 
  Y1 <- pnorm(X1)  # Probability Integral Transform
  Y2 <- pnorm(X2) 
  wv <- 10*1:6
  
  ps <- wv %>% purrr::map_df(.f = function(x) {
    p <- x/(1200-120)
    y1 <- Y1[,1]
    y2 <- Y1[,2]
    difffail1 <- sum((y1 < p & y2 < p) | (y1 > 1-p & y2 > 1-p))/length(y1)
    samefail1 <- sum((y1 < p & y2 >1- p) | (y1 > 1-p & y2 < p))/length(y1)
    y1 <- Y2[,1]
    y2 <- Y2[,2]
    difffail2 <- sum((y1 < p & y2 < p) | (y1 > 1-p & y2 > 1-p))/length(y1)
    samefail2 <- sum((y1 < p & y2 > 1-p) | (y1 > 1-p & y2 < p))/length(y1)
    data.frame(wv=x, samefail1=samefail1, samefail2=samefail2,
               difffail1=difffail1, difffail2=difffail2)
  })
  ps$i <- i
  ps
})
simu$`TRUE` <- simu$samefail1+simu$difffail1
simu$`FALSE` <- simu$samefail2+simu$difffail2
simulong <- simu %>% dplyr::select(wv, i, `TRUE`, `FALSE`) %>% 
  gather(match, failed, 3:4)
simusumm <- simulong %>% group_by(wv, match) %>%
  summarize(
    sdfailed = sd(failed),
    failed = mean(failed)
  )
simusumm$method = "CS1"
```


```{r fails, echo=FALSE, fig.height = 3, fig.width = 7, out.width='0.7\\textwidth', fig.cap="Percent of failed land-to-land comparisons using an optimization window $w_o = 120$ and a coarseness of $c = 0.25$. With an increase in the size of the validation window  a higher percentage of tests fails under both methods (CS1) and (CS2), but the percentage of failed tests is much smaller under (CS2). Observed failure rates are higher than expected rates."}
allinv$method <- "CS2"
allp$method <- "CS1"


allinv <-  allinv %>% mutate(
  loc1 = chumbley %>% purrr::map_int(.f = function(x) x$locations[1]),
  loc2 = chumbley %>% purrr::map_int(.f = function(x) x$locations[2])
)


prof_fails <- rbind(
  allinv %>% dplyr::select(wo, wv, method, p_value, match),
  allp %>% filter(coarse == 0.25) %>% 
    dplyr::select(wo, wv, method, p_value, match)
)
fails <- prof_fails %>%
  group_by(wo, wv, method, match) %>% summarize(
  failed = sum(is.na(p_value)),
  n=n()
)

fails %>% 
  ggplot(aes(x = wv, y = failed/n*100, colour=factor(match), shape=factor(match))) +
  geom_point(aes(size="Observed")) +
  xlab("Size of validation window in pixels") +
  ylab("Percent of failed tests") +
  theme_bw() +
  scale_colour_brewer("Same-source", palette="Set1") +
  scale_shape_discrete("Same-source") +
  ylim(c(0,13)) +
  facet_wrap(~method, labeller="label_both") +
  geom_point(aes(y = failed*100, size = "Expected"), data=simusumm) +
  scale_size_manual("Failures", values=c(1.5,3))
```

## Coarseness
The purpose of the coarseness parameter is to remove (sub-)class characteristics from profiles before comparisons for matching.
\citet{hadler} suggest a coarseness parameter of 0.25 in the setting of toolmark comparisons. For bullet lands, coarseness might need to be adjusted because of the strong effect  bullet curvature has on profiles.

\hh{\autoref{fig:profile-sketch} gives an overview of the effect of different coarseness parameters: from left to right, coarseness levels $c$ are varied in steps of 0.05 from 0.1 to 0.3. The top row shows  resulting signature after smoothing the profile shown in Figure~\ref{fig:sigs-profiles} with the coarseness specified. 
The histograms in the bottom row show the relative optimal location $t^*$. Optimal locations are distributed uniformly once (sub-)class characteristics are removed. However, for coarseness values of $c > 0.20$ we see quite distinct boundary effects: optimal locations $t^*$ are found at the very extreme ends of a profile more often than one would expect based on a uniform distribution.}

```{r profile-sketch, fig.height = 4, fig.width='\\textwidth', fig.cap="Overview of the effect of different coarseness parameters $c$ on the profile shown in Figure \\ref{fig:sigs-profiles} (top). The bottom row shows histograms of  the (relative) optimal locations $t^o$ identified in the optimization step for different values of the coarseness parameter $c$. "}
profile <- sigs_graphics[[1]] %>% filter(land_id==19)
profile$lw10 <- lowess(x = profile$y, y = profile$value, f=.10)$y
profile$lw15 <- lowess(x = profile$y, y = profile$value, f=.15)$y
profile$lw20 <- lowess(x = profile$y, y = profile$value, f=.20)$y
profile$lw25 <- lowess(x = profile$y, y = profile$value, f=.25)$y
profile$lw30 <- lowess(x = profile$y, y = profile$value, f=.30)$y
#profile$lw50 <- lowess(x = profile$y, y = profile$value, f=.5)$y
#profile$lw75 <- lowess(x = profile$y, y = profile$value, f=.75)$y
#profile$lw100 <- lowess(x = profile$y, y = profile$value, f=1)$y

profiles <- tidyr::gather(profile, coarseness, values, lw10:lw30)
profiles <- profiles %>% mutate(
  c = readr::parse_number(coarseness)/100
)
p1 <- profiles %>% 
  ggplot(aes(x = y, y = value-values)) + geom_line() +
  theme_bw() +
  ylab(expression(paste("Depth (in ",mu,m,")", sep=""))) + 
  xlab(expression(paste("Relative Location (in ",mu,m,")", sep=""))) +
  facet_grid(~c, labeller="label_both") 

profc10 <- readRDS("../data/profiles-coarseness/chumbley-csafe-profiles-inv-wo_160-wv_30-c_10.rds")
profc15 <- readRDS("../data/profiles-coarseness/chumbley-csafe-profiles-inv-wo_160-wv_30-c_15.rds")
profc20 <- readRDS("../data/profiles-coarseness/chumbley-csafe-profiles-inv-wo_160-wv_30-c_20.rds")
profc20$coarseness <- 0.2
profc25 <- readRDS("../data/profiles-coarseness/chumbley-csafe-profiles-inv-wo_160-wv_30-c_25.rds")
profc30 <- readRDS("../data/profiles-coarseness/chumbley-csafe-profiles-inv-wo_160-wv_30.rds")
profc30$coarseness <- .30
profc <- bind_rows(profc10, profc15, profc20, profc25, profc30)
profc <- profc %>% mutate(
  locations = chumbley %>% purrr::map_int(.f = function(x) {
    res <- NULL
    try(res <- x$locations[1], silent = TRUE)
    res
  })
)
profsumm <- read.csv("../data/profiles-summary.csv")
profsumm <- profsumm %>% dplyr::select(-run_id)
profc <- profc %>% left_join(profsumm, by=c("land2_id"="land_id", "profile2_id"="profile_id"))


p2 <- profc %>% 
  ggplot(aes(x = locations/(length-wo))) +
  geom_histogram(binwidth = 0.025) +
  facet_grid(.~coarseness, labeller="label_both") + 
  theme_bw() +
  xlab(expression("Relative optimal location "~t^"*")) +
  ylab("Number of profiles") +
  scale_x_continuous(breaks=seq(0,1, by=0.25), labels=c("0", "0.25", "0.50", "0.75", "1"))
 
grid.arrange(p1, p2, ncol=1)
```

## Type II error rates

\autoref{fig:type2} gives an overview of the type 2 error of methods CS1 and CS2 across a range of different optimization windows $w_o$. 

```{r type2, fig.height = 3, fig.width = 4.5, out.width='0.6\\textwidth', fig.cap="Type II error rates observed across a range of window sizes for optimization $w_o$. For a window size of $w_o = 130$ we see a minimum in type II error rate across all type I rates considered. Smaller validation sizes $w_v$ are typically associated with a smaller type II error."}
if (!file.exists("../data/cvcs2.RDS")) {
files <- dir("../data/crossvalidate-cs2/")
cvcs2 <- data.frame()
for (file in files) {
  tmp <- readRDS(file.path("../data/crossvalidate-cs2/",file))
  cvcs2 <- rbind(cvcs2, tmp)
}
cvcs2$coarse <- cvcs2$coarseness
cvcs2$method <- "CS2"
saveRDS(cvcs2, file = "../data/cvcs2.RDS")
} else cvcs2 <- readRDS("../data/cvcs2.RDS")

if (!file.exists("../data/cvcs1.RDS")) {
files <- dir("../data/crossvalidate-cs1/")
cvcs1 <- data.frame()
for (file in files) {
  tmp <- readRDS(file.path("../data/crossvalidate-cs1/",file))
  cvcs1 <- rbind(cvcs1, tmp)
}
cvcs1$coarse <- cvcs1$coarseness
cvcs1$method <- "CS1"
saveRDS(cvcs1, file = "../data/cvcs1.RDS")
} else cvcs1 <- readRDS("../data/cvcs1.RDS")

cvcs <- rbind(cvcs1, cvcs2)

library(plotROC)
errorsCV2 <- rbind(errorrate(cvcs2, 0.001), 
                errorrate(cvcs2, 0.005), 
                errorrate(cvcs2, 0.01), 
                errorrate(cvcs2, 0.05))
errorsCV2$method <- "CS2"

errorsCV1 <- rbind(errorrate(cvcs1, 0.001), 
                errorrate(cvcs1, 0.005), 
                errorrate(cvcs1, 0.01), 
                errorrate(cvcs1, 0.05))
errorsCV1$method <- "CS1"

errorsCV <- rbind(errorsCV1, errorsCV2)

errorsCV %>% 
  ggplot(aes(x = wo, y = beta, colour=factor(alpha))) + 
  geom_point(aes(shape=factor(wv)), size=2.5) +
  geom_smooth(aes(group=alpha), se=FALSE, method="loess", span=1,  size=.7) +
  theme_bw() +
  scale_colour_brewer(expression("Nominal\ntype I error"~alpha),
                      palette="Set2") +
  scale_shape_discrete(expression("Size of\nvalidation window "~w[v])) +
  xlab(expression("Window size for optimization "~w[o])) +
  ylab("Type II error rate") +
  facet_wrap(~method, labeller="label_both")

```

\autoref{fig:roc} gives an overview of ROC curves for methods CS1 and CS2 over a range of different optimization window sizes $w_o$. 

```{r roc, fig.height = 3.25, fig.width = 4.75, out.width='0.6\\textwidth', fig.cap="ROC curves of methods CS1 and CS2 for different sizes of optimization window $w_o$. Optimal ROC curves are reached at optimization windows of size 150 and higher. Points of equal error rates (EERs) can be found at the intersection of the dotted line and the ROC curves."}

cvcs %>% 
  ggplot(aes(d = as.numeric(!match), m = p_value, colour=factor(wo), 
             shape=factor(wv), linetype=method)) + 
  geom_roc(labels=FALSE, n.cuts=100, pointsize=0, size=0.6) +
  geom_abline(
    linetype=3, slope=-1, intercept = 1, colour="grey30", size=.3) +
  theme_bw() + 
  scale_colour_brewer(
    expression("Optimization\nwindow size"~w[o]), palette="Set2") +
  facet_grid(.~wv, labeller="label_both") +
  xlab("False Positives Rate (1 - Specificity)") +
  ylab("True Positives Rate (Sensitivity)") 

```



<!--
## Type II error rate and coarseness
\hh{Figure \ref{fig:coarse} shows the type II error rates for profiles using an optimization window $w_o= 120$ and a validation window $w_v = 30$ for varying level of coarseness. The type II error for all nominal levels of  $\alpha$ are the lowest for a coarseness range of 0.20 to 0.30, with a minimum in type II reached at a coarseness $c = 0.25$. The histograms suggest to use a value of coarseness below 0.2.}
\hh{However, for an optimal value of coarseness other aspects have to be condsidered.}
```{r coarse, fig.cap="Type II error with respect to coarseness parameter over profiles, $w_o = 120$, $w_v = 30$. Optimal values for coarseness are around $c \\approx 0.30$", fig.width=5, fig.height = 3, out.width='.6\\textwidth'}
e1<-errors2 %>% filter(wv==30)%>% ggplot(aes(x = coarse, y=beta, colour=factor(alpha))) +
  geom_point() + geom_line() +
  theme_bw() +
  scale_colour_brewer(expression("Nominal\ntype I error"~alpha), palette="Set2") + xlab("Profile Coarseness")+
  ylab("Type II error") 
e1
```

-->

## Observed versus Nominal Type I error rates

\autoref{fig:type1} shows observed type I error rates across a range of optimization windows $w_o$. Generally, observed type I errors are higher than expected, but with an increase in the size of the optimization window observed type I error rates decrease. 

```{r type1, fig.width=8, fig.height=5, out.width='\\textwidth', fig.cap="Comparison of observed and nominal type I error rates  across a range of window sizes for optimization $wo$. The horizontal line in each facet indicates the nominal type I error rate."}
errorsCV %>% filter(wv %in% c(30,50)) %>%
  ggplot(aes(x = wo, y = actual, colour=factor(alpha))) + 
  geom_hline(aes(yintercept=alpha), colour="grey30", size=.5) +
  geom_point(aes(shape=factor(wv)), size=2.5) +
  #  facet_grid(.~wo, labeller="label_both") + 
  geom_smooth(aes(group=alpha), size=.8, se=FALSE, method="lm", alpha=.9) +
  theme_bw() +
#  scale_y_log10(breaks=c(0.001,.005, 0.01, 0.05)) +
  scale_shape_discrete(expression("Size of validation window "~w[v])) +
  ylab("Observed type I error rate") +
  xlab(expression("Window size for optimization, "~w[o])) +
  scale_colour_brewer(expression("Nominal type I error "~alpha), palette="Set2") +
  facet_wrap(~alpha, labeller="label_both", scales="free") +
  theme(legend.position="bottom")
  
```

<!--
### Results for Signatures

\hh{For signatures from NIST scans we see three problems: 
\begin{enumerate}
\item type-2 error rate is at best 30\% for a type-1 error rate of 5\%, which is well above the error rates we see for tool marks from screw drivers, see figure \ref{fig:type2};
\item the observed type-1 error, which generally close to the nominal type-1 error rate, depends on the size of the optimization window: as the window size increases, the observed type-1 error decreases, see figure \ref{fig:type1};
\item the Chumbley-score fails to provide a result for up to 3\% of the cases. The number of failed tests increases linearly in the size used for the window in the optimization step. The rate of failed tests is considerably higher when the two lands are from same source than when the lands are from different sources, see figure \ref{fig:missings}. 
\end{enumerate}
}


Figure \ref{fig:missings} gives an overview of the number of failed tests, i.e. tests in which a particular parameter setting did not return a valid result. This happens e.g.\ when the shift to align two markings is so large, that the remaining overlap is too small to accommodate  windows for validation. The problem is therefore exacerbated by a larger validation window. Figure \ref{fig:missings}(left) also shows that the number of failed tests is approximately linear in the size of the optimization window.
Tests also fail at a higher rate than expected when the markings are from the same source (right). This difference is the least pronounced around an optimized window size $w_o$ of around 120. However, even in this scenario, the number of failed tests for markings from the same source is about twice as high as expected given the number of same source and different source pairings in the data set.
-->


<!--
Following on similar lines to the setup of toolmarks, the first step here is to first identify what difference does different window sizes of optimization and the validation step have, when adapting the toolmark method to bullets.

The marking made on bullets are smaller than toolmarks and is also less wider. The idea is to find out possible areas of error while adapting the score based method proposed for toolmarks, using cross-validation setup to identify appropriate parameter settings for (a) signatures and (b) profiles directly

### Signatures
-->

<!--
```{r type2b, fig.width=8, fig.height=5, out.width='.8\\textwidth', fig.cap="Type II error rates observed across a range of window sizes for optimization $w_o$. For a window size of $w_o = 120$ we see a drop in type II error rate across all type I rates considered. Smaller validation sizes $w_v$ are typically associated with a smaller type II error."}
errors %>% filter(wv %in% c(30,50)) %>%
  ggplot(aes(x = wo, y = beta, colour=factor(alpha))) + 
  geom_point(aes(shape=factor(wv)), size=3.5) +
#  facet_grid(.~wo, labeller="label_both") + 
  geom_smooth(aes(group=alpha), se=FALSE, method="loess", size=.7) +
  theme_bw() +
  scale_colour_brewer(expression("Nominal\ntype I error"~alpha), palette="Set2") +
  scale_shape_discrete(expression("Size of\nvalidation window "~w[v])) +
  xlab(expression("Window size for optimization "~w[o])) +
  ylab("Type II error rate")


labels = expression(P[M1](tilde(z)>0),P[M0](tilde(z)>0))
```
-->



<!--
```{r missings, out.width='\\textwidth', fig.width=8, fig.height=4, fig.cap="The number of failed tests increases with an increase in the size of the optimization window (left). Unfortunately there is also a dependency between failed tests and ground truth. The plot on the right shows the ratio of the number of land pairs from same sources and different sources for failed tests. For small optimization windows and large windows the number of failed tests for same-source land-to-land comparisons is increasing. Even in the minimum, same-source land-to-land comparisons fail at twice the rate that they are expected to based on the ratio of the number of known matches and known non-matches (horizontal line).",fig.align = "center"}
#ns1<- ns[which(ns$wv == c(30,50), arr.ind = T),]

fails <- all %>% group_by(wv, wo) %>% summarize(
  miss = sum(is.na(p_value)),
  missperc = miss/n()*100
)

failsbymatch <- all %>% group_by(wv, wo, match) %>% summarize(
  fails = sum(is.na(p_value))
) %>% mutate(
    ratio = fails[match==TRUE]/fails[match==FALSE],
    match = c("Different Source", "Same Source")[as.numeric(as.logical(match)) + 1]
    ) 

missings <- fails %>% filter(wv %in% c(30, 50)) %>%
  ggplot(aes(x = wo, y = missperc, 
             colour = factor(wv),
             shape=factor(wv)), size=.3) + 
  geom_smooth(method="lm", size=.5, se=FALSE) +
  geom_point(size=2.5) + 
  theme_bw() +
  ylab("Percent of failed tests.") +
  xlab(expression("Size of optimization window "~w[o])) +
  scale_colour_brewer(expression("Size of validation window "~w[v]), palette="Set1") +
  scale_shape(expression("Size of validation window "~w[v])) +
    theme(legend.position="bottom")


missingsbymatch <- failsbymatch %>% 
  filter(wv %in% c(30, 50),
         match=="Same Source") %>%
  ggplot(aes(x = wo, y = ratio, 
             colour = factor(wv),
             shape=factor(wv)), size=.3) + 
  geom_hline(yintercept = 43960/2948225, size=0.5, colour="grey30") +
  geom_smooth(method="loess", span=0.8, size=.5, se=FALSE) +
  geom_point(size=2.5) + 
  theme_bw() +
  ylab("Ratio of same/different source\namong failed tests") +
  xlab(expression("Size of optimization window "~w[o])) +
  scale_colour_brewer(expression("Size of validation window "~w[v]), palette="Set1") +
  scale_shape(expression("Size of validation window "~w[v])) +
    theme(legend.position="bottom") +
  scale_y_continuous(breaks=c(0.015, 0.03, 0.06, 0.09))



gridExtra::grid.arrange(missings, missingsbymatch, ncol=2)


```
-->

<!--
Signatures of lands for all Hamby-44 and Hamby-252 scans made available through the NIST ballistics database \citep{nist} were considered. Both of these sets of scans are part of the larger Hamby study \citep{hamby} and each consist of twenty known bullets (two each from ten consecutively rifled Ruger P85 barrels) and fifteen questioned bullets (each matching one of the ten barrels). Ground truth for both of these Hamby sets is known and was used to assess correctness of the tests results. 

Bullet signatures being compared at this time are therefore from the Hamby 44 and Hamby 252 data. The database setup and pre-processing system used for choosing the Bullet signatures are as described by \citet{aoas}. In order to choose the bullet signatures we first filter out Land_id for Profiles from the Hamby 44 and Hamby 252 data and remove all NA values. Then run_id = 3 is chosen as the signatures generated from this run_id give the closest match. Different run_id's have some different settings for generating the signatures.(The level of smoothing does not seem to be one of them)

The bullet signatures when generated by this process already includes a loess smoothing. Therefore, the coarseness factor is set to 1 while running the chumbley non random algorithm for comparing different optimization windows.The algorithm generates the same_shift, different_shift, U-Stat and P_value parameters which are then used to calculate the errors associated with different sets of window sizes.

### Profiles

The profiles are cross-sectional values of the the bullet striation mark which are chosen at an optimum height (x as used by \citet{aoas}). This x or height is not a randomly chosen level. The rationale behind the choice has been explained by \citet{aoas}. A region is first chosen where the cross-correlation seems to change very less and in this region an optimum height is chosen. The profiles generally resemble a curve which is more or less similar to a quadratic curve (a quadratic fit to the raw data values of the profile is not an exact fit but it does show a similar trend). Profiles are the set of raw values representing the striation marks, and signatures are generated from these by removal of the inherent curvature and applying some smoothing (the signatures generated by \citet{aoas} use a loess function for smoothing). 

Similar to signatures the run_id = 3 was used when applying the chumbley algorithm using the database setup given by \citet{aoas} of Hamby-44 and Hamby-252 datasets, on the profiles. The run_id not only defines the level of smoothing but also signifies the chosen height at which the profiles were selected initially. Another important aspect is the range of horizontal values (which is referred to as the y values in \citet{aoas}) in the signatures. These have already been pre-processed in the database to not include any grooves.

Therefore for the sake of comparison the run_id = 3 is still chosen so as to ensure that the horizontal values remain the same as that of the signatures. This also gives us profiles with the grooves removed.

The idea therefore is to first use these raw values of the profile directly in the chumbley algorithm, and see how the algorithm performs for different coarseness values (smoothing parameter as referred in the function lowess used in the chumbley algorithm).
-->





```{r, include =F}
wo120_sig <- errors %>% filter(wo==120, wv %in% c(30, 50))
wo120_pr <- errors2 %>% filter(wo==120, wv %in% c(30, 50), coarse == .25)

dt <- rbind(wo120_sig, wo120_pr) %>% 
  ungroup() %>% 
  mutate(beta = round(100*beta, 1)) %>%
  dplyr::select(wv, type, alpha, beta) %>% 
  spread(alpha, beta)
print(xtable(dt), include.rownames=FALSE)
```

\begin{table}[ht]
\caption{\label{tab:type2} Type II error rates for profiles and signatures of bullet lands. For profiles, a coarseness value of $c = 0.25$ is used to remove bullet curvature.}
\centering
\begin{tabular}{rlrrrr}
  \hline
validation && \multicolumn{4}{l}{Nominal type I error rate $\alpha$}\\  
window $w_v$ & source & 0.001 & 0.005 & 0.01 & 0.05 \\ 
  \hline
 30 & profiles & 54.40 & 43.80 & 39.70 & 30.00 \\ 
   30 & signatures & 55.00 & 45.40 & 41.40 & 31.10 \\ \hline
   50 & profiles & 58.50 & 44.40 & 40.70 & 28.70 \\ 
   50 & signatures & 62.60 & 49.60 & 44.20 & 30.50 \\ 
   \hline
\end{tabular}
\end{table}


# Conclusions

\hh{
We started out trying to assess the suitability of the Chumbley Score's suitability for matching striae on bullet lands. Besides using recommended defaults, we also proposed ways to optimize parameters: based on the assumption that once sub-class characteristics are removed, optimal locations are distributed uniformly across the profile, we found  for bullets the smaller coarseness value of $c = 0.15$ to be suitable. 
Optimized window sizes were found based on cross-validation to minimize type 2 error rates. 
Method CS1 proposed by \citet{hadler} has a minimal type 2 error rate of XXX for an optimized window size of YYY - which is considerably higher than the error rates achieved on matching toolmarks. 
Unfortunately, method CS1 also has a high rate of failed tests -- situations, in which the algorithm does not provide a result, due to the way different-shift pairs are constructed. 
XXX
The algorithm CS2 introduced here is constructed in a way that reduces the number of failures by a magnitude of XXX. While reducing the failure rate, the algorithm also shows an increase in the power of the test. Type II error rates reach a minimum of XXX for an optimized window size of YYY. While significantly reduced over CS1, CS2 still has type 2 error rates on bullet lands that are  higher than the error rates achieved on the -much larger- toolmarks.
However, bullets usually have multiple lands - in the case of Ruger P85s as used in the Hamby study, there are six lands for each bullet. We might be able to get more power out of the test by adapting CS2 to deal work in a bullet-to-bullet comparison.
}

\hh{
Fix in CS2 will also improve power for matching toolmarks. 
}

<!-- The results suggest that the Nominal type I error $\alpha$ value shows dependence on the size of the window of optimization. For a given window of optimization the actual Type I error is comparable to the nominal level for only a select few validation window sizes and for comparable validation window sizes of 30 and 50 as done here, the actual type I error does not seem to vary as much as it varies with the optimization window sizes .
A Test Fail, i.e. tests in which a particular parameter setting did not return a valid result, happens, when the shift to align two signatures is so large, that the remaining overlap is too small to accommodate  windows for validation, depends on whether known-match or known non-matches has predictive value, with test results from different sources having a much higher chance to fail. On conducting an analysis of all known bullet lands using the adjusted chumbley algorithm, Type II error was identified to be least bad for window of validation 30 and window of optimization 120. In case of unsmoothed raw marks (profiles), Type II error increases with the amount of smoothing and least for LOWESS smoothing coarseness value about 0.25 or 0.3. In an effort to identify the level of adaptiveness of the algorithm, comparisons were made between signatures and profiles. Their comparison with respect to validation window size for a fixed optimization window size suggested that, profiles have a total error (i.e all incorrect classification of known-matches and known non-matches) greater than or equal to the total error of signatures for all sizes of validation window. Profiles also fail more number of times than signatures in a test fail (for different coarseness keeping windows fixed and also for different validation windows keeping coarseness fixed) which lets us conclude that the behaviour of the algorithm for the profiles instead of pre-processed signatures is not better. Finally it should be noted that the current version of the adjusted chumbley algorithm seems to fall short when compared to other machine-learning based methods \citet{aoas}, and some level of modification to the deterministic algorithm needs to be identified and tested that would reduce the number of incorrect classifications.

```{r wo120-profile, fig.width = 6, fig.height=4, fig.cap="The figure shows the Type II error for different validation window sizes and different chosen nominal alpha levels when the size of the optimization window = 120", eval = FALSE}
wo120_profile <- errors2 %>% filter(wo==120, coarse == 0.25) %>% 
  #gather(type, error, actual:beta) %>% 
  gather(type, error, beta) %>% 
  ggplot(aes(x = wv, y = error, colour = factor(alpha)))  + 
  geom_line() + geom_point() + facet_wrap(~type, scales="free") +
    theme_bw()+ 
  theme(legend.position="bottom") + ggtitle("Profile" , subtitle = "Varying validation window sizes, Optimization window = 120, Coarseness = 0.25")

#combined120 <- full_join(errors2 %>% filter(wo==120, coarse == 0.25), wo120 %>% 

wo120_1<- wo120_profile %>% gather(type, error, beta) %>%  
  ggplot(aes(x = wv, y = error, colour = factor(alpha)))+
  geom_line(linetype=5)+  geom_point(shape= 24) + facet_wrap(~type, scales="free") +
  theme_bw()+ 
  theme(legend.position="bottom") + ggtitle("Signatures" , subtitle = "Varying validation window sizes, Optimization window = 120")

combined120<- 
  ggplot(data = wo120_profile %>% gather(type, error, beta), aes(x = wv, y = error, colour = factor(alpha)))+
  geom_line(aes(linetype="Signatures"))+  geom_point(shape= 24) + 
  geom_line(data =  errors2 %>% filter(wo==120, coarse == 0.25) %>% 
  gather(type, error, beta), aes(x = wv, y = error, colour = factor(alpha), linetype = "Profiles")) +       geom_point(data =  errors2 %>% filter(wo==120, coarse == 0.25) %>% 
  gather(type, error, beta)) +
  facet_wrap(~type, scales="free") +
  scale_linetype_manual("Lines",values=c("Signatures"=2,"Profiles"=1))+
  guides(fill = guide_legend(keywidth = 1, keyheight = 1),
    linetype=guide_legend(keywidth = 3, keyheight = 1),
    colour=guide_legend(keywidth = 3, keyheight = 1))+
  theme_bw()+ 
  theme(legend.position="right") + ggtitle("Type II error with Validation window" , subtitle = "Profiles at coarseness 0.25") + xlab("Type II error")
    
combined120
#wo120_profile
#wo120_1
#multiplot(wo120_1, wo120_profile1)
```
```{r failtest-coarseness, fig.width = 6, fig.height=4, fig.cap="The figure on the left shows the actual Type I error while the figure on the right shows the Type II error for different validation window sizes and different chosen nominal alpha levels when the size of the optimization window = 120", eval= FALSE}

miss_1
```
```{r failtest-wv, fig.width = 6, fig.height=4, fig.cap="The figure on the left shows the actual Type I error while the figure on the right shows the Type II error for different validation window sizes and different chosen nominal alpha levels when the size of the optimization window = 120", eval= FALSE}

miss_2
```
```{r failtest-wo, fig.width = 6, fig.height=4, fig.cap="The figure on the left shows the actual Type I error while the figure on the right shows the Type II error for different validation window sizes and different chosen nominal alpha levels when the size of the optimization window = 120", eval= FALSE}

miss_2
```
-->

<!-- # Algorithm Modification -->

<!-- The proposed algorithm modification is at the same shift step which comes after the shift distance is identified by finding two windows in the two markings that have the highest correlation. -->

<!-- The modification allows for a "wiggle" room in the second marking of the two sets of markings being compared. This means that each set of same shift windows that are being compared, the second marking will window that is under consideration is allowd to move a little towards the left and a little towards the right. -->

<!-- This gives us a new set of 4 windows, 2 to the left and 2 to the right of the original comparison window. Then the correlations for each one of these 5 windows with the window under consideration of the 1st marking is retrieved (from the validation step correlation matrix) or computed. -->

<!-- Then the window that has the maximum correlation (from the set of 5 windows in the 2nd marking) with the window of the 1st marking is chosen. -->

<!-- This new maximum correlation is then used to compute the U-statistics using the same method as before. -->

<!-- ## Expected advantage of modification -->

<!-- We are expecting that this modification would improve the type I and II errors for the good. Less number of false negatives and false positives. -->
<!-- The reason for this expectation is many a times the bullet markings are not made at the same distances for two or more bullets because of the way it comes out of the barrel. -->

<!-- Therefore rigid same shifts might not necessarily compare the right set of windows. In the same shift step for one set of windows, allowing the window in the 2nd marking to wiggle left and right and finding the best match to the window in the 1st marking, lets us adjust for situations where markings are compressed or elongated. -->

<!-- ## Dependence on the delta of new windows from the original same shift window -->

<!-- From initial tests the amount of movement to get new windows left and right of the original same shift window seems to directly influence the -->
<!-- the pvalue and U statistic that we look at.  -->

<!-- ### other questions -->
<!-- How much movement is reasonable and should be used? Is there a way to understand if the 2nd marking in comparison to the 1st marking is compressed or elongated or neither? Should the amount of movement selected depend on this compression or elongation? or using a static delta movement value justifieable. -->
<!--
# Appendix
\begin{appendix}

On the other hand Figure \ref{fig:prof_missings} (b) shows if the coarseness level set in the chumbley agorithm has any effect on the signatures, which are pre-processed and already smoothed to a certain extent. From Figure \ref{fig:prof_missings} (b) we can notice that for different nominal $\alpha$ levels, the type II error fluctuates slightly but does not change much, thereby helping us conclude that the coarseness levels set in the LOWESS smoothing in the chumbley alggorithm does effect the type II error much for signatures.

\subsection{Comparison of profiles and signatures}

Another reason for failed tests can be incorrect identification of maximum correlation windows in the optimization step as seen in figure \ref{fig:prof_missings}(d) because of the level of smoothing, as too much smoothing would subdue intricate features that might otherwise help in the correlation calculations and correct identification of maximum correlation windows irrespective of the size. This would again cause a simiar effect as explained for figure \ref{fig:missings} with validation windows, irrespective of size, during the shifts end up at the ends of the markings resulting in an invalid calculation and failed comparison attempt. 

In figure \ref{fig:prof_missings}(d) and (f), we compare profiles and signatures on the basis of number of failed tests. The profiles chosen for figure \ref{fig:prof_missings}(f) have a constant coarseness of 0.25 and window of optimization as 120. The signatures in this case are not smoothed using the chumbley algorithm step of LOWESS smoothing. Instead signatures are used as calculated by \citet{aoas}. The smoothing in these signatures were determined and fixed on the basis of their performance in the random forest based algorithm proposed by \citet{aoas}. The comparison of profiles and signatures with variation of validation window size therefore is made on even footing. The trends are similar to figure \ref{fig:missings} in the sense that for known non-matches the number of failed tests are more for both signatures and profiles and increasing linearly with the validation window size. The problem is however, worse for profiles which has higher number of failed tests than signatures for all validation windows.

The total error for different validation window sizes for signatures and profiles can be seen in figure \ref{fig:prof_missings} (e).The optimization window size is 120 and profiles are calculated at a default 0.25 coarseness level while signatures as before are not smoothed again in the modified chumbley algorithm. We can see that the total error is always higher for profiles as compared to signatures for all sizes of validation window. 

```{r, fig.cap="Type II errors for different levels of coarseness."}
e1<-errors2 %>% filter(wv==30)%>% ggplot(aes(x = coarse, y=beta, colour=factor(alpha))) +
  geom_point() + geom_line() +
  theme_bw() +
  scale_colour_brewer(expression("Nominal\ntype I error"~alpha), palette="Set2") + xlab("Profile Coarseness")+
  ylab("Type II error") + ggtitle(label = "(a)")

e2<- errors_sig_c %>% filter(wv==30)%>% ggplot(aes(x = coarse, y=beta, colour=factor(alpha))) +
  geom_point() + geom_line() +
  theme_bw() +
  scale_colour_brewer(expression("Nominal\ntype I error"~alpha), palette="Set2")+ xlab("Signature Coarseness") + ylab("Type II error") + ggtitle(label = "(b)")

#multiplot(e1,e2, cols = 1)

```



```{r, eval=FALSE}
#errors %>% ggplot(aes(x = wv, y=total, colour=factor(alpha), shape="signatures")) + geom_point() + facet_grid(~wo) +geom_point(data=errors2, aes(shape="profiles"))

total1<- errors_sig_c %>% ggplot(aes(x = coarse, y=total, colour=factor(alpha), shape="signatures")) + geom_point() + geom_line()  + geom_point(data= errors2 %>% filter(wv==30), aes(shape="profiles")) + geom_line(data=errors2 %>% filter(wv==30), aes(shape="profiles")) + theme_bw() +
  scale_colour_brewer(expression("Nominal\ntype I error"~alpha), palette="Set2") + xlab("Coarseness") +ylab("Total error") + ggtitle(label = "(c)")

total2<- wo120_profile %>% filter(wv>10) %>% ggplot(aes(x = wv, y=total, colour=factor(alpha), shape="signatures")) + geom_point() + geom_line() +geom_point(data=errors2 %>% filter(wv>10) %>% filter(coarse==0.25), aes(shape="profiles")) + geom_line(data=errors2 %>% filter(wv>10)%>% filter(coarse==0.25), aes(shape="profiles")) + theme_bw() +
  scale_colour_brewer(expression("Nominal\ntype I error"~alpha), palette="Set2") + ylab("Total error") + ggtitle(label = "(e)")


```

```{r prof_missings, fig.width=10, fig.height=10, fig.cap="Row 3:  Total error and Number of failed tests by the window validation size, wv, and ground truth, Row 2: Total error and Number of failed tests with Coarseness for both profiles and signatures, Row 1: Type II error for different coarseness levels as used in the modified chumbley algorithm for profiles and signatures"}


#ns1<- ns[which(ns$wv == c(30,50), arr.ind = T),]
miss_1<- nsp %>% filter(wv == 30) %>%
#ns %>% filter(wv == c(30, 50)) %>%
  ggplot(aes(x = coarse, y = missperc, shape= "profile", colour = factor(match))) + 
  #geom_smooth(method="auto", size=.5, se=FALSE) +
  geom_point(size = 2) + 
  theme_bw() +
  ylab("Percent of failed tests.") +
  xlab("Coarseness") +
  scale_colour_brewer("Match", palette="Set1") + geom_point(data=ns_sig_c  %>% filter(wv %in% c(30,50)) , aes(shape="signature"), size = 2) + ggtitle(label = "(d)")
# + scale_colour_brewer("Match", palette="Set2") #+geom_smooth(method="auto", size=.5, se=FALSE) 
  #scale_shape_discrete("Size of \nvalidation\nwindow, #wv")

miss_2<- nsp %>% filter(coarse == 0.25) %>%
#ns %>% filter(wv == c(30, 50)) %>%
  ggplot(aes(x = wv, y = missperc, shape= "profile", colour = factor(match))) + 
  #geom_smooth(method="auto", size=.5, se=FALSE) +
  geom_point(size = 2) + 
  theme_bw() +
  ylab("Percent of failed tests.") +
  xlab("wv") +
  scale_colour_brewer("Match", palette="Set1") + geom_point(data=ns %>% filter(wo == 120)  %>% filter(coarse == 1) , aes(shape="signature"), size = 2) + ggtitle(label = "(f)")#+ scale_colour_brewer("Match", palette="Set2")

#multiplot(plotlist = list(total1, total2, miss_1, miss_2, e1, e2), cols = 2)#, matrix(c(1,2,3,3), nrow=2, byrow=TRUE))
multiplot(plotlist = list(e1, total1, total2,e2, miss_1, miss_2), cols = 2)
```
\end{appendix} 
-->