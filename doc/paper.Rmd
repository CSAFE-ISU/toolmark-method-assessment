---
title: "Adaption of the Chumbley Score to matching of bullet striation marks"
authors:
- affiliation: Department of YYY, University of XXX
  name: Ganesh Krishnan
  thanks: The authors gratefully acknowledge ...
- affiliation: Department of ZZZ, University of WWW
  name: Heike Hofmann
biblio-style: apsr
output:
  pdf_document:
    citation_package: natbib
    fig_caption: yes
    keep_tex: yes
    latex_engine: pdflatex
    template: template.tex
  html_document: default
blinded: 0
keywords:
- 3 to 6 keywords
- that do not appear in the title
bibliography: bibliography
abstract: null
---

\newcommand{\hh}[1]{{\textcolor{orange}{#1}}}
\newcommand{\cited}[1]{{\textcolor{red}{#1}}}


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
library(knitr)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  comment = "",
  fig.height = 6,
  fig.width = 6,
  fig.align = "center",
  out.width= '\\textwidth',
  cache = FALSE,
  fig.path='figures/',
  echo=FALSE
)
options(knitr.table.format = "latex")
library(tidyverse)
```

```{r functions}
errorrate <- function(data, alpha) {
  summ <- data %>% filter(!is.na(p_value)) %>%
    mutate(signif = p_value < alpha) %>%
    group_by(wv, wo, match, signif) %>% tally()
  summ$error <- with(summ, match != signif)
  summ$alpha <- alpha
  
  rates <- summ %>% group_by(wv, wo, match, alpha) %>% summarize(
    rate = n[error==TRUE]/sum(n)
  )
  rates %>% spread(match,rate) %>% rename(
    actual = `FALSE`,
    beta = `TRUE`
  )
}

# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
  #  ref: http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
```

```{r data}
if (!file.exists("../all-sigs.rds")) {
  files <- dir("../data/signatures", pattern="csv")
  all <- data.frame()
  for (file in files) {
    tmp <- read.csv(file=file.path(path="../data/signatures", file)) 
    if (length(grep("x",names(tmp)) > 0)) tmp <- tmp %>% select(-x)
    all <- rbind(all, tmp)
  }
  all <- all %>% filter(land1_id < land2_id)
  saveRDS(all, file="../data/all-sigs.rds")
} else {
  all <- readRDS("../all-sigs.rds")
}

errors <- rbind(errorrate(all, 0.001), 
                errorrate(all, 0.005), 
                errorrate(all, 0.01), 
                errorrate(all, 0.05))
```

# Introduction

## Problem statement: same source matching based on striations

\hh{'considered to be unique' by AFTE - that is right, but we should not use this argument, and I don't think we have to. It's better to focus on the problem at hand: striation marks are used to determine same source for bullets. XXX we could include some figures here XXX Current standards ask for a visual inspection of the marks under a comparison microscope by a firearms examiner. One of the problems raised by the PCAST report in 2009 is the subjectivity involved in this comparison. There are different approaches used to achieve more objectivity and assess error rates. XXX here we need some examples of approaches XXX
For toolmarks, the Chumbley score has been suggested to determine same-source. XXX summary of results from original paper and the adjustment XXX
In this paper we are interested in how this method has to be adjusted to best be applied to the smaller marks on bullets. }

Striation marks created on Bullets by the Barrel of a firearm are considered to be unique to the barrel of the firearm. This was first stated by the AFTE Criteria fo Identification Committee in 1992 (AFTE 1992 article) \citet{afte-article1992}. The maximum number of consecutively matching striae (CMS) as first seen by Biasotti 1959 \citet{biasotti} and later mentioned in \citet{chu2013} are the striated markings that line up exactly without any dissimilarity. This definition involves counting a single peak as a striae. A modified meaning of the terminology as explained by (Hare, Hofmann and Carriquiry) includes both peaks and valleys as means of identifying striae. Such identification methods more often than not include subject bia and are error prone and in case of CMS as seen in the work of \citet{miller} the number of CMS may turn to out to be high even when the bullet is not fires by the same firearm.

Thus, an important aspect of same source matching in firearms is identification of bullet microtopographies that are unique to be chosen for a match, Bullet profiles and signatures prove to be such features. The choice of a bullet signature to uniquely define a bullet depends on finding first, a stable region on a Bullet land which minimal noise and many pronounced striation marks. \citet{aoas}

The method employed by \citet{aoas} uses the cross correlation factor as a means to identify the stable region. The markings or striae in this region are supposed to have a high CCF with each other. Therefore a Bullet profile is chosen from this region by taking the cross section at a given height.
A loess fit on the bullet profile produces  residuay which are termed as signatures and can be used as a unique identifier that can be used while matching two bullets to a firearm.

## Existing solution for toolmarks: Chumbley Score

Compairing pairs of toolmarks with the intention of matching it to a tool has been studied in the past, and \citet{chumbley} have described in their paper an algorithm and analytic method that compares two toolmarks and comes to the conclusion if they are from the same tool. The motivation for this lies in the idea of developing analytical methods for which error rates can be found. This moves reduces subject bias and empirical based studies to designate two toolmarks as matches or non-matches, although \citet{chumbley} use this kind of a setup to validate their proposed analytic and quantitative algorithm.

The data that is used by \citet{chumbley} is generated by a surface profileometer that gives the height in terms of distance along a linear trace. This is taken perpendicular to the striations present in the toolmark. Two such trace are then compared using the algorithm. 

Th algoritm works in two phases, namely, an optimization step and a validation step, at the end of which a Mann Whitney U statistic is calculated. \citet{hadler} in their paper proposed an improvement to this algorithm by trying to remove mutual dependence of parameters (due to serial correlation in surface depth values of a toolmark and because of a random sampling sub step in the validation phase which makes a group of pixels to be chosen more than once and hence introduces lack of independence) in certain steps, especially because the Mann-Whitney U statistic that is later calculated in the algorithm and used as a measure to differentiate between matches and non-matches works under the assumption of independence of parameters.

Since we are only interested in a non-parameteric U statistic, Hadler et al. proposed a normalization procedure in the Validation step that goes to some extent to address the issue of mutual dependence. Also the same shift and different shift substep was modified to use a deterministic rule for sampling sam-shhift and dofferent shift-samples as opposed to the originally proposed random samples.

## Chumbley Algorithm and Chumbley score

###Optimization step
The idea behind this step is to first identify the area of best agreement in the two toolmark data.

Comparison window size is predefined by the user, which in case of screwdriver toolmarks was chosen by Chumbley et al and Hadler et al as around 10 percent of the length of the toolmarks, which was around 500.
This window is henceforth referred to as Window of optimization.

A maximum correlation statistic is used to identify the region of best agreement, with the maximum usually seen being near 1 for both cases of matches ( which is intuitively what we expect) and non matches (which is not what we intuitively expect)

First, there are a very large number of correlations calculated, for eg if the window of optimization was defined as 200 and toolmark pixel length is around 1000 then we have 1200-(200-1) = 1001. Can be thought of ordered arrangements without replacement where n = 1001 and r = 1.

This is the number of windows we have for one toolmark and each window is compared with all windows of the second toolmark (the number of windows is again similar), and the window with the maximum correlation is identified to be the region where the toolmarks are in maximum agreement with each other


###Aside (Bullets):
Window of optimization will be shorter as the signatures are smaller. Idea is to keep the number of windows of optimization sufficiently large. This means shorter Trace segments (partition of signature or toolmark with length = size of window of optimization) and lets us compare smaller segments of one signature to another.

This introduces a problem as if we go too small in the window of optimization the unique feature of the trace segments are lost and seem similar, and too large sizes vastly reduces the weightage of small features that would otherwise uniquely classify a signature and hence identify the region of agreement.

This means it has a direct influence on whether we are
Falsely rejecting the null when it is true (Type I) or Falsely accepting the null when it is false (Type II)

Hence identification of the optimum size of window of optimization is very important.


###Validation Step

####Same-shift: 
In this step a series of windows are chosen at random (originally by chumbley et al) and deterministically (by hadler et al), but at a common distance (rigid-shift) from the window identified as the region of best agreement in the vaildation step.
The correlation of these windows would be as intuitively assumed i.e. lower than the maximum correlation window (Optimization step), but the significance is that these same shift windows will still have large enough correlation values for two toolmarks or signatures that are in reality a match.

This also validates that if in the optimization step the maximum correlation window chosen (with correlation value near 1) was by accident (like in case of signatures that in reality are a non-match), all these same- shift correlations (A fixed number of these trace segments are identified) would not be anywhere large enough for all same-shift windows.


####Different Shift:
Primary reason for this substep is to give perspective to the correlation values of the same shift window correlation values.

This time there are no rigid-shifts but different shifts (distance from Window of Opt with max correlation) chosen randomly by \citet{chumbley} and deterministically by Hadler et al. such that there is an equal possibilty of comparing a trace segment from one signature or toolmark to any one in the second signature or toolamrk.

Neither of above sets of correlation are allowed to include the maximum correlation window as identified earlier.

Therefore the assumption is that if two toolmarks or signatures match each other the same-shift correlations would be larger than the different-shift windows,
and if they are not a match the correlations in the two sets will be very similar.

###U Statistic:

This is computed from the joint rank of all correlations of both the same and different shift samples. As given by \citet{hadler}

Null Hypothesis: If the toolmarks were not match i.e not made by the same tool.

Let ns and nd be the number of same shift and different shift windows
$N = n_{s} + n_{d}$

The mann whitney U statistic is given by
$U =\sum^{ns}_{i=1}R_{s}\left( i\right)$

with the standardized version which includes provision for rank ties

$\overline{U}= \dfrac{U-M}{\sqrt{V}}$

where prior to normalization the U-statistic has the mean as

$M = n_{s}\left(\dfrac{N+1}{2}\right)$

and variance 

$V = \dfrac{n_{s}n_{d}}{N\left(N-1\right)}\left[\Sigma^{n_{s}}R_{s}\left(i\right)^{2}+\Sigma^{n_{d}}R_{d}\left(j\right)^{2}\right] -\dfrac{n_{s} n_{d}\left(N+1\right)^{2}}{4\left(N-1\right)}$
 


## Potential limitations in the application to striation marks on bullet lands: smaller in width and curved - need to adjust parameter settings; something similar is done in \citet{afte-chumbley} for toolmark comparisons of slip-joint pliers.

Bullets are much smaller in length, width, are not flat and curved in the cross-sectional topography as opposed to tools like screw driver tips which produced longer and pronounced markings. This means the makings made by barrels in comparison to toolmarks may have a problem in distinctiveness. The majority of Bullet profiles and signatures extracted by procedures mentioned by Hare, Hofmann and Carriquiry \citet{aoas} are almost 1/4 th the size of toolmarks as used by \citet{chumbley} or even smaller. Striations on Bullets are made on their curved surfaces, whereas the algorithm developed by \citet{chumbley} and \citet{hadler} has only been tested for flatter and wider surfaces which have negligible curvature. Therefore, using methods proposed for toolmarks may need adaptation in order to give tangible results for bullets. Moreoverr, in order to to get flat bullet signatures and remove the curvatures some kind of smoothing needs to be applied as a pre-step which needs further investigation as to whether the level of smoothing does effect the working of the algorithm on Bullets.

## Using cross-validation setup to identify appropriate parameter settings for (a) signatures and (b) profiles directly

Following on similar lines to the setup of toolamrks, the first step here is to first identify what difference does different window sizes of optimization and the validation step have, when adapting the toolmark method to bullets.

The marking made on bullets are smaller than toolmarks and is also less wider. The idea is to find out possible areas of error while adapting the score based method proposed for toolmarks.

Bullet signatures being compared at this time are from the Hamby 44 and Hamby 252 data, which have the best set of known-matched and known non-matches.

Bullet signatures are chosen by
1. Filtering out Land_id for Profiles from the Hamby 44 and Hamby 252 data and removing 
all NA values
2. run_id = 3 is chosen, Seems like the signatures generated from this run_id give the closest
match. Different run_id's have some different settings for generating the signatures
(The level of smoothing does not seem to be one of them)

The bullet signatures when generated already included the LOWESS smoothing.
Therefore, the coarseness factor is set to 1 while running the chumbley_non_random()
which generates the same_shift, different_shift, U-Stat and P_value parameters.
 

```{r win-comparison, fig.width=8, fig.height=5, out.width='\\textwidth', fig.cap=" The two plots on the left show how the same shift behaves in case of a matching pair and the two plots on the right show how the different shift behaves in case of a matching pair"}
a<- matrix(NA, ncol = 3, nrow = 2)
toolmark.falsepositive<- data.frame(a)
colnames(toolmark.falsepositive)<- c("Classification", "Match", "Non-Match")
toolmark.falsepositive$Classification<- c("Match", "Non-Match")
toolmark.falsepositive$Match<- c(47,0)
toolmark.falsepositive$`Non-Match`<- c(3,50)
toolmark.falsepositive

asd<- all %>% filter(wo==120, wv==50, match == TRUE, land1_id == 1)

sigs_graphics<- readRDS("../data/sigs_generate_window.rds")

sigs_graphics.match<- as.data.frame(sigs_graphics[1])
sigs_graphics.nonmatch<- as.data.frame(sigs_graphics[2])
colnames(sigs_graphics.match)<- gsub("^.*\\.","", colnames(sigs_graphics.match))
colnames(sigs_graphics.nonmatch)<- gsub("^.*\\.","", colnames(sigs_graphics.nonmatch))

# Matching signature
#gi<-
#sigs_graphics.match<- sigs_graphics.match %>% filter(land_id == c(1,9))  
#sigs_graphics.match<- sigs_graphics.match %>% select(y, l30, land_id)
d1<- sigs_graphics.match[which(sigs_graphics.match$land_id==1,arr.ind = TRUE),]
d2<- sigs_graphics.match[which(sigs_graphics.match$land_id==19,arr.ind = TRUE),]
data1<-d1 %>%select(l30)
#data1<- data1$y
data2<- d2 %>%select(l30)
#data2<- data2$y
window_opt = 120 
window_val = 50
coarse = 1
data1<- matrix(unlist(data1))
data2<- matrix(unlist(data2))

  unity <- function(x) {x / sqrt(sum(x^2))} ## normalize columns of a matrix to make correlation computation faster
  
  ####################################################
  ##Clean the marks and compute the smooth residuals##
  ####################################################
  
  data1 <- matrix(data1[round((0.01*nrow(data1))):round(0.99*nrow(data1)),], ncol = 1)
  data2 <- matrix(data2[round((0.01*nrow(data2))):round(0.99*nrow(data2)),], ncol = 1)
  
  ##Normalize the tool marks
  y1 <- data1 - lowess(y = data1,  x = 1:nrow(data1), f= coarse)$y
  y2 <- data2 - lowess(y = data2,  x = 1:nrow(data2), f= coarse)$y
  
  
  ############################################
  ##Compute the observed maximum correlation##
  ############################################
  
  #####################
  ##Optimization step##
  #####################
  ##Each column in these matrices corresponds to a window in the respective tool mark
  y1_mat_opt <- matrix(NA, ncol = length(1:(length(y1) - (window_opt - 1))), nrow = window_opt)
  for(l in 1:(length(y1) - (window_opt - 1))){
    y1_mat_opt[,l] <- y1[l:(l+(window_opt - 1))]
  }
  y2_mat_opt <- matrix(NA, ncol = length(1:(length(y2) - (window_opt - 1))), nrow = window_opt)
  for(l in 1:(length(y2) - (window_opt - 1))){
    y2_mat_opt[,l] <- y2[l:(l+(window_opt - 1))]
  }
  
  ##Compute the correlation between all pairs of windows for the two marks
  ##Rows in the following matrix are mark 2, columns are mark 1
  y2_mat_opt <- apply(scale(y2_mat_opt), 2, unity)
  y1_mat_opt <- apply(scale(y1_mat_opt), 2, unity)
  corr_mat_opt <- t(y2_mat_opt) %*% y1_mat_opt ##correlation matrix
  max_corr_opt_loc <- which(corr_mat_opt == max(corr_mat_opt), arr.ind = TRUE) ##pair of windows maximizing the correlation
  

  s1.rect.opt <- data.frame(xmin=max_corr_opt_loc[1,2], xmax= max_corr_opt_loc[1,2]+window_opt, ymin=-Inf, ymax=Inf)
    s1.rect.sameshift1 <- data.frame(xmin=max_corr_opt_loc[1,2]-window_val, xmax= max_corr_opt_loc[1,2]- 2*window_val, ymin=-Inf, ymax=Inf)
       s1.rect.sameshift2 <- data.frame(xmin=max_corr_opt_loc[1,2]+window_opt+ window_val, xmax= max_corr_opt_loc[1,2]+ window_opt+2*window_val, ymin=-Inf, ymax=Inf)
       
  s2.rect.opt <- data.frame(xmin=max_corr_opt_loc[1,1], xmax= max_corr_opt_loc[1,1]-window_opt, ymin=-Inf, ymax=Inf)
    s2.rect.sameshift1 <- data.frame(xmin=max_corr_opt_loc[1,1]-window_opt-window_val, xmax= max_corr_opt_loc[1,1]-window_opt- 2*window_val, ymin=-Inf, ymax=Inf)
       s2.rect.sameshift2 <- data.frame(xmin=max_corr_opt_loc[1,1]+ window_val, xmax= max_corr_opt_loc[1,1]+ 2*window_val, ymin=-Inf, ymax=Inf)
       
p1<-  ggplot(data = d1, aes(x = y, y = l30)) + geom_line() + 
  geom_rect(data=s1.rect.opt, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "black",alpha=0.5, inherit.aes = FALSE)+
    geom_rect(data=s1.rect.sameshift1, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "red",alpha=0.1, inherit.aes = FALSE)+
    geom_rect(data=s1.rect.sameshift2, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "blue",alpha=0.1, inherit.aes = FALSE)+
  theme_bw() + labs(y = "Signature Land Id = 1")
  
 p2<-   ggplot(data = d2, aes(x = y, y = l30)) + geom_line() + 
   geom_rect(data=s2.rect.opt, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "black",alpha=0.5, inherit.aes = FALSE)+
    geom_rect(data=s2.rect.sameshift1, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "red",alpha=0.1, inherit.aes = FALSE)+
    geom_rect(data=s2.rect.sameshift2, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "blue",alpha=0.1, inherit.aes = FALSE)+
  theme_bw() + labs(y = "Signature Land Id = 9")
 ##########
 
 d1.rect.opt <- data.frame(xmin=max_corr_opt_loc[1,2], xmax= max_corr_opt_loc[1,2]+window_opt, ymin=-Inf, ymax=Inf)
    d1.rect.sameshift1 <- data.frame(xmin=max_corr_opt_loc[1,2]-2*window_opt, xmax= max_corr_opt_loc[1,2]-2*window_opt-window_val, ymin=-Inf, ymax=Inf)
       d1.rect.sameshift2 <- data.frame(xmin=max_corr_opt_loc[1,2]+window_opt+ window_val, xmax= max_corr_opt_loc[1,2]+ window_opt+2*window_val, ymin=-Inf, ymax=Inf)
       
  d2.rect.opt <- data.frame(xmin=max_corr_opt_loc[1,1], xmax= max_corr_opt_loc[1,1]-window_opt, ymin=-Inf, ymax=Inf)
    d2.rect.sameshift1 <- data.frame(xmin=max_corr_opt_loc[1,1]-window_opt-window_val, xmax= max_corr_opt_loc[1,1]-window_opt- 2*window_val, ymin=-Inf, ymax=Inf)
       d2.rect.sameshift2 <- data.frame(xmin=max_corr_opt_loc[1,1]+ 4*window_val, xmax= max_corr_opt_loc[1,1]+ 5*window_val, ymin=-Inf, ymax=Inf)
       
p3<-  ggplot(data = d1, aes(x = y, y = l30)) + geom_line() + 
  geom_rect(data=d1.rect.opt, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "black",alpha=0.5, inherit.aes = FALSE)+
    geom_rect(data=d1.rect.sameshift1, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "red",alpha=0.1, inherit.aes = FALSE)+
    geom_rect(data=d1.rect.sameshift2, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "blue",alpha=0.1, inherit.aes = FALSE)+
  theme_bw() + labs(y = "Signature Land Id = 1")
  
 p4<-   ggplot(data = d2, aes(x = y, y = l30)) + geom_line() + 
   geom_rect(data=d2.rect.opt, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "black",alpha=0.5, inherit.aes = FALSE)+
    geom_rect(data=d2.rect.sameshift1, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "red",alpha=0.1, inherit.aes = FALSE)+
    geom_rect(data=d2.rect.sameshift2, aes(xmin=xmin, xmax=xmax , ymin=ymin, ymax=ymax),color = "blue",alpha=0.1, inherit.aes = FALSE)+
  theme_bw() + labs(y = "Signature Land Id = 9")
  

multiplot(p1, p2, p3, p4, cols=2)
```


window Optimization 320 Window Validation 50
```{r echo=FALSE}
all %>% filter(wo==320, wv==50) %>% xtabs(data=., ~signif+match)
```
window Optimization 120 Window Validation 50
```{r echo=FALSE}
all %>% filter(wo==120, wv==50) %>% xtabs(data=., ~signif+match)
```
window Optimization 80 Window Validation 50
```{r echo=FALSE}
all %>% filter(wo==80, wv==50) %>% xtabs(data=., ~signif+match)

```

# Results

## Signatures

Signatures of lands for all Hamby-44 and Hamby-252 scans made available through the NIST ballistics database \citep{nist} were considered. Both of these sets of scans are part of the larger Hamby study \citep{hamby} and each consist of twenty known bullets (two each from ten consecutively rifled Ruger P85 barrels) and fifteen questioned bullets (each matching one of the ten barrels). Ground truth for both of these Hamby sets is known and was used to assess correctness of the tests results. 

We used the adjusted Chumbley method as proposed in \citet{hadler} and implemented in the R package `toolmaRk` \citep{toolmark} on all pairwise land-to-land comparisons of the Hamby scans (a total of 85,491 comparisons) in the following manner:

\hh{table with an overview of the different parameter settings}
```{r, echo=FALSE, results='asis'}
param_setting<- all %>% select(wo, wv) %>% group_by(wo, wv) %>% tally() %>% select(wo,wv)
#t(param_setting)
#param_setting %>% tally() 

knitr::kable(t(param_setting), digits=0, caption = "Overview of parameter settings used for optimization and validation windows for bullet land signatures.", booktabs=TRUE)
```


### Signatures

Figure \ref{fig:type2} gives an overview of type II error rates observed when varying the window size in the optimization step.


```{r type2, fig.width=8, fig.height=5, out.width='\\textwidth', fig.cap="Type II error rates observed across a range of window sizes for optimization $wo$. For a window size of $wo = 120$ we see a drop in type II error rate across all type I rates considered. Smaller validation sizes $wv$ are typically associated with a smaller type II error."}
errors %>% filter(wv %in% c(30,50)) %>%
  ggplot(aes(x = wo, y = beta, colour=factor(alpha))) + 
  geom_point(aes(shape=factor(wv))) +
#  facet_grid(.~wo, labeller="label_both") + 
  geom_smooth(aes(group=alpha), se=FALSE, method="loess") +
  theme_bw() +
  scale_colour_brewer(expression("Nominal type I error"~alpha), palette="Set2") +
  scale_shape_discrete("Size of validation\nwindow, wv") +
  xlab(expression("Window size for optimization, "~wo)) +
  ylab("Type II error rate")


labels = expression(P[M1](tilde(z)>0),P[M0](tilde(z)>0))
```

Figure \ref{fig:type1} compares nominal (fixed) type I error and actually observed type I errors for the parameter settings in table XXX. With an increasing size of the window used in the optimization step the observed type I error rate decreases (slighty). This might be related to the increasing number of tests that fail for larger window sizes, in particular for non-matching striae (see fig \ref{fig:missings}).

```{r type1, fig.width=8, fig.height=5, out.width='\\textwidth', fig.cap="Comparison of observed and nominal type I error rates  across a range of window sizes for optimization $wo$. The horizontal line in each facet indicates the nominal type I error rate.  As the optimization window increase the observed type I error rate gets smaller. A smaller validation window tends to be associated with a higher type I error rate."}
errors %>% filter(wv %in% c(30,50)) %>%
  ggplot(aes(x = wo, y = actual, colour=factor(alpha))) + 
  geom_hline(aes(yintercept=alpha), colour="grey30") +
  geom_point(aes(shape=factor(wv))) +
  #  facet_grid(.~wo, labeller="label_both") + 
  geom_smooth(aes(group=alpha), se=FALSE, method="lm") +
  theme_bw() +
#  scale_y_log10(breaks=c(0.001,.005, 0.01, 0.05)) +
  scale_shape_discrete("Size of validation window, wv") +
  ylab("Observed type I error rate") +
  xlab(expression("Window size for optimization, "~wo)) +
  scale_colour_brewer(expression("Nominal type I error "~alpha), palette="Set2") +
  facet_wrap(~alpha, labeller="label_both", scales="free") +
  theme(legend.position="bottom")
  
```



```{r missings, fig.width=8, fig.height=5, fig.cap="Number of failed tests by the window optimization size, wo, and ground truth. Test results from different sources have a much higher chance to fail, raising the question, whether failed tests should be treated as rejections of the null hypothesis of same source."}
ns <- all %>% group_by(wv, wo, match) %>% summarize(
  n_ss = mean(same_shift_n),
  n_ds = mean(diff_shift_n),
  zero_ss = sum(same_shift_n==0),
  zero_ds = sum(diff_shift_n==0),
  miss = sum(is.na(p_value)),
  missperc = miss/85491*100
)
ns %>% filter(wv %in% c(30, 50)) %>% 
  ggplot(aes(x = wo, y = missperc, shape=factor(wv), colour = factor(match))) + 
  geom_smooth(method="lm", size=.5, se=FALSE) +
  geom_point() + 
  theme_bw() +
  ylab("Percent of failed tests.") +
  xlab("Size of optimization window, wo") +
  scale_colour_brewer("Match", palette="Set1") +
  scale_shape_discrete("Size of \nvalidation\nwindow, wv")
```
Figure \ref{fig:missings} gives an overview of the number of failed tests, i.e. tests in which a particular parameter setting did not return a valid result. This happens, when the shift to align two signatures is so large, that the remaining overlap is too small to accommodate  windows for validation. The problem is therefore exacerbated by a larger validation window. 
Figure \ref{fig:missings} also shows that the number of failed tests is approximately linear in the size of the optimization window. 

```{r lms, results='asis'}
nsnest <- ns %>% filter(wv %in% c(30,50)) %>% group_by(match, wv) %>% nest()
nsnest <- nsnest %>% mutate(
  model = data %>% purrr::map(.f = function(d) lm(I(missperc*100)~wo, data=d)),
  coefs = model %>% purrr::map(.f = broom::tidy)
)
tab <- nsnest %>% unnest(coefs) %>% filter(term=="wo") %>% arrange(match) %>%
  select(-term, -statistic, -p.value)
knitr::kable(tab, digits=3, caption = "Estimates of the increase in percent of failed tests corresponding to a 100 point increase in the optimization window.", booktabs=TRUE)
```

```{r wo120, fig.width = 8, fig.height=4.5}
wo120 <- errors %>% filter(wo==120)
wo120 %>% gather(type, error, actual:beta) %>%
  ggplot(aes(x = wv, y = error, colour = factor(alpha)))  + 
  geom_line() + geom_point() + facet_wrap(~type, scales="free") +
  theme_bw()+ 
  theme(legend.position="bottom")
```