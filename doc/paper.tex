% !TeX program = pdfLaTeX
\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{hyperref}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}

\usepackage{url} % not crucial - just used below for the URL

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

\newenvironment{definition}[1]% environment name 
{% begin code 
  \par\vspace{.75\baselineskip}\noindent 
  \textbf{Definition (#1)}\begin{itshape}% 
  \par\vspace{.5\baselineskip}\noindent\ignorespaces 
}% 
{% end code 
  \end{itshape}\ignorespacesafterend 
}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Adaption of the Chumbley Score to matching of bullet striation marks}

  \author{
        Ganesh Krishnan \thanks{The authors gratefully acknowledge \ldots{}} \\
    Department of Statistics, Iowa State University\\
     and \\     Heike Hofmann \\
    Department of Statistics and CSAFE, Iowa State University\\
      }
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Adaption of the Chumbley Score to matching of bullet striation marks}
  \end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}

\end{abstract}

\noindent%
{\it Keywords:} 3 to 6 keywords, that do not appear in the title
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!

\newcommand{\hh}[1]{{\textcolor{orange}{#1}}}
\newcommand{\gk}[1]{{\textcolor{green}{#1}}}
\newcommand{\cited}[1]{{\textcolor{red}{#1}}}

\tableofcontents
\newpage

\section{Introduction and Background}\label{introduction-and-background}

\subsection{Motivation}\label{motivation}

Same source analyses are a major part of an Forensic Toolmark Examiner's
job. In current practice examiners make these comparisons by visual
inspection under a comparison microscope and come to one of the
following four conclusions: identification, inconclusive, elimination or
unsuitable for examination\textasciitilde{}\citep{afte-toolmarks1998}.
These conclusions are made on the basis of ``unique surface contours''
of the two toolmarks being in ``sufficient agreement''
\citep{afte-toolmarks1998}. AFTE describes the term ``sufficient
agreement'' as the possibility of another tool producing the markings
under comparison, as practically impossible \citep{afte-toolmarks1998}.
This subjectivity in the assessment as well as the lack of error rates
are the main points of criticisms first raised by the National Research
Council in 2009 \citep{NAS:2009} and later emphasized further by the
President's Council of Advisors on Science and Technology
\citep{pcast2016}.

Technological advances, such as profilometers and confocal microscopy
allow to measure 3D surfaces in a high-resolution digitized form. This
technology has become more accessible over the last decade, and has made
its way into topological images of ballistics evidence, such as bullet
lands and breech faces
\citep{DeKinder1, DeKinder2, Bachrach1, vorburger2016}. Digitized images
of 3D surfaces of form the data basis of statistical analysis of
toolmarks. A statistical approach based on data removes both
subjectivity from the assessment and allows a quantification of error
rates for both false positive and false negative identifications.

\hh{In the next page and a half it is easy to lose the red line. It might help to include a table with an overview.
The table should include the reference to the paper, the data used, the statistical method and the associated error rates.}
Various toolmarks have been studied in the literature:
\citet{manytoolmarks1} and \citet{chumbley} have been analyzing
screwdriver marks digitized using a profilometer; \citet{manytoolmarks2}
have investigated 3D marks from screwdriver, tongue and groove pliers
captured using a confocal microscope; \citet{afte-chumbley} have been
investigated digitized marks from slip-joint pliers generated by a
surface profilometer.

\hh{We need an additional sentence here to get from the data to the statistical methods ... }
\citet{manytoolmarks2} define a relative distance metric and use it as
similarity measure between two toolmarks. \citet{manytoolmarks1} extract
many small segments in the markings of two toolmarks and compare
similiarity using a maximum pearson correlation coefficient. The
Chumbley scoring method, first introduced by \citet{chumbley}, uses a
similar but more extensive framework based on a Mann-Whitney U test of
the resulting correlation coefficients. This approach was later improved
by Hadler and Morris \citep{hadler} where the authors introduce a
deterministic approach and address the issue of lack of independence
between segements of striae. In this paper, we are investigating the
applicability of the Chumbley scoring method by \citet{hadler} to assess
striation marks on bullet lands for same-source identification.

Striation marks on bullets are made by impurities in the barrel. As the
bullet travels through the barrel, these imperfections leave
``scratches'' on the bullet surface. Typically, only striation marks in
the land engraved areas (LEAs) are considered \citet{afte-article1992}.
Bullet lands are depressed areas between the grooves made by the rifling
action of the barrel. Compared to toolmarks made by screwdrivers
striation marks on bullets are typically much smaller, both in length
and in width. Bullets also have a curved cross-sectional topography.
Figure\textasciitilde{}\ref{fig:rgl} shows us how the signature from a
bullet land (bottom) lines up with the image of the land (top) from
which it was extracted. We can also see in the figure how the depth and
relative position of the striation markings seen in the image are
interpreted as the signature.

Bullet matching methods are usually based on these associated
signatures. \citet{chu2013} use an automatic method for counting
consecutive matching striae (CMS). The authors report an error rate of
52\% of the known same source lands comparisons as misidentified (false
negative) and zero false positives for known different source lands.
\citet{ma2004} and \citet{vorburger2011} discuss CCF (cross-correlation
function) and its discriminating power and applicability for same-source
analyses of bullets, but do not provide any error rates in their
discussion. \citet{aoas} use multiple features like CCF, CMS, D
(distance measure) etc in a random forest based method and compare every
land against every other land of digitised versions of Hamby 252 and
Hamby 44 \citep{hamby} published on the NIST Ballistics Database
\citep{nist}. The authors report an out-of-bag overall error rate of
0.46\%, comprised of a false positive error rate of 30.05\% and a false
negative rate of 0.026\%.

The Chumbley score provides us with another approach in the same-source
assessment of bullet striation marks. \citet{chumbley} compare two
toolmarks for same-source. The data for this study was obtained from 50
sequentially manufactured screwdriver tips. \citet{chumbley} report
error rates for markings made by the tips at different angles. For
markings made under a 30 degree the authors report an average false
negative error rate of 0.023 and an average false positive error rate of
0.09. For other angles the error rates for false negatives stay the same
while the rate of false positives decreases to 0.01.
\hh{are the angles steeper?} The paper by \citet{hadler} is based on the
same data but the authors focus on markings made under the same angle.
The error rates associated with the deterministic version of the score
are 0.06 for false negatives and a false poisitive error rate of 0.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/B6-B2-L6-rescaled.png}


\caption{\label{fig:rgl} Image of a bullet land from a confocal light microscope at 20 fold magnification (top) and a chart of the corresponding signature of the same land (bottom). The dotted lines connect some peaks visible in both visualizations.}

\end{figure}

\subsection{Scans for land engraved
areas}\label{scans-for-land-engraved-areas}

Comparisons of striae from bullets are usually based on comparisons of
striae in land engraved areas, which are extracted in form of cross
sections, called \emph{profiles} \citep{aoas,ma2004}. From profiles
bullet \emph{signatures} \citep{chu2013,aoas} are extracted as residuals
of a loess fit (or Gaussian filter). They are considered to be noise
free and a good reflection of the key attributes of the raw marking, and
the unique features of a bullet.
\hh{Do you have a reference for the previous sentenced?} A detailed
discussion of the extraction technique for signatures is given in
\citet{aoas}.

\hh{don't split the discussion on the size. between the next paragraph and }
The scans for the Hamby study come from two sources namely, NIST scans
\citep{nist} and scans produced at Iowa State University
\hh{Iowa State is too general. it's the High-resolution microscopy lab and we are CSAFE}.
Both scans were generated using confocal light microscopes at a 20x
magnification but the microscopes collected data at different
resolutions. The NIST database contains scans of the bullet sets Hamby
44 and 252 at a resolution of 1.5625 \(\mu m\) per pixel. The Iowa State
scans are available for only the Hamby 44 bullet set and at a resoultion
of 0.645 \(\mu m\) per pixel. The data used by \citet{chumbley, hadler}
on the other hand, was produced by a profilometer at a resolution of
about 0.73 \(\mu m\) per pixel. The different resolutions indicate that
the length of the digitized markings would be significantly different
for each setting.

\subsection{Potential Challenges in Chumbley Score
Adaptation}\label{potential-challenges-in-chumbley-score-adaptation}

Markings made by screw driver tips \citep{chumbley} are longer and
pronounced and about 7 mm in length \citep{manytoolmarks1}, while the
groove to groove length of the bullet lands from the Hamby study
\citep{hamby} are about 2mm in length. The Ruger pistol barrels used in
the Hamby study have a 9mm caliber. This means that bullets used in
these pistols have land markings of only about a quarter of the size of
screwdriver toolmarks used by \citet{chumbley, hadler}, therefore
raising a problem of distinctiveness. One way to investigate this is to
evaluate bullet striations at different resolutions. This gives us an
incentive to use both the NIST and Iowa State scans and investigate the
effect of length of the digitized mark on the working of the algorithm.
Apart from this, the algorithm developed by \citet{hadler} has only been
tested for flatter and wider surfaces which have negligible curvature,
whereas striations on bullets are made on their curved surfaces.
Therefore, using methods proposed for toolmarks need adaptation in order
to give tangible results for bullets. Another challenge that arises from
these topographical differences, is the understanding of that effect of
different levels of smoothing. Especially investigating its effect on
the algorithm performance when signatures are being compared and when
raw markings or profiles are being compared.

Some other challenges associated with the adaptation process call for a
brief understanding of the methodology involved. The Chumbley method and
algorithm works on the premise of comparison windows that are supposed
to represent small segments of pixels of equal sizes in the two
markings. These windows are to be compared in a predetermined fashion
and are termed as windows of optimization and validation. The
optimization window is selected in the first step called the
optimization step where areas/windows of best agreement between the the
two markings are found.

The window of optimization for bullet striations are shorter as bullet
signatures in terms of absolute length (in mm), are smaller compared to
toolmarks. Even in terms of digitized pixels, there is no
straightforward way of establishing what window size is to be chosen.
One important way to address this is to keep the number of windows of
optimization sufficiently large or not too small. This means, if the
number of windows are too small we inadvertently use bigger window sizes
as we want to compare all segements of one mark with the segments of the
other mark. This leads to not giving enough emphasis to smaller
segements of the two marking that might be in high agreement. In terms
of absolute length we end up comparing smaller segments between the two
markings while dealing with bullet lands, than those being compared for
toolmarks.

For too large window sizes, the weight of small features that would
otherwise uniquely classify a signature and hence identify the region of
agreement is vastly reduced. On the other hand, this introduces another
problem as, if we go too small in the window of optimization, the unique
features of the trace segments are lost and all seem similar. The
problem of distinctiveness in the structure of bullet striations may
also lead to potential failure of the algorithm to identify correct
windows of agreements or not be able to identify these windows at all.
This may lead to incorrect classification of bullets as coming from the
same-source (or matches) or from different source (or non-matches).

Thus, the comaparison windows have a direct influence on the false
positives and false negatives, making identification of the optimum
sizes of these windows very important. This raises important questions
about how and what parameter settings need to be chosen for bullet land
comparison. Something similar is done in \citet{afte-chumbley} for
toolmark comparisons of slip-joint pliers where optimum window sizes are
determined. There is a need to figure out the best parameter settings
which minimizes the errors for unsmoothed markings or profiles and
pre-processed signatures. An analysis of these error rates and
comparison with other methods will help us understand the adaptibility
of the chumbley score to bullets.

\subsection{The Chumbley Score Test}\label{the-chumbley-score-test}

The Chumbley score algorithm takes input as two vectorized processes.
Here the processes are of the form \(z(t)\) which is a spatial process
for some location indexed with \(t\). This means that while \(z(t)\)
represents any striae, \(z(t_1)\) is a realization of the spatial
process. \(t\) here denotes a vector of equally spaced pixel locations
for the striation marks under consideration. The vectorized processes
representing two sets of striaes are therefore shown as \(x(t_1)\),
\(t_1 = 1,2,...T_1\) and \(y(t_2)\), \(t_2 = 1,2...T_2\). Here \(x\) and
\(y\) denote two instances of the process \(z(*)\) which means two
striation marks, while the indexing \(t\) or pixel range is shown as
\(t_1\) and \(t_2\) for \(x\) and \(y\) respectively. This
representation of \(t\) as \(t_1\) and \(t_2\) lets us identify them as
either of same or different lengths. The striation marks under
consideration are potentially from two different bullets whose source
needs to be identified as being same or different. \(T_1\) and \(T_2\),
as represented above, are the final pixel indexes of each marking and
therefore give the respective lengths of the markings. The similarity in
the two markings is then judged by the algorithm on the basis of
cross-correlation of a fixed and constant number consecutive pixels (say
\(k\)) taken from the two markings. This can for example be \(k\) taken
from the two indexed marking \(x(t_1)\) and \(y(t_2)\) such that in
theory \(k\) remains smaller than length of the two striae or marks.
Depending on what stage of the algorithm we are in, matching of
different pixel lengths and locations is done. This in the end,
effectively compares all possible windows that would guarantee in
quantifying the two marks or striae as coming from the same source or
not.

The algorithm works in two phases, namely, an optimization step and a
validation step, at the end of which a Mann Whitney U statistic is
calculated as a measure to differentiate between matches and
non-matches. A pre-processing step to the algorithm is to choose a
coarseness value which is used as a parameter to the lowess smoothing
function. The coarseness essentially gives the proportion of points
which influence the smooth at each value, which means larger values lead
to more smoothness. The lowess smoothing is applied to each of two sets
of vectorized striae or marks \(x(t_1)\) and \(y(t_2)\), before
proceeding to the algorithm. The algorithm starts with the optimization
step where the area of best agreement in the two markings being compared
is identified. Comparison window size is predefined. Each window of the
first marking is compared with all windows of the second marking. A
maximum correlation statistic is used to identify the region of best
agreement, with the maximum usually seen being near 1 for both cases
which is what is intuitively expected for matches, and not expected
intuitively for non matches. \citet{hadler} in their paper proposed an
improvement to this algorithm by trying to remove mutual dependence of
parameters. The mututal dependence was due to the serial correlation in
surface depth values of a marking. Also a random sampling sub step in
the validation phase makes a group of pixels to be chosen more than once
and hence introduces lack of independence in certain steps. The reason
for removing the mutual dependence was the Mann-Whitney U statistic
works under the assumption of independence of parameters. The validation
step builds on this with two sub-steps namely, same shift and different
shift. As the aim is to come up with a non-parameteric U statistic,
\citet{hadler} proposed a normalization procedure in the validation step
which goes to some extent to address the issue of mutual dependence. A
series of windows are chosen in both the same and different shift
sub-steps for the purpose of comparison. Both substeps were modified by
\citet{hadler} who introduced a deterministic rule for sampling of
windows in the same-shift and different shift as opposed to the
originally proposed random sampling by \citet{chumbley}. In th same
shift substep the chosen series of windows are at a common distance
(rigid-shifts) from the window earlier identified as the region of best
agreement in the optimization step. The correlation of these windows
generally turn out to be lower than the maximum correlation window. The
significance is that these same shift windows still have large enough
correlation values for the two markings being compared that are in
reality a match. Any choice of maximum correlation windows (where
correlation values are often near 1) for non-matches in the optimization
step are also validated by this sub-step. This is because the same-shift
correlations would not be anywhere as large for same-shift windows when
all windows in the sub-step are compared. The different shift substep on
the other hand gives perspective to the correlation values of the same
shift window correlation values. There are no rigid-shifts but different
shifts where distance from the maximum correlation window are chosen
randomly by \citet{chumbley} and deterministically as per
\citet{hadler}. This means that there is an equal possibilty of
comparing a trace segment from one marking to any trace segment or
window in the second marking.

Neither of above sets of correlation in the two sub-steps are allowed to
include the maximum correlation window as identified earlier. Therefore
the assumption is that if two markings match each other, the same-shift
correlations would be larger than the different-shift windows. And if
they are not a match the correlations in the two sets will be very
similar. The U-statistic tests for the null hypothesis that the two
markings are not a match and are therefore not made by the same source.
As given by \citet{hadler}, it is computed from the joint rank of all
correlations of both the same and different shift samples.

\paragraph{endedit}\label{endedit}

\hh{end of intro: remaining paper is structured as follows: introduce to the data we get from confocal microscopy, introduce to profiles and signatures.
Introduce to chumbley score method, apply chumbley and discuss results ....
}

\subsection{Scans for land engraved
areas}\label{scans-for-land-engraved-areas-1}

\begin{itemize}
\tightlist
\item
  scans available: NIST database (citation), Hamby 44 and Hamby 252
  (Hamby citation)
\item
  move figure up
\item
  discuss cross section, profiles and signatures
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{figures/win-comparison-1} 

}

\caption{ The two plots on the left show how the same shift behaves in case of a matching pair and the two plots on the right show how the different shift behaves in case of a matching pair.}\label{fig:win-comparison}
\end{figure}

\section{Simulation setup}\label{simulation-setup}

Following on similar lines to the setup of toolmarks, the first step
here is to first identify what difference does different window sizes of
optimization and the validation step have, when adapting the toolmark
method to bullets.

The marking made on bullets are smaller than toolmarks and is also less
wider. The idea is to find out possible areas of error while adapting
the score based method proposed for toolmarks, using cross-validation
setup to identify appropriate parameter settings for (a) signatures and
(b) profiles directly

\subsubsection{Signatures}\label{signatures}

Signatures of lands for all Hamby-44 and Hamby-252 scans made available
through the NIST ballistics database \citep{nist} were considered. Both
of these sets of scans are part of the larger Hamby study \citep{hamby}
and each consist of twenty known bullets (two each from ten
consecutively rifled Ruger P85 barrels) and fifteen questioned bullets
(each matching one of the ten barrels). Ground truth for both of these
Hamby sets is known and was used to assess correctness of the tests
results.

Bullet signatures being compared at this time are therefore from the
Hamby 44 and Hamby 252 data. The database setup and pre-processing
system used for choosing the Bullet signatures are as described by
\citet{aoas}. In order to choose the bullet signatures we first filter
out Land\_id for Profiles from the Hamby 44 and Hamby 252 data and
remove all NA values. Then run\_id = 3 is chosen as the signatures
generated from this run\_id give the closest match. Different run\_id's
have some different settings for generating the signatures.(The level of
smoothing does not seem to be one of them)

The bullet signatures when generated by this process already includes a
loess smoothing. Therefore, the coarseness factor is set to 1 while
running the chumbley non random algorithm for comparing different
optimization windows.The algorithm generates the same\_shift,
different\_shift, U-Stat and P\_value parameters which are then used to
calculate the errors associated with different sets of window sizes.

\subsubsection{Profiles}\label{profiles}

The profiles are cross-sectional values of the the bullet striation mark
which are chosen at an optimum height (x as used by \citet{aoas}). This
x or height is not a randomly chosen level. The rationale behind the
choice has been explained by \citet{aoas}. A region is first chosen
where the cross-correlation seems to change very less and in this region
an optimum height is chosen. The profiles generally resemble a curve
which is more or less similar to a quadratic curve (a quadratic fit to
the raw data values of the profile is not an exact fit but it does show
a similar trend). Profiles are the set of raw values representing the
striation marks, and signatures are generated from these by removal of
the inherent curvature and applying some smoothing (the signatures
generated by \citet{aoas} use a loess function for smoothing).

Similar to signatures the run\_id = 3 was used when applying the
chumbley algorithm using the database setup given by \citet{aoas} of
Hamby-44 and Hamby-252 datasets, on the profiles. The run\_id not only
defines the level of smoothing but also signifies the chosen height at
which the profiles were selected initially. Another important aspect is
the range of horizontal values (which is referred to as the y values in
\citet{aoas}) in the signatures. These have already been pre-processed
in the database to not include any grooves.

Therefore for the sake of comparison the run\_id = 3 is still chosen so
as to ensure that the horizontal values remain the same as that of the
signatures. This also gives us profiles with the grooves removed.

The idea therefore is to first use these raw values of the profile
directly in the chumbley algorithm, and see how the algorithm performs
for different coarseness values (smoothing parameter as referred in the
function lowess used in the chumbley algorithm).

\pagebreak

\section{Results}\label{results}

We used the adjusted Chumbley method as proposed in \citet{hadler} and
implemented in the R package \texttt{toolmaRk} \citep{toolmark} on all
pairwise land-to-land comparisons of the Hamby scans (a total of 85,491
comparisons) with the pairwise sets for the comparisons given in the
table \ref{tab:param}.

\begin{table}[!h][!h]
\caption{\label{tab:param}Overview of parameter settings used for optimization and validation windows for bullet land signatures.}

\centering
\resizebox{\linewidth}{!}{\resizebox{\linewidth}{!}{\begin{tabular}[t]{lrrrrrrrrrrrrrrrrrr}
\toprule
wo & 50 & 50 & 60 & 60 & 80 & 80 & 90 & 90 & 100 & 100 & 110 & 110 & 120 & 120 & 120 & 120 & 120 & 130\\
wv & 30 & 50 & 30 & 50 & 30 & 50 & 30 & 50 & 30 & 50 & 30 & 50 & 10 & 20 & 30 & 50 & 60 & 30\\
\bottomrule
\end{tabular}}}
\centering
\begin{tabular}[t]{lrrrrrrrrrrrrrrr}
\toprule
wo & 130 & 140 & 140 & 150 & 150 & 160 & 160 & 200 & 200 & 200 & 240 & 240 & 280 & 280 & 320\\
wv & 50 & 30 & 50 & 30 & 50 & 30 & 50 & 20 & 30 & 50 & 30 & 50 & 30 & 50 & 30\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!h]
\caption{\label{tab:confusion}Confusion Table for different optimization window sizes with validation window size as 30.}

\centering
\begin{tabular}[t]{llrl}
\toprule
\hspace{1em}\hspace{1em}signif & match & Freq & Type\\
\midrule
\addlinespace[0.5em]
\multicolumn{4}{l}{\textbf{Size of Optimization Window = 280}}\\
\hspace{1em}FALSE & FALSE & 78909 & True Negative\\
\hspace{1em}TRUE & FALSE & 4192 & False Positive (Type I)\\
\hspace{1em}FALSE & TRUE & 446 & False Negative (Type II)\\
\hspace{1em}TRUE & TRUE & 731 & True Positive\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{llrl}
\toprule
signif & match & Freq & Type\\
\midrule
\addlinespace[0.5em]
\multicolumn{4}{l}{\textbf{Size of Optimization Window = 120}}\\
\hspace{1em}FALSE & FALSE & 79249 & True Negative\\
\hspace{1em}TRUE & FALSE & 4523 & False Positive (Type I)\\
\hspace{1em}FALSE & TRUE & 386 & False Negative (Type II)\\
\hspace{1em}TRUE & TRUE & 854 & True Positive\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{llrl}
\toprule
signif & match & Freq & Type\\
\midrule
\addlinespace[0.5em]
\multicolumn{4}{l}{\textbf{Size of Optimization Window = 80}}\\
\hspace{1em}FALSE & FALSE & 79477 & True Negative\\
\hspace{1em}TRUE & FALSE & 4503 & False Positive (Type I)\\
\hspace{1em}FALSE & TRUE & 463 & False Negative (Type II)\\
\hspace{1em}TRUE & TRUE & 781 & True Positive\\
\bottomrule
\end{tabular}
\end{table}

\subsection{Signatures}\label{signatures-1}

Figure \ref{fig:type2} gives an overview of type II error rates observed
when varying the window size in the optimization step. Two levels of
validation window size 30 and 50 were chosen as to compare the error
rates for different nominal type I errors. We notice that the trends for
these nominal type I errors are similar and in most cases a validation
window of 50 has higher type II error than for 30. A change in this
trend is seen for a 0.05 \(\alpha\) level, although the difference
between the two windows is very small for this case. We can also notice
an obvious trend of increase in the Type II error as the window of
optimization increases and see a minimum around the optimization window
size of 120 pixels. Hence we are inclined to choose a smaller validation
window size and optimization window as 120.

Table \ref{tab:confusion} shows the confusion tables with the
classification of type I and type II errors and how the numbers change
with a change in the optimization window. The windows represent areas to
the left of the window with minimum type II, near the minimum type II
window and to the far right of the minimum type II error

\begin{figure}

{\centering \includegraphics[width=\textwidth]{figures/type2-1} 

}

\caption{Type II error rates observed across a range of window sizes for optimization $wo$. For a window size of $wo = 120$ we see a drop in type II error rate across all type I rates considered. Smaller validation sizes $wv$ are typically associated with a smaller type II error.}\label{fig:type2}
\end{figure}

Figure \ref{fig:type1} compares nominal (fixed) type I error and
actually observed type I errors for the parameter settings in table
\ref{tab:param}. With an increasing size of the window used in the
optimization step the observed type I error rate decreases (slighty).
This means as the optimization window increase the observed type I error
rate gets smaller. A smaller validation window on the other hand, tends
to be associated with a higher type I error rate. This can be better
imagined for a given window of optimization, where the actual Type I
error is comparable to the nominal level for only a select few
validation window sizes. For these comparable validation window sizes of
30 and 50 as done here, the actual type I error increases very slightly
and can be seen in Figure \ref{fig:type1}. This increase is not as much
when compared to the variation seen with the optimization window sizes.
This effect might be related to the increasing number of tests that fail
for larger optimization window sizes, in particular for non-matching
striae (see fig \ref{fig:missings}).

\begin{figure}

{\centering \includegraphics[width=\textwidth]{figures/type1-1} 

}

\caption{Comparison of observed and nominal type I error rates  across a range of window sizes for optimization $wo$. The horizontal line in each facet indicates the nominal type I error rate.}\label{fig:type1}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{figures/wo120-1} 

}

\caption{The figure on the left shows the actual Type I error while the figure on the right shows the Type II error for different validation window sizes and different chosen nominal alpha levels when the size of the optimization window = 120}\label{fig:wo120}
\end{figure}

The actual type I error and type II error for signatures were also
compared for different validation window sizes. Figure \ref{fig:wo120}
shows the actual rates for different nominal \(\alpha\) levels. We can
see that the type II error rises with higher validation windows for the
smaller nominal \(\alpha\) levels while for the nominal \(\alpha\) =
0.05 its almost constant.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{figures/missings-1} 

}

\caption{Number of failed tests by the window optimization size, wo, and ground truth.}\label{fig:missings}
\end{figure}

\begin{table}

\caption{\label{tab:lms}Estimates of the increase in percent of failed tests corresponding to a 100 point increase in the optimization window.}
\centering
\begin{tabular}[t]{lrrr}
\toprule
match & wv & estimate & std.error\\
\midrule
FALSE & 30 & 0.503 & 0.028\\
FALSE & 50 & 0.922 & 0.035\\
TRUE & 30 & 0.043 & 0.004\\
TRUE & 50 & 0.056 & 0.005\\
\bottomrule
\end{tabular}
\end{table}

Figure \ref{fig:missings} gives an overview of the number of failed
tests, i.e.~tests in which a particular parameter setting did not return
a valid result. This happens, when the shift to align two signatures is
so large, that the remaining overlap is too small to accommodate windows
for validation. The problem is therefore exacerbated by a larger
validation window. Figure \ref{fig:missings} also shows that the number
of failed tests is approximately linear in the size of the optimization
window. Test results from different sources have a much higher chance to
fail, raising the question, whether failed tests should be treated as
rejections of the null hypothesis of same source. For known non-matches
there is a higher possibility that in the optimization pair of windows
where cross-correlations are maximum are too far apart, and same shifts
of this order hit the end of the signature.

\subsection{Profiles}\label{profiles-1}

Figure \ref{fig:prof_missings} (a) shows the type II error rates for
profiles for the optimization window 120 and validation window 30 with
varying level of coarseness. We can see that the type II error for all
the nominal \(\alpha\) levels is lowest in the range of 0.20 to 0.35.
Therefore, a value of 0.25 can be used keeping in mind it keeps the type
II error lowest while runnning simulations. Thus for comparisons of
different window sizes etc as seen in the differnt parts of Figure
\ref{fig:prof_missings} this coarseness value is used.

On the other hand Figure \ref{fig:prof_missings} (b) shows if the
coarseness level set in the chumbley agorithm has any effect on the
signatures, which are pre-processed and already smoothed to a certain
extent. From Figure \ref{fig:prof_missings} (b) we can notice that for
different nominal \(\alpha\) levels, the type II error fluctuates
slightly but does not change much, thereby helping us conclude that the
coarseness levels set in the LOWESS smoothing in the chumbley alggorithm
does effect the type II error much for signatures.

\subsubsection{Comparison of profiles and
signatures}\label{comparison-of-profiles-and-signatures}

Another reason for failed tests can be incorrect identification of
maximum correlation windows in the optimization step as seen in figure
\ref{fig:prof_missings}(d) because of the level of smoothing, as too
much smoothing would subdue intricate features that might otherwise help
in the correlation calculations and correct identification of maximum
correlation windows irrespective of the size. This would again cause a
simiar effect as explained for figure \ref{fig:missings} with validation
windows, irrespective of size, during the shifts end up at the ends of
the markings resulting in an invalid calculation and failed comparison
attempt.

In figure \ref{fig:prof_missings}(d) and (f), we compare profiles and
signatures on the basis of number of failed tests. The profiles chosen
for figure \ref{fig:prof_missings}(f) have a constant coarseness of 0.25
and window of optimization as 120. The signatures in this case are not
smoothed using the chumbley algorithm step of LOWESS smoothing. Instead
signatures are used as calculated by \citet{aoas}. The smoothing in
these signatures were determined and fixed on the basis of their
performance in the random forest based algorithm proposed by
\citet{aoas}. The comparison of profiles and signatures with variation
of validation window size therefore is made on even footing. The trends
are similar to figure \ref{fig:missings} in the sense that for known
non-matches the number of failed tests are more for both signatures and
profiles and increasing linearly with the validation window size. The
problem is however, worse for profiles which has higher number of failed
tests than signatures for all validation windows.

The total error for different validation window sizes for signatures and
profiles can be seen in figure \ref{fig:prof_missings} (e).The
optimization window size is 120 and profiles are calculated at a default
0.25 coarseness level while signatures as before are not smoothed again
in the modified chumbley algorithm. We can see that the total error is
always higher for profiles as compared to signatures for all sizes of
validation window.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{figures/prof_missings-1} 

}

\caption{Row 3:  Total error and Number of failed tests by the window validation size, wv, and ground truth, Row 2: Total error and Number of failed tests with Coarseness for both profiles and signatures, Row 1: Type II error for different coarseness levels as used in the modified chumbley algorithm for profiles and signatures}\label{fig:prof_missings}
\end{figure}

\pagebreak

\subsection{Conclusion}\label{conclusion}

The results suggest that the Nominal type I error \(\alpha\) value shows
dependence on the size of the window of optimization. For a given window
of optimization the actual Type I error is comparable to the nominal
level for only a select few validation window sizes and for comparable
validation window sizes of 30 and 50 as done here, the actual type I
error does not seem to vary as much as it varies with the optimization
window sizes . A Test Fail, i.e.~tests in which a particular parameter
setting did not return a valid result, happens, when the shift to align
two signatures is so large, that the remaining overlap is too small to
accommodate windows for validation, depends on whether known-match or
known non-matches has predictive value, with test results from different
sources having a much higher chance to fail. On conducting an analysis
of all known bullet lands using the adjusted chumbley algorithm, Type II
error was identified to be least bad for window of validation 30 and
window of optimization 120. In case of unsmoothed raw marks (profiles),
Type II error increases with the amount of smoothing and least for
LOWESS smoothing coarseness value about 0.25 or 0.3. In an effort to
identify the level of adaptiveness of the algorithm, comparisons were
made between signatures and profiles. Their comparison with respect to
validation window size for a fixed optimization window size suggested
that, profiles have a total error (i.e all incorrect classification of
known-matches and known non-matches) greater than or equal to the total
error of signatures for all sizes of validation window. Profiles also
fail more number of times than signatures in a test fail (for different
coarseness keeping windows fixed and also for different validation
windows keeping coarseness fixed) which lets us conclude that the
behaviour of the algorithm for the profiles instead of pre-processed
signatures is not better. Finally it should be noted that the current
version of the adjusted chumbley algorithm seems to fall short when
compared to other machine-learning based methods \citet{aoas}, and some
level of modification to the deterministic algorithm needs to be
identified and tested that would reduce the number of incorrect
classifications.

\bibliographystyle{agsm}
\bibliography{bibliography}

\end{document}
