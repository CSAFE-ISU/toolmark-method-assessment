% !TeX program = pdfLaTeX
\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{hyperref}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}

\usepackage{url} % not crucial - just used below for the URL

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

\newenvironment{definition}[1]% environment name 
{% begin code 
  \par\vspace{.75\baselineskip}\noindent 
  \textbf{Definition (#1)}\begin{itshape}% 
  \par\vspace{.5\baselineskip}\noindent\ignorespaces 
}% 
{% end code 
  \end{itshape}\ignorespacesafterend 
}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Adaption of the Chumbley Score to matching of bullet striation marks}

  \author{
        Ganesh Krishnan \thanks{The authors gratefully acknowledge \ldots{}} \\
    Department of Statistics, Iowa State University\\
     and \\     Heike Hofmann \\
    Department of Statistics and CSAFE, Iowa State University\\
      }
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Adaption of the Chumbley Score to matching of bullet striation marks}
  \end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}

\end{abstract}

\noindent%
{\it Keywords:} 3 to 6 keywords, that do not appear in the title
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!

\newcommand{\hh}[1]{{\textcolor{orange}{#1}}}
\newcommand{\gk}[1]{{\textcolor{green}{#1}}}
\newcommand{\cited}[1]{{\textcolor{red}{#1}}}

\tableofcontents
\newpage

\section{Introduction and Background}\label{introduction-and-background}

\subsection{Motivation}\label{motivation}

Striation marks are used to determine same source for bullets. Current
standards ask for a visual inspection of the marks under a comparison
microscope by a firearms examiner. One of the problems raised by the NAS
report in 2009 is the subjectivity involved in this comparison and the
need for determining error rates \citet{NAS:2009}. The issue of
subjectivity of a firearms examiner has been reviewed by many authors
where things like scientific principle testability and error rates are
considered to be defining aspects for an objective analysis such that
without identifying the uncertainities associated with a method of
comparison, it is inconclusive to regards an analysis complete. This
means that with the usual method of visual comparison that is adopted by
firearms examiners, lies the problem of lack of any systemized
quantification and as such there seems no certain way of associating it
with any kind of mathematical probability. Since determining error rates
has been duly noted as a fundamental problem in forensic science
\citet{NAS:2009}, there is a need of methods that address this problem.
Some of the methods that have been used to estimate error rates in
firearm examinations include Automatic cartridge case comparison where
same source identification of cartridge cases and associated error rates
have been provided by \citet{riva}, although as noted by \citet{aoas},
in this case alignments of striae involves roation of planes, which
cannot be generalized for bullets. In case of methods that use machine
learning algorithms some methods which are bootsrap based methods often
tend to give over-estimated error rates \citet{efron}, but there are
alternate error estimation techniques as described by \citet{aoas},
\citet{efron} and \citet{vorburger2016} which give better results.

In case of methods that do not use machine learning algorithm, matching
two striation marks with each other in order to identify if it is from
the same source, can also be done using a well defined comparative
statistical algorithm. The statistical algorithm is therefore expected
to go through a step by step procedure of identifying the class
characteristics i.e striation marks for bullets, choosing the striae,
and then follow a systematic procedure of comparing two striae with each
other on the basis of criteria like cross-correlation (or others). Such
a methodology would make it possible to determine error rates too and
give definite results regarding what proportion of cases would the
analysis be successful in identifying a match correctly and in how many
cases would it end up being a false positive.

Identification criteria like consecutively matching striae (CMS) as
first seen by Biasotti 1959 \citet{biasotti} and later mentioned in
\citet{chu2013} more often than not include subject bias and are error
prone and as seen in the work of \citet{miller} the number of CMS may
turn out to be high even when the bullet is not fired by the same
firearm. Parameters like cross-correlation factor as seen in the
extensive comparisons made by \citep{aoas} seem to perform much better
for matching purposes.

An important aspect of same source matching in firearms is
identification of bullet microtopographies that are unique to be chosen
for a match, Bullet profiles and signatures prove to be such features.
The choice of a bullet signature to uniquely define a bullet depends on
finding first, a stable region on a Bullet land which minimal noise and
many pronounced striation marks. \citet{aoas}

The method employed by \citet{aoas} uses the cross correlation factor as
a means to identify the stable region. The markings or striae in this
region are supposed to have a high CCF with each other. Therefore a
Bullet profile is chosen from this region by taking the cross section at
a given height. A loess fit on the bullet profile produces residuals
which are termed as signatures and can be used as a unique identifier
that can be used while matching two bullets to a firearm.

Compairing pairs of toolmarks on the otherhand, with the intention of
matching it to a tool has been studied relatively more in the past as
compared to bullets, and \citet{chumbley} have described in their paper
an algorithm and analytic method that compares two toolmarks and come to
the conclusion if they are from the same tool or not. The method also
determines the error rates, reduces subject bias and designate the two
toolmarks as matches or non-matches with respect to a source.
\citet{chumbley} used an empirical based setup to validate their
proposed analytic and quantitative algorithm.

\citet{chumbley} found that the algorithm gives false-positives in
slightly over 2 cases (or 2\%) of the time and about 9 false-negatives
(or 9\%) for every 100 comparisons. \citet{hadler} on the other hand, in
the improved version of the algorithm first described by
\citet{chumbley} found false-positives for 0 cases and 3 false-negatives
(or 6\%) for 50 comparisons of knwon matches.

\begin{table}[!h]
\centering
\begin{tabular}{lrr}
\toprule
Classification & Match & Non-Match\\
\midrule
Match & 47 & 3\\
Non-Match & 0 & 50\\
\bottomrule
\end{tabular}
\end{table}

\subsection{Potential limitations of the Chumbley Score adaptation to
bullets}\label{potential-limitations-of-the-chumbley-score-adaptation-to-bullets}

Bullets are much smaller in length, width, are not flat and curved in
the cross-sectional topography as opposed to tools like screw driver
tips which produced longer and pronounced markings. This means the
makings made by barrels in comparison to toolmarks may have a problem in
distinctiveness. The majority of Bullet profiles and signatures
extracted by procedures mentioned by \citet{aoas} are almost 1/4 th the
size of toolmarks as used by \citet{chumbley} or even smaller.
Striations on Bullets are made on their curved surfaces, whereas the
algorithm developed by \citet{chumbley} and \citet{hadler} has only been
tested for flatter and wider surfaces which have negligible curvature.
Therefore, using methods proposed for toolmarks may need adaptation in
order to give tangible results for bullets. Moreover, in order to to get
flat bullet signatures and remove the curvatures some kind of smoothing
needs to be applied as a pre-step which needs further investigation as
to whether the level of smoothing does effect the working of the
algorithm on Bullets.

Also when in the optimization step, the Window of optimization for
bullets will be shorter as the signatures are smaller. The idea is to
keep the number of windows of optimization sufficiently large, which
means shorter Trace segments (partition of signature or toolmark with
length = size of window of optimization) that lets us compare smaller
segments of one signature to another. This introduces a problem as, if
we go too small in the window of optimization, the unique features of
the trace segments are lost and seem similar, while too large sizes
vastly reduces the weight of small features that would otherwise
uniquely classify a signature and hence identify the region of
agreement.

Thus the Window of Optimization has a direct influence on whether we are
Falsely rejecting the null when it is true (Type I) or Falsely accepting
the null when it is false (Type II) making identification of the optimum
size of window of optimization very important. This raises important
questions about what parameter settings need to be chosen for bullet
land comparison. Something similar is done in \citet{afte-chumbley} for
toolmark comparisons of slip-joint pliers where optimum window sizes are
determined. There is a need to figure out the best parameter settings
which minimizes the errors for unsmoothed markings and pre-processed
signatures as determined by \citet{aoas}. An analysis of these error
rates and comparison with other methods will help us understand the
adaptibility of the chumbley score to bullets.

\hh{figure \ref{fig:rgl} needs to be mentioned somewhere in the write-up}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/B6-B2-L6-rescaled.png}


\caption{\label{fig:rgl} Image of a bullet land from a confocal light microscope at 20 fold magnification (top) and a chart of the corresponding signature of the same land (bottom). The dotted lines connect some peaks visible in both visualizations.}

\end{figure}

\section{The Chumbley Score Test}\label{the-chumbley-score-test}

The Chumbley score algorithm takes input as two vectorized processes
\(x(t_1)\), \(t_1 = 1,2,...T_1\) and \(y(t_2)\), \(t_2 = 1,2...T_2\)
which denote two sets of marks or striae. Here the processes are of the
form \(z(s)\) which is a spatial process for some \(t\), while
\(z(s_1)\) is the realization of the process in \(s_1\). The \(t_1\) and
\(t_2\) can be better understood as equally spaced pixel locations for
the two marks under consideration, where a `pixel' is the resolution of
the confocal light microscope. In the case for bullet signatures a pixel
corresponds to 0.645 microns. These marks or striae are potentially from
two different bullets or two different toolmarks whose source needs to
be identified as being same or different. The marks or striae are
indexed by the pixel location where \(t_1\) is for the first striae
referred to as \(x\) and \(t_2\) is for the second striae which is
referred as \(y\). \(T_1\) and \(T_2\) being the respective lengths of
the markings, need not be the same but are usually of similar lenghtsThe
similarity is then judged by the algorithm on the basis of
cross-correlation of a fixed and constant number consecutive pixels say
\(k\) taken from the two indexed marks \(x(t_1)\) and \(y(t_2)\) such
that in theory \(k\) remains smaller that length of the two striae or
marks which is the same as \(t_1\) and \(t_2\). Depending on the what
stage of the algorithm we are in, matching of different pixel lengths
and locations is done, which at the end effectively compares all
possible windows that would guarantee in quantifying the two marks or
striae as coming from the same source or not, which also lets us assess
the error rates by checking for a large number of cases.

The algorithm works in two phases, namely, an optimization step and a
validation step, at the end of which a Mann Whitney U statistic is
calculated. A pre-processing step to the algorithm is to choose a
coarseness value which is used as a parameter to the LOWESS smoothing
function. The coarseness essentially gives the proportion of points
which influence the smooth at each value, which means larger values lead
to more smoothness. The LOWESS smoothing is applied to each of two sets
of vectorized striae or marks \(x(t_1)\) and \(y(t_2)\), before
proceeding to the algorithm.

\citet{hadler} in their paper proposed an improvement to this algorithm
by trying to remove mutual dependence of parameters (due to serial
correlation in surface depth values of a toolmark and because of a
random sampling sub step in the validation phase which makes a group of
pixels to be chosen more than once and hence introduces lack of
independence) in certain steps, especially because the Mann-Whitney U
statistic that is later calculated in the algorithm and used as a
measure to differentiate between matches and non-matches works under the
assumption of independence of parameters.

Since we are only interested in a non-parameteric U statistic, Hadler et
al. proposed a normalization procedure in the Validation step that goes
to some extent to address the issue of mutual dependence. Also the same
shift and different shift substep was modified to use a deterministic
rule for sampling sam-shhift and dofferent shift-samples as opposed to
the originally proposed random samples.

The data that is used by \citet{chumbley} is generated by a surface
profilometer that gives the height in terms of distance along a linear
trace. This is taken perpendicular to the striations present in the
toolmark. Two such trace are then compared using the algorithm.

\subsection{Detailed algorithm}\label{detailed-algorithm}

\subsubsection{Optimization step}\label{optimization-step}

The idea behind this step is to first identify the area of best
agreement in the two toolmark data. Comparison window size is predefined
by the user, which in case of screwdriver toolmarks was chosen by
Chumbley et al and Hadler et al as around 10 percent of the length of
the toolmarks, which was around 500. This window is henceforth referred
to as Window of optimization.

A maximum correlation statistic is used to identify the region of best
agreement, with the maximum usually seen being near 1 for both cases
which is what we intuitively expect for matches, and something which we
do not intuitively expect for non matches

First, there are a very large number of cross-correlations calculated
for the two series of striae or marks \(x(t_1)\) and \(y(t_2)\), for eg
if the window of optimization was defined as 200 and toolmark pixel
length is around 1000 then we have 1200-(200-1) = 1001.

This is the number of windows we have for one toolmark and each window
is compared with all windows of the second toolmark (the number of
windows is again similar), and the window with the maximum correlation
is identified to be the region where the toolmarks are in maximum
agreement with each other

\subsubsection{Validation Step}\label{validation-step}

\paragraph{Same-shift:}\label{same-shift}

In this step a series of windows are chosen at random (originally by
chumbley et al) and deterministically (by hadler et al), but at a common
distance (rigid-shift) from the window identified as the region of best
agreement in the vaildation step. The correlation of these windows would
be as intuitively assumed i.e.~lower than the maximum correlation window
(Optimization step), but the significance is that these same shift
windows will still have large enough correlation values for two
toolmarks or signatures that are in reality a match.

This also validates that if in the optimization step the maximum
correlation window chosen (with correlation value near 1) was by
accident (like in case of signatures that in reality are a non-match),
all these same- shift correlations (A fixed number of these trace
segments are identified) would not be anywhere large enough for all
same-shift windows.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{figures/win-comparison-1} 

}

\caption{ The two plots on the left show how the same shift behaves in case of a matching pair and the two plots on the right show how the different shift behaves in case of a matching pair.}\label{fig:win-comparison}
\end{figure}

\paragraph{Different Shift:}\label{different-shift}

Primary reason for this substep is to give perspective to the
correlation values of the same shift window correlation values.

This time there are no rigid-shifts but different shifts (distance from
Window of Opt with max correlation) chosen randomly by \citet{chumbley}
and deterministically by Hadler et al. such that there is an equal
possibilty of comparing a trace segment from one signature or toolmark
to any one in the second signature or toolamrk.

Neither of above sets of correlation are allowed to include the maximum
correlation window as identified earlier.

Therefore the assumption is that if two toolmarks or signatures match
each other the same-shift correlations would be larger than the
different-shift windows, and if they are not a match the correlations in
the two sets will be very similar.

\subsubsection{U Statistic:}\label{u-statistic}

This is computed from the joint rank of all correlations of both the
same and different shift samples. As given by \citet{hadler}

Null Hypothesis: If the toolmarks were not match i.e not made by the
same tool.

Let ns and nd be the number of same shift and different shift windows
\[N = n_{s} + n_{d}\]

The mann whitney U statistic is given by
\[U =\sum^{ns}_{i=1}R_{s}\left( i\right)\]

with the standardized version which includes provision for rank ties

\[\overline{U}= \dfrac{U-M}{\sqrt{V}}\]

where prior to normalization the U-statistic has the mean as

\[M = n_{s}\left(\dfrac{N+1}{2}\right)\]

and variance

\[V = \dfrac{n_{s}n_{d}}{N\left(N-1\right)}\left[\Sigma^{n_{s}}R_{s}\left(i\right)^{2}+\Sigma^{n_{d}}R_{d}\left(j\right)^{2}\right] -\dfrac{n_{s} n_{d}\left(N+1\right)^{2}}{4\left(N-1\right)}\]

\subsection{Testing setup}\label{testing-setup}

Following on similar lines to the setup of toolmarks, the first step
here is to first identify what difference does different window sizes of
optimization and the validation step have, when adapting the toolmark
method to bullets.

The marking made on bullets are smaller than toolmarks and is also less
wider. The idea is to find out possible areas of error while adapting
the score based method proposed for toolmarks, using cross-validation
setup to identify appropriate parameter settings for (a) signatures and
(b) profiles directly

\subsubsection{Signatures}\label{signatures}

Signatures of lands for all Hamby-44 and Hamby-252 scans made available
through the NIST ballistics database \citep{nist} were considered. Both
of these sets of scans are part of the larger Hamby study \citep{hamby}
and each consist of twenty known bullets (two each from ten
consecutively rifled Ruger P85 barrels) and fifteen questioned bullets
(each matching one of the ten barrels). Ground truth for both of these
Hamby sets is known and was used to assess correctness of the tests
results.

Bullet signatures being compared at this time are therefore from the
Hamby 44 and Hamby 252 data. The database setup and pre-processing
system used for choosing the Bullet signatures are as described by
\citet{aoas}. In order to choose the bullet signatures we first filter
out Land\_id for Profiles from the Hamby 44 and Hamby 252 data and
remove all NA values. Then run\_id = 3 is chosen as the signatures
generated from this run\_id give the closest match. Different run\_id's
have some different settings for generating the signatures.(The level of
smoothing does not seem to be one of them)

The bullet signatures when generated by this process already includes a
loess smoothing. Therefore, the coarseness factor is set to 1 while
running the chumbley non random algorithm for comparing different
optimization windows.The algorithm generates the same\_shift,
different\_shift, U-Stat and P\_value parameters which are then used to
calculate the errors associated with different sets of window sizes.

\subsubsection{Profiles}\label{profiles}

The profiles are cross-sectional values of the the bullet striation mark
which are chosen at an optimum height (x as used by \citet{aoas}). This
x or height is not a randomly chosen level. The rationale behind the
choice has been explained by \citet{aoas}. A region is first chosen
where the cross-correlation seems to change very less and in this region
an optimum height is chosen. The profiles generally resemble a curve
which is more or less similar to a quadratic curve (a quadratic fit to
the raw data values of the profile is not an exact fit but it does show
a similar trend). Profiles are the set of raw values representing the
striation marks, and signatures are generated from these by removal of
the inherent curvature and applying some smoothing (the signatures
generated by \citet{aoas} use a loess function for smoothing).

Similar to signatures the run\_id = 3 was used when applying the
chumbley algorithm using the database setup given by \citet{aoas} of
Hamby-44 and Hamby-252 datasets, on the profiles. The run\_id not only
defines the level of smoothing but also signifies the chosen height at
which the profiles were selected initially. Another important aspect is
the range of horizontal values (which is referred to as the y values in
\citet{aoas}) in the signatures. These have already been pre-processed
in the database to not include any grooves.

Therefore for the sake of comparison the run\_id = 3 is still chosen so
as to ensure that the horizontal values remain the same as that of the
signatures. This also gives us profiles with the grooves removed.

The idea therefore is to first use these raw values of the profile
directly in the chumbley algorithm, and see how the algorithm performs
for different coarseness values (smoothing parameter as referred in the
function lowess used in the chumbley algorithm).

\pagebreak

\section{Results}\label{results}

We used the adjusted Chumbley method as proposed in \citet{hadler} and
implemented in the R package \texttt{toolmaRk} \citep{toolmark} on all
pairwise land-to-land comparisons of the Hamby scans (a total of 85,491
comparisons) with the pairwise sets for the comparisons given in the
table \ref{tab:param}.

\begin{table}[!h][!h]
\caption{\label{tab:param}Overview of parameter settings used for optimization and validation windows for bullet land signatures.}

\centering
\resizebox{\linewidth}{!}{\resizebox{\linewidth}{!}{\begin{tabular}[t]{lrrrrrrrrrrrrrrrrrr}
\toprule
wo & 50 & 50 & 60 & 60 & 80 & 80 & 90 & 90 & 100 & 100 & 110 & 110 & 120 & 120 & 120 & 120 & 120 & 120\\
wv & 30 & 50 & 30 & 50 & 30 & 50 & 30 & 50 & 30 & 50 & 30 & 50 & 10 & 20 & 30 & 40 & 50 & 60\\
\bottomrule
\end{tabular}}}
\centering
\begin{tabular}[t]{lrrrrrrrrrrrrrrr}
\toprule
wo & 130 & 130 & 140 & 140 & 150 & 150 & 160 & 160 & 200 & 200 & 200 & 240 & 240 & 280 & 280\\
wv & 30 & 50 & 30 & 50 & 30 & 50 & 30 & 50 & 20 & 30 & 50 & 30 & 50 & 30 & 50\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!h]
\caption{\label{tab:confusion}Confusion Table for different optimization window sizes with validation window size as 30.}

\centering
\begin{tabular}[t]{llrl}
\toprule
\hspace{1em}\hspace{1em}signif & match & Freq & Type\\
\midrule
\addlinespace[0.3em]
\multicolumn{4}{l}{\textbf{Size of Optimization Window = 280}}\\
\hspace{1em}FALSE & FALSE & 78909 & True Negative\\
\hspace{1em}TRUE & FALSE & 4192 & False Positive (Type I)\\
\hspace{1em}FALSE & TRUE & 446 & False Negative (Type II)\\
\hspace{1em}TRUE & TRUE & 731 & True Positive\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{llrl}
\toprule
signif & match & Freq & Type\\
\midrule
\addlinespace[0.3em]
\multicolumn{4}{l}{\textbf{Size of Optimization Window = 120}}\\
\hspace{1em}FALSE & FALSE & 79249 & True Negative\\
\hspace{1em}TRUE & FALSE & 4523 & False Positive (Type I)\\
\hspace{1em}FALSE & TRUE & 386 & False Negative (Type II)\\
\hspace{1em}TRUE & TRUE & 854 & True Positive\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{llrl}
\toprule
signif & match & Freq & Type\\
\midrule
\addlinespace[0.3em]
\multicolumn{4}{l}{\textbf{Size of Optimization Window = 80}}\\
\hspace{1em}FALSE & FALSE & 79477 & True Negative\\
\hspace{1em}TRUE & FALSE & 4503 & False Positive (Type I)\\
\hspace{1em}FALSE & TRUE & 463 & False Negative (Type II)\\
\hspace{1em}TRUE & TRUE & 781 & True Positive\\
\bottomrule
\end{tabular}
\end{table}

\subsection{Signatures}\label{signatures-1}

Figure \ref{fig:type2} gives an overview of type II error rates observed
when varying the window size in the optimization step. Two levels of
validation window size 30 and 50 were chosen as to compare the error
rates for different nominal type I errors. We notice that the trends for
these nominal type I errors are similar and in most cases a validation
window of 50 has higher type II error than for 30. A change in this
trend is seen for a 0.05 \(\alpha\) level, although the difference
between the two windows is very small for this case. We can also notice
an obvious trend of increase in the Type II error as the window of
optimization increases and see a minimum around the optimization window
size of 120 pixels. Hence we are inclined to choose a smaller validation
window size and optimization window as 120.

Table \ref{tab:confusion} shows the confusion tables with the
classification of type I and type II errors and how the numbers change
with a change in the optimization window. The windows represent areas to
the left of the window with minimum type II, near the minimum type II
window and to the far right of the minimum type II error

\begin{figure}

{\centering \includegraphics[width=\textwidth]{figures/type2-1} 

}

\caption{Type II error rates observed across a range of window sizes for optimization $wo$. For a window size of $wo = 120$ we see a drop in type II error rate across all type I rates considered. Smaller validation sizes $wv$ are typically associated with a smaller type II error.}\label{fig:type2}
\end{figure}

Figure \ref{fig:type1} compares nominal (fixed) type I error and
actually observed type I errors for the parameter settings in table
\ref{tab:param}. With an increasing size of the window used in the
optimization step the observed type I error rate decreases (slighty).
This means as the optimization window increase the observed type I error
rate gets smaller. A smaller validation window on the other hand, tends
to be associated with a higher type I error rate. This can be better
imagined for a given window of optimization, where the actual Type I
error is comparable to the nominal level for only a select few
validation window sizes. For these comparable validation window sizes of
30 and 50 as done here, the actual type I error increases very slightly
and can be seen in Figure \ref{fig:type1}. This increase is not as much
when compared to the variation seen with the optimization window sizes.
This effect might be related to the increasing number of tests that fail
for larger optimization window sizes, in particular for non-matching
striae (see fig \ref{fig:missings}).

\begin{figure}

{\centering \includegraphics[width=\textwidth]{figures/type1-1} 

}

\caption{Comparison of observed and nominal type I error rates  across a range of window sizes for optimization $wo$. The horizontal line in each facet indicates the nominal type I error rate.}\label{fig:type1}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{figures/wo120-1} 

}

\caption{The figure on the left shows the actual Type I error while the figure on the right shows the Type II error for different validation window sizes and different chosen nominal alpha levels when the size of the optimization window = 120}\label{fig:wo120}
\end{figure}

The actual type I error and type II error for signatures were also
compared for different validation window sizes. Figure \ref{fig:wo120}
shows the actual rates for different nominal \(\alpha\) levels. We can
see that the type II error rises with higher validation windows for the
smaller nominal \(\alpha\) levels while for the nominal \(\alpha\) =
0.05 its almost constant.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{figures/missings-1} 

}

\caption{Number of failed tests by the window optimization size, wo, and ground truth.}\label{fig:missings}
\end{figure}

\begin{table}

\caption{\label{tab:lms}Estimates of the increase in percent of failed tests corresponding to a 100 point increase in the optimization window.}
\centering
\begin{tabular}[t]{lrrr}
\toprule
match & wv & estimate & std.error\\
\midrule
FALSE & 30 & 0.452 & 0.027\\
FALSE & 50 & 0.922 & 0.035\\
TRUE & 30 & 0.037 & 0.004\\
TRUE & 50 & 0.056 & 0.005\\
\bottomrule
\end{tabular}
\end{table}

Figure \ref{fig:missings} gives an overview of the number of failed
tests, i.e.~tests in which a particular parameter setting did not return
a valid result. This happens, when the shift to align two signatures is
so large, that the remaining overlap is too small to accommodate windows
for validation. The problem is therefore exacerbated by a larger
validation window. Figure \ref{fig:missings} also shows that the number
of failed tests is approximately linear in the size of the optimization
window. Test results from different sources have a much higher chance to
fail, raising the question, whether failed tests should be treated as
rejections of the null hypothesis of same source. For known non-matches
there is a higher possibility that in the optimization pair of windows
where cross-correlations are maximum are too far apart, and same shifts
of this order hit the end of the signature.

\subsection{Profiles}\label{profiles-1}

Figure \ref{fig:prof_missings} (a) shows the type II error rates for
profiles for the optimization window 120 and validation window 30 with
varying level of coarseness. We can see that the type II error for all
the nominal \(\alpha\) levels is lowest in the range of 0.20 to 0.35.
Therefore, a value of 0.25 can be used keeping in mind it keeps the type
II error lowest while runnning simulations. Thus for comparisons of
different window sizes etc as seen in the differnt parts of Figure
\ref{fig:prof_missings} this coarseness value is used.

On the other hand Figure \ref{fig:prof_missings} (b) shows if the
coarseness level set in the chumbley agorithm has any effect on the
signatures, which are pre-processed and already smoothed to a certain
extent. From Figure \ref{fig:prof_missings} (b) we can notice that for
different nominal \(\alpha\) levels, the type II error fluctuates
slightly but does not change much, thereby helping us conclude that the
coarseness levels set in the LOWESS smoothing in the chumbley alggorithm
does effect the type II error much for signatures.

\subsubsection{Comparison of profiles and
signatures}\label{comparison-of-profiles-and-signatures}

Another reason for failed tests can be incorrect identification of
maximum correlation windows in the optimization step as seen in figure
\ref{fig:prof_missings}(d) because of the level of smoothing, as too
much smoothing would subdue intricate features that might otherwise help
in the correlation calculations and correct identification of maximum
correlation windows irrespective of the size. This would again cause a
simiar effect as explained for figure \ref{fig:missings} with validation
windows, irrespective of size, during the shifts end up at the ends of
the markings resulting in an invalid calculation and failed comparison
attempt.

In figure \ref{fig:prof_missings}(d) and (f), we compare profiles and
signatures on the basis of number of failed tests. The profiles chosen
for figure \ref{fig:prof_missings}(f) have a constant coarseness of 0.25
and window of optimization as 120. The signatures in this case are not
smoothed using the chumbley algorithm step of LOWESS smoothing. Instead
signatures are used as calculated by \citet{aoas}. The smoothing in
these signatures were determined and fixed on the basis of their
performance in the random forest based algorithm proposed by
\citet{aoas}. The comparison of profiles and signatures with variation
of validation window size therefore is made on even footing. The trends
are similar to figure \ref{fig:missings} in the sense that for known
non-matches the number of failed tests are more for both signatures and
profiles and increasing linearly with the validation window size. The
problem is however, worse for profiles which has higher number of failed
tests than signatures for all validation windows.

The total error for different validation window sizes for signatures and
profiles can be seen in figure \ref{fig:prof_missings} (e).The
optimization window size is 120 and profiles are calculated at a default
0.25 coarseness level while signatures as before are not smoothed again
in the modified chumbley algorithm. We can see that the total error is
always higher for profiles as compared to signatures for all sizes of
validation window.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{figures/prof_missings-1} 

}

\caption{Row 3:  Total error and Number of failed tests by the window validation size, wv, and ground truth, Row 2: Total error and Number of failed tests with Coarseness for both profiles and signatures, Row 1: Type II error for different coarseness levels as used in the modified chumbley algorithm for profiles and signatures}\label{fig:prof_missings}
\end{figure}

\pagebreak

\subsection{Conclusion}\label{conclusion}

The results suggest that the Nominal type I error \(\alpha\) value shows
dependence on the size of the window of optimization. For a given window
of optimization the actual Type I error is comparable to the nominal
level for only a select few validation window sizes and for comparable
validation window sizes of 30 and 50 as done here, the actual type I
error does not seem to vary as much as it varies with the optimization
window sizes . A Test Fail, i.e.~tests in which a particular parameter
setting did not return a valid result, happens, when the shift to align
two signatures is so large, that the remaining overlap is too small to
accommodate windows for validation, depends on whether known-match or
known non-matches has predictive value, with test results from different
sources having a much higher chance to fail. On conducting an analysis
of all known bullet lands using the adjusted chumbley algorithm, Type II
error was identified to be least bad for window of validation 30 and
window of optimization 120. In case of unsmoothed raw marks (profiles),
Type II error increases with the amount of smoothing and least for
LOWESS smoothing coarseness value about 0.25 or 0.3. In an effort to
identify the level of adaptiveness of the algorithm, comparisons were
made between signatures and profiles. Their comparison with respect to
validation window size for a fixed optimization window size suggested
that, profiles have a total error (i.e all incorrect classification of
known-matches and known non-matches) greater than or equal to the total
error of signatures for all sizes of validation window. Profiles also
fail more number of times than signatures in a test fail (for different
coarseness keeping windows fixed and also for different validation
windows keeping coarseness fixed) which lets us conclude that the
behaviour of the algorithm for the profiles instead of pre-processed
signatures is not better. Finally it should be noted that the current
version of the adjusted chumbley algorithm seems to fall short when
compared to other machine-learning based methods \citet{aoas}, and some
level of modification to the deterministic algorithm needs to be
identified and tested that would reduce the number of incorrect
classifications.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{figures/wo120-profile-1} 

}

\caption{The figure on the left shows the actual Type I error while the figure on the right shows the Type II error for different validation window sizes and different chosen nominal alpha levels when the size of the optimization window = 120}\label{fig:wo120-profile}
\end{figure}\begin{figure}

{\centering \includegraphics[width=\textwidth]{figures/failtest-coarseness-1} 

}

\caption{The figure on the left shows the actual Type I error while the figure on the right shows the Type II error for different validation window sizes and different chosen nominal alpha levels when the size of the optimization window = 120}\label{fig:failtest-coarseness}
\end{figure}\begin{figure}

{\centering \includegraphics[width=\textwidth]{figures/failtest-wv-1} 

}

\caption{The figure on the left shows the actual Type I error while the figure on the right shows the Type II error for different validation window sizes and different chosen nominal alpha levels when the size of the optimization window = 120}\label{fig:failtest-wv}
\end{figure}\begin{figure}

{\centering \includegraphics[width=\textwidth]{figures/failtest-wo-1} 

}

\caption{The figure on the left shows the actual Type I error while the figure on the right shows the Type II error for different validation window sizes and different chosen nominal alpha levels when the size of the optimization window = 120}\label{fig:failtest-wo}
\end{figure}

\bibliographystyle{agsm}
\bibliography{bibliography}

\end{document}
