---
title: "Adapting the Chumbley Score to Bullet Striations"
author: "Ganesh Krishnan, Heike Hofmann"
date: "April 21, 2018"
header-includes:
   - \usepackage{cleveref}
   - \usepackage{amsmath}
   - \usepackage{amsfonts}
output: 
  beamer_presentation:
    citation_package: natbib
bibliography: bibliography.bib
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
library(knitr)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  comment = "",
  fig.height = 6,
  fig.width = 6,
  fig.align = "center",
  out.width= '\\textwidth',
  cache = FALSE,
  fig.path='figures/',
  echo=FALSE,
  cache=TRUE
)
options(knitr.table.format = "latex")
library(tidyverse)
library(kableExtra)
library(xtable)
library(gridExtra)
```

```{r functions}
errorrate <- function(data, alpha) {
  summ <- data %>% filter(!is.na(p_value)) %>%
    mutate(signif = p_value < alpha) %>%
    group_by(wv, wo, match, coarse, signif) %>% tally()
  summ$error <- with(summ, match != signif)
  summ$alpha <- alpha
  
  rates <- summ %>% dplyr::group_by(wv, wo, coarse, match, alpha) %>% dplyr::summarize(
    rate = n[error==TRUE]/sum(n)
  )
  totals <- summ %>% dplyr::group_by(wv, wo, coarse, alpha) %>% dplyr::summarize(
    total= sum(n[error==TRUE])/sum(n)
  )
  rates <- rates %>% spread(match,rate) %>% dplyr::rename(
    actual = `FALSE`,
    beta = `TRUE`
  )
  left_join(rates, totals)
}


# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
#  ref: http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
```

```{r data}
if (!file.exists("../data/all-sigs.rds")) {
  files <- dir("../data/signatures", pattern="csv")
  all <- data.frame()
  for (file in files) {
    tmp <- read.csv(file=file.path(path="../data/signatures", file)) 
    if (length(grep("x",names(tmp)) > 0)) tmp <- tmp %>% select(-x)
    all <- rbind(all, tmp)
  }
  all <- all %>% filter(land1_id < land2_id)
  saveRDS(all, file="../data/all-sigs.rds")
} else {
  all <- readRDS("../data/all-sigs.rds")
}

all$coarse <- 1
errors <- rbind(errorrate(all, 0.001), 
                errorrate(all, 0.005), 
                errorrate(all, 0.01), 
                errorrate(all, 0.05))
errors$type <- "signatures"

ns <- all %>% group_by(wv, wo, match) %>% summarize(
  n_ss = mean(same_shift_n),
  n_ds = mean(diff_shift_n),
  zero_ss = sum(same_shift_n==0),
  zero_ds = sum(diff_shift_n==0),
  miss = sum(is.na(p_value)),
  missperc = miss/n()*100
)
```

```{r data-profiles}
if (!file.exists("../data/all-profiles.rds")) {
  files <- dir("../data/profiles/", pattern="csv")
  allp <- data.frame()
  for (file in files) {
    tmp <- read.csv(file=file.path(path="../data/profiles", file)) 
    #print(names(tmp))
    if (length(grep("coarse",names(tmp))) == 0)
      tmp$coarse <- gsub(".*coar-(.*).csv", "\\1", file)
    if (length(grep("x",names(tmp)) > 0)) tmp <- tmp %>% select(-x)
    allp <- rbind(allp, tmp)
  }
  allp <- allp %>% filter(land1_id < land2_id)
  allp <- allp %>% mutate(coarse = as.numeric(gsub("pt", ".", coarse)))
  saveRDS(allp, file="../data/all-profiles.rds")
} else {
  allp <- readRDS("../data/all-profiles.rds")
}

errors2 <- rbind(errorrate(allp, 0.001), 
                errorrate(allp, 0.005), 
                errorrate(allp, 0.01), 
                errorrate(allp, 0.05))
errors2$type <- "CS1"


if (!file.exists("../data/invprofiles.rds")) {
  files <- dir("../data/inverse-profiles/", pattern="rds")
  allinv <- data.frame()
  for (file in files) {
    tmp <- readRDS(file=file.path(path="../data/inverse-profiles", file)) 
    allinv <- rbind(allinv, tmp)
  }
  allinv <- allinv %>% filter(land1_id < land2_id)
  allinv <- allinv %>% select(-coarseness)
  saveRDS(allinv, file="../data/invprofiles.rds")
} else {
  allinv <- readRDS("../data/invprofiles.rds")
}

errors3 <- rbind(errorrate(allinv, 0.001), 
                errorrate(allinv, 0.005), 
                errorrate(allinv, 0.01), 
                errorrate(allinv, 0.05))
errors3$type <- "CS2"


```

```{r data-sig-coars}
if (!file.exists("../data/all-sig-c.rds")) {
  files <- dir("../data/sig_diff_coarseness/", pattern="csv")
  all_sig_c <- data.frame()
  for (file in files) {
    tmp <- read.csv(file=file.path(path="../data/sig_diff_coarseness/", file)) 
    #print(names(tmp))
    if (length(grep("coarse",names(tmp))) == 0)
      tmp$coarse <- gsub(".*coarse-(.*).csv", "\\1", file)
    if (length(grep("x",names(tmp)) > 0)) tmp <- tmp %>% select(-x)
    all_sig_c <- rbind(all_sig_c, tmp)
  }
  all_sig_c <- all_sig_c %>% filter(land1_id < land2_id)
  all_sig_c <- all_sig_c %>% mutate(coarse = as.numeric(gsub("pt", ".", coarse)))
  saveRDS(all_sig_c, file="../data/all-sig-c.rds")
} else {
  all_sig_c <- readRDS("../data/all-sig-c.rds")
}

errors_sig_c <- rbind(errorrate(all_sig_c, 0.001), 
                errorrate(all_sig_c, 0.005), 
                errorrate(all_sig_c, 0.01), 
                errorrate(all_sig_c, 0.05))


```

## Objective and Motivation

- Same Source Matching of Bullet lands
- Evaluate performance of Chumbley Score method when used for Bullet Striations
- Bullet striations have curvature, not present in toolmarks
- Identify Error rates and effect of different parameters on them (In short finding the best error rates possible )


### Structure
- Data being used
- What is the Chumbley Score Method?
- Identifying Best parameter Settings for Bullets
- Modifications to the Algorithm
- Results using \citet{hadler} method and results using Modified method





## Variations of Chumbley score method and Error Rates for toolmarks

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{rlrrr}
  \hline
Research paper & Method & Data Source & False Positives & False Negatives\\
  \hline
 \textbf{\citet{manytoolmarks1}} & Maximized & Screwdrivers & & \\ & Correlation &  & - & - \\ \hline
 \textbf{\citet{chumbley}} & Randomized & Screwdrivers & & \\ (Same-Surface Same-Angle) & Chumbley Score & & 2.3\% & 8.9\%  \\ \hline
  \textbf{\citet{afte-chumbley}} & Randomized & Slip-joint  &  & \\ & Chumbley Score &  & - & - \\ \hline 
  \textbf{\citet{hadler}} & Deterministic & Screwdrivers & & \\ (Same-Surface Same-Angle) & Chumbley Score & & 0\% & 6\%  \\ 
   \hline
\end{tabular}}
\caption{\label{tab:toolmarks-ER} Error Rates for Toolmarks using  variations of the chumbley score method}
\end{table}

 <!-- \textbf{\citet{manytoolmarks2}} & Similarity Measure & Screwdrivers & & \\ (Different Surfaces-same angle) & Relative Distance Metric & & 5.9\% &  9.4\% \\ (Same Surfaces-same angle)&  & & 0.22\% &  0\% \\ -->


## Digitized Striation Marks
- Data
    -  Ruger P85s Bullet Lands, or Hamby scans (\cite{hamby}) provided by NIST  (85,491 comparisons)
    -  Bullet striation marks $\approx$ 2mm 
    -  Screwdriver marks $\approx$ 7mm (all chumbley score papers)

- Let $x(t_1)$, $t_1 = 1,2,...T_1$ and $y(t_2)$, $t_2 = 1,2...T_2$ be two digitized marks (where $T_1$ and $T_2$ are not necessarily equal). 

- $T_1$ and $T_2$ are the final pixel indexes of each marking. 
Therefore give the respective lengths of the markings. 

- Signatures/ Profiles (NIST- Hamby) $\approx$ 1200 pixels (2 mm)
  Screwdriver toolmarks (Chumbley Papers) $\approx$ 9000 pixels (7 mm)


## Chumbley Score

#### Step 0 : Defining a coarseness parameter
- Used to remove drift and (sub)class characteristics from individual markings
<!-- ### Getting from Profiles to Signatures -->
<!-- \vspace{-0.5em} -->
- Lowess or Loess fit residuals $=$ Signatures
- Removes topographic structure (curvature)
- Improve the signal to noise Ratio

\vspace{-1.5em}

```{r sigs-profiles, fig.width = 8, fig.height = 2.5, fig.cap="Bullet land profile (left) and the corresponding signature (right)."}
sigs_graphics<- readRDS("../data/sigs_generate_window.rds")

sigs_graphics.match<- as.data.frame(sigs_graphics[1])
sigs_graphics.nonmatch<- as.data.frame(sigs_graphics[2])
colnames(sigs_graphics.match)<- gsub("^.*\\.","", colnames(sigs_graphics.match))
colnames(sigs_graphics.nonmatch)<- gsub("^.*\\.","", colnames(sigs_graphics.nonmatch))
p1 <- sigs_graphics[[1]] %>% filter(land_id==19) %>%
  ggplot(aes(x = y, y = value)) + geom_line() +
  theme_bw() +
  ylab(expression(paste("Depth (in ",mu,m,")", sep=""))) + 
  xlab(expression(paste("Relative Location (in ",mu,m,")", sep=""))) + 
  ggtitle("Profile")

p2 <- sigs_graphics[[1]] %>% filter(land_id==19) %>%
  ggplot(aes(x = y, y = l30)) + 
  geom_line(aes(y=resid), size=.25) +
  geom_line(aes(y=l30)) +
  theme_bw() +
  ylab(expression(paste("Depth (in ",mu,m,")", sep=""))) + 
  xlab(expression(paste("Relative Location (in ",mu,m,")", sep=""))) + 
  ggtitle("Signature")

grid.arrange(p1,p2, ncol=2)
```

## Algorithm

- Two steps: Optimization ($1^{st}$) and Validation ($2^{nd}$).
- Windows $\implies$ short segments of the markings
    - Have \textit{predefined sizes}. ($T_1$ or $T_2$ >>> $w_o$ & w_v$)
          1. $w_o$ used in the Optimization step 
          2. $w_v$ used in the Validation step  

\vspace{-1em}  

#### Optimization step
\vspace{-0.5em}

- \textbf{Goal :}Align markings horizontally as best as possible
- Correlation Matrix of all possible windows of size $w_o$ between $x(t_1)$ and $y(t_2)$ computed
- \textit{Identify lag for horizontal alignment} \newline Window Pair with maximized correlation $\implies$ \newline Optimal vertical (in-phase) shift of $t_1^o - t_2^o$ 
      - For aligning the two markings. 
\vspace{-1em}
\[
\left(t_1^o, t_2^o\right) = \mathop{\arg \max}\limits_{1 \le t_1 \le T_1, 1 \le t_2 \le T_2} \text{cor} \left(x^{w_o} (t_1), y^{w_o}(t_2) \right)
\] 

\hspace{-1.5em}
where $t_1^o,t_2^0$ are the respective starting points of $w_o$ in $x(t_1)$ and $y(t_2)$



## 2
- Let $t_1^*$ and $t_2^*$ be relative optimal locations, where $t_i^* = t_i^o/(T_i-w_o)$ for $i=1,2$, such that $t_1^*, t_2^* \in [0,1]$. 
-  Once (sub-)class characteristics are removed, these locations have uniform distribution in $[0,1]$

#### Validation Step
- Two sets of windows of size $w_v$ chosen from both markings (see Figure \ref{fig:win-comparison})


- First set or \textbf{Same Shift}
      - pairs of windows are extracted from the two markings using the optimal vertical shift.
$t_1^o - t_2^o$

- Second set or \textbf{Different Shift}
      - the windows are extracted using a different (out-of-phase) shift. 

## In-phase and Out-of-phase

```{r win-comparison, warning=FALSE, fig.width=8, fig.height=3.5, fig.cap="Two markings made by the same source. For convenience, the markings are moved into phase on the left and out-of phase on the right. In-phase (left) and out-of-phase (right) samples are shown by the light grey background. The Chumbley-score is based on a Mann-Whitney U test of the correlations derived from these two sets of samples."}
sigs_graphics <- readRDS("../data/sigs_generate_window.rds")
sigs_graphics$sigs_land_id_1_match <- sigs_graphics$sigs_land_id_1_match %>% group_by(land_id) %>% mutate(y = y-min(y, na.rm=T))

sigs_graphics.match<- as.data.frame(sigs_graphics[1])
sigs_graphics.nonmatch<- as.data.frame(sigs_graphics[2])
colnames(sigs_graphics.match)<- gsub("^.*\\.","", colnames(sigs_graphics.match))
colnames(sigs_graphics.nonmatch)<- gsub("^.*\\.","", colnames(sigs_graphics.nonmatch))

# Matching signature
d1<- sigs_graphics.match[which(sigs_graphics.match$land_id==1,arr.ind = TRUE),]
d2<- sigs_graphics.match[which(sigs_graphics.match$land_id==276,arr.ind = TRUE),]
data1<-d1 %>%dplyr::select(l30)
#data1<- data1$y
data2<- d2 %>%dplyr::select(l30)
#data2<- data2$y
window_opt = 120 
window_val = 50
coarse = 1
data1<- matrix(unlist(data1))
data2<- matrix(unlist(data2))

  unity <- function(x) {x / sqrt(sum(x^2))} ## normalize columns of a matrix to make correlation computation faster
  
  ####################################################
  ##Clean the marks and compute the smooth residuals##
  ####################################################
  
  data1 <- matrix(data1[round((0.01*nrow(data1))):round(0.99*nrow(data1)),], ncol = 1)
  data2 <- matrix(data2[round((0.01*nrow(data2))):round(0.99*nrow(data2)),], ncol = 1)
  
  ##Normalize the tool marks
  y1 <- data1 - lowess(y = data1,  x = 1:nrow(data1), f= coarse)$y
  y2 <- data2 - lowess(y = data2,  x = 1:nrow(data2), f= coarse)$y
  
  ############################################
  ##Compute the observed maximum correlation##
  ############################################
  
  #####################
  ##Optimization step##
  #####################
  ##Each column in these matrices corresponds to a window in the respective tool mark
  y1_mat_opt <- matrix(NA, ncol = length(1:(length(y1) - (window_opt - 1))), nrow = window_opt)
  for(l in 1:(length(y1) - (window_opt - 1))){
    y1_mat_opt[,l] <- y1[l:(l+(window_opt - 1))]
  }
  y2_mat_opt <- matrix(NA, ncol = length(1:(length(y2) - (window_opt - 1))), nrow = window_opt)
  for(l in 1:(length(y2) - (window_opt - 1))){
    y2_mat_opt[,l] <- y2[l:(l+(window_opt - 1))]
  }
  
  ##Compute the correlation between all pairs of windows for the two marks
  ##Rows in the following matrix are mark 2, columns are mark 1
  y2_mat_opt <- apply(scale(y2_mat_opt), 2, unity)
  y1_mat_opt <- apply(scale(y1_mat_opt), 2, unity)
  corr_mat_opt <- t(y2_mat_opt) %*% y1_mat_opt ##correlation matrix
  max_corr_opt_loc <- which(corr_mat_opt == max(corr_mat_opt), arr.ind = TRUE) ##pair of windows maximizing the correlation

sigs1.min = sigs_graphics[[1]] %>% filter(land_id==1) %>%
  dplyr::select(y) %>% min()
sigs2.min = sigs_graphics[[1]] %>% filter(land_id==276) %>% 
  dplyr::select(y) %>% min()
window_val <- 3*window_val*0.645

same.shift <- -(365-338)*1/.645 + sigs2.min - sigs1.min 
xs <- seq(100, 1800, length=10)
rects <- data.frame(ymin=-Inf, ymax= Inf, xmin = xs, xmax = xs+window_val)


p1 <- sigs_graphics[[1]] %>% filter(land_id %in% c(1, 276)) %>% 
  ggplot() + 
  geom_segment(aes(x = xmin, y=ymin, xend = xmin, yend=ymax), 
               colour="grey50", linetype=3, data = rects) +
  geom_segment(aes(x = xmax, y=ymin, xend = xmax, yend=ymax), 
               colour="grey50", linetype=3, data = rects) +
  geom_rect(aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), 
            data=rects, fill= "grey90") +
  geom_line(aes(x = y-same.shift*I(land_id==276), 
                y=l30-5*I(land_id==276), group=land_id)) + 
  theme_bw() +
  theme(axis.title.y = element_blank(), 
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  xlab(expression(paste("Relative Location (in ",mu,m,")", sep=""))) + 
  ggtitle("In-phase sample")

diff.shift <- -(365-338)*1/.645 + sigs2.min - sigs1.min + 70 

p2 <- sigs_graphics[[1]] %>% filter(land_id %in% c(1, 276)) %>% 
  ggplot() + 
  geom_segment(aes(x = xmin, y=ymin, xend = xmin, yend=ymax), 
               colour="grey50", linetype=3, data = rects) +
  geom_segment(aes(x = xmax, y=ymin, xend = xmax, yend=ymax), 
               colour="grey50", linetype=3, data = rects) +
  geom_rect(aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), 
            data=rects, fill= "grey90") +
  geom_line(aes(x = y-diff.shift*I(land_id==276), 
                y=l30-5*I(land_id==276), group=land_id)) + 
  theme_bw() +
  theme(axis.title.y = element_blank(), 
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  xlab(expression(paste("Relative Location (in ",mu,m,")", sep=""))) + 
  ggtitle("Out-of-phase sample")

grid.arrange(p1,p2, ncol=2)
```

## 4
<!-- - No overlaps of pairs within selected marks $\implies$ independence. -->
- Both same- and different-shift pairs correlations between the  markings are calculated. 
- For Same-Source markings, correlations
    - for the in-phase shift should be high
    - for out-of-phase shift should be low.
          - Provide a measure for the base-level correlation to which in-phase shift correlations can be compared.
- The Chumbley score is the Mann Whitney U statistic computed by comparing between in-phase sample and out-of-phase sample.


## Block Diagram

\begin{figure}[hbtp]
\centering
\includegraphics[width=1\textwidth]{imgs/flow-chart-adj-chumbley.png}

\caption{\label{flow-chart} An overview of the adjusted chumbley score method as given by \citet{hadler}}
\end{figure}

## Starting Points

More precisely, let us define starting points of the windows of validation $s_i^{(k)}$ for each marking $k = 1, 2$ as 
\begin{eqnarray}\label{eq.start}
s^{(k)}_i = 
\begin{cases}
t_k^o + i w_v & \text{ for } i < 0 \\
t_k^o + w_o + i w_v & \text{ for } i \ge 0,
\end{cases}
\end{eqnarray} for integer values of $i$ with $0 <  s^{(k)}_i \le T_k - w_v$ where $s \ \in \mathbb{Z}$
<!-- $0 <  s^{(k)}_i \le T_k - w_v$. -->

## The \citet{hadler} method (CS1)

- Same-shift pairs of length $w_v$ are all pairs that start in: $$(s_i^{(1)}, s_i^{(2)}) \quad \forall \, i \, \in \, \mathbb{Z}$$ for which both $s_i^{(1)}$ and  $s_i^{(2)}$ are defined. 
- Different-shift pairs are defined as $$(s_i^{(1)}, s_{-i-1}^{(2)}) \quad \forall \, i \, \in \, \mathbb{Z}$$ where both $s_i^{(1)}$ and  $s_{-i-1}^{(2)}$ are defined (see \cref{sketch-same-diff}).

\begin{figure}[hbtp]
\centering
\includegraphics[width=.7\textwidth]{../doc/images/sketch-same.png}

\includegraphics[width=.7\textwidth]{../doc/images/sketch-diff.png}
\caption{\label{sketch-same-diff}Sketch of same-shift pairings  (top) and different-shift pairings (bottom). Filled in rectangles show pairings resulting in correlations, unfilled rectangles are segments without a match.}
\end{figure}



## Failed Tests

- By definition (equation \autoref{eq.start}), some number of tests fail to produce a result
- Either because the number of eligible same-shift pairs is 0, or the number of different-shift pairs is 0. 
- $t_1^o, t_2^o$ not necessarily independent
    - \textbf{same-source:} Assume high dependence, corr($t_1^o,t_2^o$) $\approx$ 1
        - Example: $w_o$ = 120, coarseness (c) = 0.3, corr($t_1^o,t_2^o$) = 0.85
    - \textbf{diff-source:} Assume independence of $t_1^o,t_2^o$
        - Example: $w_o$ = 120, coarseness (c) = 0.3, corr($t_1^o,t_2^o$) = 0.12
<!-- \vspace{-0.5em} -->

<!-- ###### No Same-shift pairs if -->
<!-- - the optimal locations $t_1^{o}$ and $t_2^{o}$ are in the extremes -->
<!--     - no segments of size $w_v$ are left on the same sides of the optimal locations,  -->
<!--     - For all $t_1^{o} < w_v$ and $t_2^{o} > T_2-w_o-w_v$ or \newline $t_1^{o} < T_1- w_o - w_v$ and $t_2^{o} < w_v$ -->

<!-- \vspace{-0.5em} -->

<!-- ###### No Different Shift Pairs if -->
<!-- - the optimal locations are close to the boundaries -->
<!--     - \# of different-shift pairs also depends on the placement of the  optimal locations $t_i^o$. -->
<!--     -  For all $t_i^{(o)} < w_v$ or  $t_i^{(o)} > T_i-w_o- w_v$. -->

<!-- \vspace{-0.5em} -->

#### Failure Rate
\vspace{-1.5em}

\[
\begin{split}
P\left( t_1^{o} < w_v \ \bigcap \ t_2^{o} > T_2-w_o-w_v\right) + \\P\left( t_1^{o} < T_1- w_o - w_v \ \bigcap \ t_2^{o} < w_v\right).
\end{split}
\]

\vspace{-1.5em}

## Same-shift failure
- Same-source $\approx$ 0
- Different-source $\approx$ 2 $P(t_i < w_o)^2$ = $\frac{2 w_v^2}{(T_1-w_o)(T_2-w_o)}$ 

\begin{figure}[hbtp]
\centering
\includegraphics[width=.7\textwidth]{imgs/same-shift-failure.png}

\caption{\label{same-shift-failure}Sketch of same-shift pairings  (top) when the lag is too large to accomodate a a vaildation window in either of the two signatures}
\end{figure}

## Different-Shift Failure

- Same-source (Assuming $t_1^{o} \approx t_2^{o}$) $\approx$ $2w_v / (T_i-w_o)$\newline

\vspace{-2em}

$P( t_1^{o} < w_v \ \cap \ t_2^{o} < w_v) + P( t_1^{o} <  w_v \ \cap \ t_2^{o} < w_v)  = \ 2P(t_o^1 \ < \ w_v)$

\vspace{-1em}

- Different-source $\approx$  $2P(t_i < w_o)^2$ = $\frac{2 w_v^2}{(T_1-w_o)(T_2-w_o)}$ 

\vspace{-1em}

\begin{figure}[hbtp]
\centering
\includegraphics[width=0.8\textwidth]{imgs/diff-shift-failure.png}

\caption{\label{diff-shift-failure}Sketch of diff-shift pairings  (top) when the number of diff-shift computations is likely to 0}
\end{figure}

<!-- - For different-source matches -->
<!--     - we can assume $t_1^o$ and $t_2^o$ are independent. -->
<!--     - Failure rate = $\frac{2 w_v^2}{(T_1-w_o)(T_2-w_o)}$  -->
<!--     - For an average length of $T_i$ of 1200 pixels, $w_o = 120$ pixels and $w_v = 30$ pixels this probability is about 0.0015. -->
<!-- - For same-source matches,  -->
<!--     - we expect a strong dependency between optimal locations $t_i^o$ -->
<!--     - $t_1^o$ and $t_2^o$ are not independent of each other -->
<!--     - A large difference between locations is unlikely -->
<!--     - Failure Rate = $2 w_v / (T_i-w_o)$, or about 5.6\% for an average length of $T_i$ of 1200 pixels, $w_o = 120$ pixels and $w_v = 30$ pixels. -->

<!-- ##  Failure rate for same-source matches -->
<!-- The failure rate for same-source matches can also be written as follows -->
<!-- $$ -->
<!-- \begin{split} -->
<!-- P\left( t_1^{o} < w_v \ \bigcup \ t_2^{o} > T_2-w_o-w_v\right) + \\P\left( t_1^{o} < T_1- w_o - w_v \ \bigcup \ t_2^{o} < w_v\right) -->
<!-- \end{split} -->
<!-- $$ -->


<!-- Taking first part of the above equation, let $A = t_1^{o} < w_v$ and $B = t_2^{o} > T_2-w_o-w_v$ -->

<!-- - Again we assume high correlation between optimal locations -->
<!-- - In order to have at least one same-shift pair or different shift pair -->
<!--     - \textbf{Either}, if $x \in A$, then $B$ is empty and $P(A \cup B) = P (A)$ -->
<!--     - \textbf{or}, if $x \in B$, then $A$ is empty and $P(A \cup B) = P(B)$ -->

<!-- This would hold for the second part too, and would therefore give an expected rate of failure as $2 w_v / (T_i-w_o)$ -->


## Proposed Modification

- Failures due to missing Same-shift pairs unavoidable
- Failures due to missing different-shift pairs preventable

Define same-shift pairs identical to \citet{hadler} as pairs 
$$(s_i^{(1)}, s_i^{(2)}) \quad \forall \, i \, \in \, \mathbb{Z}$$ where the boundary conditions of both sequences are met simultaneously. \newline

- Let us assume that this results in $I$ pairs. \newline
- Let $s_{(j)}^{(k)}$ to be the $j$th starting location in sequence $k = 1, 2$, i.e.\ $s_{(1)}^{(k)} < s_{(2)}^{(k)} < ... < s_{(I)}^{(k)}$.  

## 

We then define the pairs for different-shifts as
\begin{equation}\label{eq.diff2}
\left(s_{(j)}^{(1)}, s_{(I-j+1)}^{(1)} \right) \text{ for } j = 
\begin{cases}
1, ..., I & \text{ for even } I \\
1, ..., (I-1)/2, (I-1)/2 + 2, ..., I & \text{ for odd } I
\end{cases},
\end{equation}

- For an odd number of same-shift correlations
    - We skip the middle pair for the different-shift correlations (see \cref{sketch-diff-2}).
- This pairing ensures that the number of different-shift pairings is the same or at most one less than the number of same-shift pairings in all tests.

## 10

<!-- Algorithm defined by \citet{hadler} \textbf{(CS1)} and the suggested modified algorithm as \textbf{(CS2)} -->


\begin{figure}[hbtp]
\centering
\includegraphics[width=.7\textwidth]{../doc/images/sketch-diff-2.png}
\caption{\label{sketch-diff-2}Sketch of adjusted different-shift pairings. At most one of the same-shift pairings can not be matched with a different-shift pair. }
\end{figure}

## Case where CS1 fails but CS2 does not fail

\small{\textbf{CS1} \citet{hadler}} algorithm \newline  \small{\textbf{CS2} the suggested modified algorithm}
\begin{figure}[hbtp]
\centering
\includegraphics[width=.7\textwidth]{imgs/CS1-failure-CS2-success.png}
\caption{\label{CS1-failure-CS2-success}Sketch of a case where CS1 fails but CS2 does not fail}
\end{figure}


## Results Failed Tests

## Conclusions Failed Tests
\fontsize{10pt}{11}\selectfont

- Number is highly dependent on the comparison window sizes
- Correlated to the ground truth, 
- \textbf{Higher for known same-sourced lands (CS1)} than for known different sourced lands. 
- \textbf{CS1} fails to conduct a test about 8 to 13 % of the time for \textbf{known same-source lands}, and 2 to 6% of the time for \textbf{known different source lands}.
- Number for CS1 always higher than the corresponding theoretical number of failed tests.
- Using CS2, the case with \textbf{largest} # of failed tests is still \textbf{lower} than the case where CS1 gives the \textbf{lowest} # of failed tests
- Even for high coarseness, CS2 will have lower number of failed tests than CS1, Making it more robust.
- CS2 performs better for \textbf{both} same and different-sources
- Solves a \textbf{critical issue} of CS1 known same-source matching, by having a \textbf{negligible number of Known same-source failed tests}
 
## Coarseness

- Remove (sub-)class characteristics from profiles before comparisons for matching.
- \citet{hadler} suggest a coarseness parameter of 0.25 for toolmark comparisons. 
- For bullet lands, coarseness might need to be adjusted because of the strong effect bullet curvature has on profiles.
- Optimal locations are distributed uniformly once (sub-)class characteristics are removed. 
- Distinct boundary effects: c > 0.20 optimal locations $t^*$ are found at the very extreme ends of a profile more often than one would expect based on a uniform distribution.
-smaller coarseness value of $c = 0.15$ to be suitable

##

```{r profile-sketch, fig.height = 4, fig.width='\\textwidth', fig.cap="Overview of the effect of different coarseness parameters $c$ on the profile shown in Figure \\ref{fig:sigs-profiles} (top). The bottom row shows histograms of  the (relative) optimal locations $t^o$ identified in the optimization step for different values of the coarseness parameter $c$. "}
profile <- sigs_graphics[[1]] %>% filter(land_id==19)
profile$lw10 <- lowess(x = profile$y, y = profile$value, f=.10)$y
profile$lw15 <- lowess(x = profile$y, y = profile$value, f=.15)$y
profile$lw20 <- lowess(x = profile$y, y = profile$value, f=.20)$y
profile$lw25 <- lowess(x = profile$y, y = profile$value, f=.25)$y
profile$lw30 <- lowess(x = profile$y, y = profile$value, f=.30)$y
#profile$lw50 <- lowess(x = profile$y, y = profile$value, f=.5)$y
#profile$lw75 <- lowess(x = profile$y, y = profile$value, f=.75)$y
#profile$lw100 <- lowess(x = profile$y, y = profile$value, f=1)$y

profiles <- tidyr::gather(profile, coarseness, values, lw10:lw30)
profiles <- profiles %>% mutate(
  c = readr::parse_number(coarseness)/100
)
p1 <- profiles %>% 
  ggplot(aes(x = y, y = value-values)) + geom_line() +
  theme_bw() +
  ylab(expression(paste("Depth (in ",mu,m,")", sep=""))) + 
  xlab(expression(paste("Relative Location (in ",mu,m,")", sep=""))) +
  facet_grid(~c, labeller="label_both") 

profc10 <- readRDS("../data/profiles-coarseness/chumbley-csafe-profiles-inv-wo_160-wv_30-c_10.rds")
profc15 <- readRDS("../data/profiles-coarseness/chumbley-csafe-profiles-inv-wo_160-wv_30-c_15.rds")
profc20 <- readRDS("../data/profiles-coarseness/chumbley-csafe-profiles-inv-wo_160-wv_30-c_20.rds")
profc20$coarseness <- 0.2
profc25 <- readRDS("../data/profiles-coarseness/chumbley-csafe-profiles-inv-wo_160-wv_30-c_25.rds")
profc30 <- readRDS("../data/profiles-coarseness/chumbley-csafe-profiles-inv-wo_160-wv_30.rds")
profc30$coarseness <- .30
profc <- bind_rows(profc10, profc15, profc20, profc25, profc30)
profc <- profc %>% mutate(
  locations = chumbley %>% purrr::map_int(.f = function(x) {
    res <- NULL
    try(res <- x$locations[1], silent = TRUE)
    res
  })
)
profsumm <- read.csv("../data/profiles-summary.csv")
profsumm <- profsumm %>% dplyr::select(-run_id)
profc <- profc %>% left_join(profsumm, by=c("land2_id"="land_id", "profile2_id"="profile_id"))


p2 <- profc %>% 
  ggplot(aes(x = locations/(length-wo))) +
  geom_histogram(binwidth = 0.025) +
  facet_grid(.~coarseness, labeller="label_both") + 
  theme_bw() +
  xlab(expression("Relative optimal location "~t^"*")) +
  ylab("Number of profiles") +
  scale_x_continuous(breaks=seq(0,1, by=0.25), labels=c("0", "0.25", "0.50", "0.75", "1"))
 
grid.arrange(p1, p2, ncol=1)
```
## Type II error rates

<!-- \autoref{fig:type2} gives an overview of the type 2 error of methods CS1 and CS2 across a range of different optimization windows $w_o$. -->
```{r type2, fig.height = 8, fig.width = 8.5, out.width='0.6\\textwidth', fig.cap="Type II error rates observed across a range of window sizes for optimization $w_o$. For a window size of $w_o = 130$ we see a minimum in type II error rate across all type I rates considered. Smaller validation sizes $w_v$ are typically associated with a smaller type II error.", eval = FALSE}

# source("./type-II.R")
type2<- readRDS("./imgs/type2.rds")
grid.arrange(type2)

```

<!-- \small{\textbf{CS1} \citet{hadler}} algorithm \newline  \small{\textbf{CS2} the suggested modified algorithm} -->
\begin{figure}[hbtp]
\centering
\includegraphics[width=0.9\textwidth]{imgs/type-2-both-methods.png}
\caption{\label{type2}  Type 2 error of methods CS1 and CS2 across a range of different optimization windows $w_o$. Top two figures are for a \textbf{coarseness} $\approx$ \textit{0.15} and the bottom one is for \textit{0.3}}
\end{figure}

## Conclusions for Type II Error

\fontsize{10pt}{11}\selectfont

#### CS1
- Best works best for $w_o$ of $\approx$ 130 to 160 and $w_v$ 50 when the smoothing is $c$ $\approx$ 0.3.
- The Type II rate is lowest for a nominal $\alpha$ of 5%, with type I error rate of  6.2% and the Type II error rate of 24%. 
- For lower nominal alpha levels of 1%, 0.5% and 0.1% the lowest type II error rate increases to about 36.4%, 41% and 52.5% respectively.
- Gets worse for coarseness 0.15

#### CS2
- Significantly reduced over CS1
- For a window size of $w_o = 130$ we see a minimum in type II error rate across all type I rates considered. - Smaller validation sizes $w_v$ are typically associated with a smaller type II error.
- CS2 shows an increase in the power of the test.
- Type II CS2, still much higher for bullet lands than for toolmarks.
- Fix in CS2 will also improve power for matching toolmarks thans CS1
- Bullet-to-bullet comparison using CS2 $\approx$ more power out of the test.

## ROC Curves
\fontsize{10pt}{11}\selectfont
\begin{figure}[hbtp]
\centering
\includegraphics[width=0.9\textwidth]{imgs/roc-curves.png}
\caption{\label{roc-curves}  ROC curves of methods CS1 and CS2 for different sizes of optimization window $w_o$.}
\end{figure}

\vspace{-0.5em}

- Superior performance of CS2 over CS1
- Best performances wrt  ROC curves are reached for $w_0$ 150 and higher. 
- Points of equal error rates (EERs): intersection of the dotted line and the ROC curves.

##

### \textbf{THANK YOU. Questions?}

